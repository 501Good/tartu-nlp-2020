{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1. NLP Basics\n",
    "### Text preprocessing\n",
    "Text preprocessing is, probably, one of the least pleasant yet one of the most important steps of a natural language processing (NLP) pipelines. This step determines how your NLP algorithms are going to see the data. If your preprocessing breaks, the whole model can break or, what is even worse, keep silent and give incorrect results.\n",
    "\n",
    "Text preprocessing can be devided into three main parts:\n",
    "- Tokenization\n",
    "- Normalization\n",
    "- Noise reduction\n",
    "\n",
    "The parts are not necessarily applied in that particular order. Sometimes, before tokenization the noise reduction should be performed. In other cases, the some steps can be repeated several times.\n",
    "\n",
    "In the next steps, we are going to look into more details for each part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# If you don't have the model installed run \"python -m spacy download en_core_web_sm\"\n",
    "# in the console and restart the python kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install all the necessary files for NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "__Tokenization__ is a general term for splitting the text into smaller parts. We can highlight __word segmentation__ and __sentence segmentation__. Depending on the task, you might need to use only word segmentation, for other tasks, you might want to have both sentences and words.\n",
    "\n",
    "As the names suggest, _word segmentation_ is dividing the raw text sequence into words and _sentence segmentation_ is dividing the text into sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we need to parse the first paragraph from the Wikipedia article about Hawaii. We have the following raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"Hawaii is a state of the United States of America. \" + \\\n",
    "           \"It is the only state located in the Pacific Ocean and the only state composed entirely of islands.\"\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest and the most logical way to split the text into tokens would be to split it by the whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But already here, we can see the problem with the tokens like `'America.'` and `'islands.'`. In our case, the dot is the part of the token that we definetely don't want. One solution is to strip each token from the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    ...\n",
    "\n",
    "print(tokenize(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say now, that we want to split the text into sentences and then get tokens for each sentence. The simplest way is to split the text by dot first and then get tokens for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sents(text):\n",
    "    ...\n",
    "\n",
    "print(segment_sents(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, it worked fine so far. But this task hold many surprises for an unprepared person. Let's see another examples that can cause troubles if using our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficult_sents = [\n",
    "    \"Dr. Ford did not ask Col. Mustard the name of Mr. Smith's dog.\",\n",
    "    '\"What is all the fuss about?\" asked Mr. Peters.',\n",
    "    \"This full-time student isn't living in on-campus housing, and she's not wanting to visit Hawai'i.\"\n",
    "]\n",
    "\n",
    "for sent in difficult_sents:\n",
    "    print(segment_sents(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that different abbreviations like *Dr.*, *Col.*, *Mr.* were treated as a sentence end. Also, contractions like _isn't_ and _she's_ are in fact two words: _is not_ and _she is_. However, _Smith's_ can be either _Smith is_ or rather, like in our case, one word showing possession. Finally, we have to decide if _full-time_ and _on-campus_ have one word or two.\n",
    "\n",
    "Luckily, for English, we can use different libraries like __nltk__ or __spacy__ which tackle most of these problems. Let's see, how they manage with our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NLTK tokenization:\\n\")\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spacy tokenization:\\n\")\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Spacy somewhat better for this task. However, this is only that good for English and, probably, most of the European languages. If we take a language where the words are not graphically separated in writing, like Chinese, Thai, or German compound words, we need to choose another approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization is another important step in text preprocessing since it removes a lot of input information and makes it easier for the model to choose the most important things. Two main steps in normalization are __stemming__ and __lemmatization__. \n",
    "_Stemming_ usually refers to removing endings and prefixes from a word. For example, `playing` and `played` are going to be reduced to `play` after going through the stemmer. It works rather well for English but it can be troublesome for other languages with not so straightforward morphology. Also, the past tense for `run`,  `ran` is not going to be changed with stemming and finally is going to be considered a different word.\n",
    "\n",
    "NLTK library includes a stemming package as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_stem = ['playing', 'played', 'plays', 'play', 'running', 'ran', 'runs', 'run']\n",
    "...\n",
    "print('Stemming with NLTK:\\n')\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem with the words that change their roots in different grammarical forms, we should use more complicated method, called _lemmatization_. Lemmatization usually uses more sophisticated rules to find the normal form of the word. Now, however, most of the lemmatizers are trained using neural networks.\n",
    "\n",
    "Both NLTK and Spacy have a lemmatization module for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lemmatization with NLTK:\\n')\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see immediately that NLTK doesn't give correct lemmas for our words. This is because the NLTK lemmarizer expects to have a part-of-speech (POS) tag for each word, i.e. the information if the word is a noun, a verb, an adjective etc. We can, of course, specify the pos tag for each word but if our corpus is big, it will be tiresome to determine the pos tags by hand. In order to do that, we can use already pretrained pos tagger. We're going to look at pos tagging later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lemmatization with NLTK with correct pos tags:\\n')\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently for us, Spacy does pos tagging and other necessary preprocessing for lemmatization, and we can get all the lemmas with only one command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lemmatization with Spacy:\\n')\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how our sentences from the previous exercise look after stemming and lemmatization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NLTK stemming:\\n\")\n",
    "for sent in difficult_sents:\n",
    "    nltk_sents = ...\n",
    "    print(f'Original sentence:\\n{nltk_sents}')\n",
    "    nltk_stems = []\n",
    "    for sent in nltk_sents:\n",
    "        ...\n",
    "    print(f'Stemmed sentence:\\n{nltk_stems}')\n",
    "    print('\\n------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the NLTK stemmer also __puts all the words to lowercase__ which is another part of normalization. Also, we can also see some artifacts with the stemming like `thi`, `full-tim`, `on-campu`.\n",
    "\n",
    "Let's now see the lemmatized sentence from Spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spacy lemmatization:\\n\")\n",
    "for sent in difficult_sents:\n",
    "    doc = ...\n",
    "    print(f'Original sentence:\\n{...}')\n",
    "    print(f'Lemmatized sentence:\\n{...}')\n",
    "    print('\\n------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With lemmatization, the results look better: `did` trasformed to `do`, as well as `is` and `'s` to `be`. Another good thing is that in the first sentence `'s` in `Smith's dog` stayed as `'s` which is important because in this case it is not a contraction from the verb _is_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another parts for the normalization include:\n",
    "- Removing the punctuation\n",
    "- Removing whitespace\n",
    "- Removing numbers or converting them into text\n",
    "- Removing stop words\n",
    "\n",
    "Finally, we can look a bit more into the __stop words__. Stop words are the words that are very common in some language but usually don't carry any useful information about the idea of the text. For English, they can be *is*, *are*, *not*, *she*, *he*, *it* etc. This also usually includes prepositions and other particles. However, the stop list can be modified to fit a specific task.\n",
    "\n",
    "Both NLTK and Spacy have built-in lists for stop words, however, you are free to find it anywhere else on the internet or even compose your own list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stop words for English from NLTK:\\n')\n",
    "nltk_stopwords = ...\n",
    "print(nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stop words for English from Spacy:\\n')\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can see how our sentences look with the stop words removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NLTK stemming and stop words:\\n\")\n",
    "for sent in difficult_sents:\n",
    "    nltk_sents = ...\n",
    "    print(f'Original sentence:\\n{nltk_sents}')\n",
    "    nltk_stems = []\n",
    "    nltk_no_stop = []\n",
    "    for sent in nltk_sents:\n",
    "        ...\n",
    "    print(f'Stemmed sentence:\\n{nltk_stems}')\n",
    "    print(f'Stemmed sentence without stop words:\\n{nltk_no_stop}')\n",
    "    print('\\n------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spacy lemmatization and stop words:\\n\")\n",
    "for sent in difficult_sents:\n",
    "    doc = ...\n",
    "    print(f'Original sentence:\\n{...}')\n",
    "    print(f'Lemmatized sentence:\\n{...}')\n",
    "    print(f'Lemmatized sentence without stop words:\\n{...}')\n",
    "    print('\\n------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how it works on a real example, we can try to detect the sentiment with [Microsoft Azure Text Analysis system](https://azure.microsoft.com/en-in/services/cognitive-services/text-analytics/). Let's try to see if our example is negative or positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_review = \"The service in this restaurant is a nightmare.\"\n",
    "print(negative_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Not negative](review-example-01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_negative_review = ' '.join([token.lemma_ for token in nlp(negative_review) if not token.is_stop])\n",
    "print(preprocessed_negative_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Negative](review-example-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the preprocessed text got identified correctly as negative while for the raw text the system showed it to be neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Removal\n",
    "\n",
    "In this lab, we are not going to go into details of this step. It includes:\n",
    "- Removal of headers, footers and other parts of the articles\n",
    "- Removal of HTML, XML etc. markup\n",
    "- Extracting the data from various formats, like JSON, CONLL etc.\n",
    "\n",
    "Most of these steps can be done with the regular expressions. There are also good libraries out there to help you. For example, [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a very powerful tool for the HTML and XML parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
