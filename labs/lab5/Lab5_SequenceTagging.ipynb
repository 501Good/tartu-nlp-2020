{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab5_SequenceTagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yygt6gOJevIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1dBJUSQe6eb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7f1249bf-247a-47bb-f32c-2197f40c00de"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-27 08:29:50--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4b8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1306357571 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.ru.300.vec.gz’\n",
            "\n",
            "cc.ru.300.vec.gz    100%[===================>]   1.22G  12.9MB/s    in 98s     \n",
            "\n",
            "2020-03-27 08:31:29 (12.7 MB/s) - ‘cc.ru.300.vec.gz’ saved [1306357571/1306357571]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J3TIUs0fc0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb155b1a-9afb-4b50-c616-0447e86cd913"
      },
      "source": [
        "!gunzip cc.ru.300.vec.gz\n",
        "!mkdir vector_cache/\n",
        "!mv cc.ru.300.vec vector_cache/"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘vector_cache/’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJBLZXIEgbbo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9f3fcc80-e58b-4f1a-987b-5b6ed44022cd"
      },
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_Russian-Taiga.git"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'UD_Russian-Taiga'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Total 76 (delta 0), reused 0 (delta 0), pack-reused 76\u001b[K\n",
            "Unpacking objects: 100% (76/76), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJqCd9yQggJn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a3c3e4a6-52de-4a8b-cf0b-c8bae82c3551"
      },
      "source": [
        "!mkdir data/\n",
        "!mv UD_Russian-Taiga/ data/"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data/’: File exists\n",
            "mv: cannot move 'UD_Russian-Taiga/' to 'data/UD_Russian-Taiga': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb4fsehMgkBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD = '<PAD>'\n",
        "PAD_ID = 0\n",
        "UNK = '<UNK>'\n",
        "UNK_ID = 1\n",
        "VOCAB_PREFIX = [PAD, UNK]\n",
        "\n",
        "VEC_PATH = Path('vector_cache') / 'cc.ru.300.vec'\n",
        "DATA_PATH = Path('data') / 'UD_Russian-Taiga'\n",
        "MAX_VOCAB = 25000\n",
        "\n",
        "batch_size = 64\n",
        "validation_split = .3\n",
        "shuffle_dataset = True\n",
        "random_seed = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGsPh253hDIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaseVocab:\n",
        "    def __init__(self, data, idx=0, lower=False):\n",
        "        self.data = data\n",
        "        self.lower = lower\n",
        "        self.idx = idx\n",
        "        self.build_vocab()\n",
        "        \n",
        "    def normalize_unit(self, unit):\n",
        "        if self.lower:\n",
        "            return unit.lower()\n",
        "        else:\n",
        "            return unit\n",
        "        \n",
        "    def unit2id(self, unit):\n",
        "        unit = self.normalize_unit(unit)\n",
        "        if unit in self._unit2id:\n",
        "            return self._unit2id[unit]\n",
        "        else:\n",
        "            return self._unit2id[UNK]\n",
        "    \n",
        "    def id2unit(self, id):\n",
        "        return self._id2unit[id]\n",
        "    \n",
        "    def map(self, units):\n",
        "        return [self.unit2id(unit) for unit in units]\n",
        "\n",
        "    def unmap(self, ids):\n",
        "        return [self.id2unit(idx) for idx in ids]\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        NotImplementedError()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._unit2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMvi6s3lsFA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PretrainedWordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        self._id2unit = VOCAB_PREFIX + self.data\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9mn3bWHhXER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        if self.lower:\n",
        "            counter = Counter([w[self.idx].lower() for sent in self.data for w in sent])\n",
        "        else:\n",
        "            counter = Counter([w[self.idx] for sent in self.data for w in sent])\n",
        "\n",
        "        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOefas-mheLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pretrain:\n",
        "    def __init__(self, vec_filename, max_vocab=-1):\n",
        "        self._vec_filename = vec_filename\n",
        "        self._max_vocab = max_vocab\n",
        "        \n",
        "    @property\n",
        "    def vocab(self):\n",
        "        if not hasattr(self, '_vocab'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._vocab\n",
        "    \n",
        "    @property\n",
        "    def emb(self):\n",
        "        if not hasattr(self, '_emb'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._emb\n",
        "        \n",
        "    def read(self):\n",
        "        if self._vec_filename is None:\n",
        "            raise Exception(\"Vector file is not provided.\")\n",
        "        print(f\"Reading pretrained vectors from {self._vec_filename}...\")\n",
        "        \n",
        "        words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n",
        "        \n",
        "        if failed > 0: # recover failure\n",
        "            emb = emb[:-failed]\n",
        "        if len(emb) - len(VOCAB_PREFIX) != len(words):\n",
        "            raise Exception(\"Loaded number of vectors does not match number of words.\")\n",
        "            \n",
        "        # Use a fixed vocab size\n",
        "        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):\n",
        "            words = words[:self._max_vocab - len(VOCAB_PREFIX)]\n",
        "            emb = emb[:self._max_vocab]\n",
        "                \n",
        "        vocab = PretrainedWordVocab(words, lower=True)\n",
        "        \n",
        "        return vocab, emb\n",
        "        \n",
        "    def read_from_file(self, filename, open_func=open):\n",
        "        \"\"\"\n",
        "        Open a vector file using the provided function and read from it.\n",
        "        \"\"\"\n",
        "        first = True\n",
        "        words = []\n",
        "        failed = 0\n",
        "        with open_func(filename, 'rb') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    line = line.decode()\n",
        "                except UnicodeDecodeError:\n",
        "                    failed += 1\n",
        "                    continue\n",
        "                if first:\n",
        "                    # the first line contains the number of word vectors and the dimensionality\n",
        "                    first = False\n",
        "                    line = line.strip().split(' ')\n",
        "                    rows, cols = [int(x) for x in line]\n",
        "                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n",
        "                    continue\n",
        "\n",
        "                line = line.rstrip().split(' ')\n",
        "                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n",
        "                words.append(' '.join(line[:-cols]))\n",
        "        return words, emb, failed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8i2PuW6iafP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIELD_NUM = 10\n",
        "\n",
        "class Word:\n",
        "    def __init__(self, word):\n",
        "        self._id = word[0]\n",
        "        self._text = word[1]\n",
        "        self._lemma = word[2]\n",
        "        self._upos = word[3]\n",
        "        self._xpos = word[4]\n",
        "        self._feats = word[5]\n",
        "        self._head = word[6]\n",
        "        self._deprel = word[7]\n",
        "        self._deps = word[8]\n",
        "        self._misc = word[9]\n",
        "\n",
        "    @property\n",
        "    def id(self):\n",
        "        return self._id\n",
        "\n",
        "    @property\n",
        "    def text(self):\n",
        "        return self._text\n",
        "\n",
        "    @property\n",
        "    def lemma(self):\n",
        "        return self._lemma\n",
        "\n",
        "    @property\n",
        "    def upos(self):\n",
        "        return self._upos\n",
        "\n",
        "    @property\n",
        "    def xpos(self):\n",
        "        return self._xpos\n",
        "\n",
        "    @property\n",
        "    def feats(self):\n",
        "        return self._feats\n",
        "\n",
        "    @property\n",
        "    def head(self):\n",
        "        return self._head\n",
        "\n",
        "    @property\n",
        "    def deprel(self):\n",
        "        return self._deprel\n",
        "\n",
        "    @property\n",
        "    def deps(self):\n",
        "        return self._deps\n",
        "\n",
        "    @property\n",
        "    def misc(self):\n",
        "        return self._misc\n",
        "\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, words):\n",
        "        self._words = [Word(w) for w in words]\n",
        "\n",
        "    @property\n",
        "    def words(self):\n",
        "        return self._words\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, file_path):\n",
        "        self._sentences = []\n",
        "        self.load_conll(open(file_path, encoding='utf-8'))\n",
        "\n",
        "\n",
        "    def load_conll(self, f, ignore_gapping=True):\n",
        "        \"\"\" Load the file or string into the CoNLL-U format data.\n",
        "        Input: file or string reader, where the data is in CoNLL-U format.\n",
        "        Output: a list of list of list for each token in each sentence in the data, where the innermost list represents \n",
        "        all fields of a token.\n",
        "        \"\"\"\n",
        "        # f is open() or io.StringIO()\n",
        "        doc, sent = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if len(line) == 0:\n",
        "                if len(sent) > 0:\n",
        "                    doc.append(Sentence(sent))\n",
        "                    sent = []\n",
        "            else:\n",
        "                if line.startswith('#'): # skip comment line\n",
        "                    continue\n",
        "                array = line.split('\\t')\n",
        "                if ignore_gapping and '.' in array[0]:\n",
        "                    continue\n",
        "                assert len(array) == FIELD_NUM, \\\n",
        "                        f\"Cannot parse CoNLL line: expecting {FIELD_NUM} fields, {len(array)} found.\"\n",
        "                sent += [array]\n",
        "        if len(sent) > 0:\n",
        "            doc.append(Sentence(sent))\n",
        "        self._sentences = doc\n",
        "\n",
        "    \n",
        "    @property\n",
        "    def sentences(self):\n",
        "        return self._sentences\n",
        "\n",
        "\n",
        "    def get(self, fields, as_sentences=False):\n",
        "        assert isinstance(fields, list), \"Must provide field names as a list.\"\n",
        "        assert len(fields) >= 1, \"Must have at least one field.\"\n",
        "\n",
        "        results = []\n",
        "        for sentence in self.sentences:\n",
        "            cursent = []\n",
        "            units = sentence.words\n",
        "            for unit in units:\n",
        "                if len(fields) == 1:\n",
        "                    cursent += [getattr(unit, fields[0])]\n",
        "                else:\n",
        "                    cursent += [[getattr(unit, field) for field in fields]]\n",
        "\n",
        "            # decide whether append the results as a sentence or a whole list\n",
        "            if as_sentences:\n",
        "                results.append(cursent)\n",
        "            else:\n",
        "                results += cursent\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7UVFATAhsEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CONLLUDataset(Dataset):\n",
        "    def __init__(self, doc, pretrain, vocab=None, test=False):\n",
        "        self.pretrain_vocab = pretrain.vocab\n",
        "        self.test = test\n",
        "        data = self.load_doc(doc)\n",
        "\n",
        "        if vocab is None:\n",
        "            self.vocab = self.init_vocab(data)\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "        self.data = self.preprocess(data, self.vocab, self.pretrain_vocab)\n",
        "\n",
        "    def init_vocab(self, data):\n",
        "        wordvocab = WordVocab(data, idx=0)\n",
        "        uposvocab = WordVocab(data, idx=1)\n",
        "        vocab = {\n",
        "            'word': wordvocab,\n",
        "            'upos': uposvocab}\n",
        "        return vocab\n",
        "\n",
        "    def preprocess(self, data, vocab, pretrain_vocab):\n",
        "        processed = []\n",
        "        for sent in data:\n",
        "            processed_sent = [vocab['word'].map([w[0] for w in sent])]\n",
        "            processed_sent += [vocab['upos'].map([w[1] for w in sent])]\n",
        "            processed_sent += [pretrain_vocab.map([w[0].lower() for w in sent])]\n",
        "            processed.append(processed_sent)\n",
        "        return processed\n",
        "        \n",
        "    def load_doc(self, doc):\n",
        "        data = doc.get(['text', 'upos'], as_sentences=True)\n",
        "        return data\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9L6KBcOkVu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QraxfFjKs-jZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_doc = Document(DATA_PATH / 'ru_taiga-ud-train.conllu')\n",
        "train_dataset = CONLLUDataset(train_doc, pretrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdk0QJJheLpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = train_dataset.vocab\n",
        "dev_doc = Document(DATA_PATH / 'ru_taiga-ud-dev.conllu')\n",
        "dev_dataset = CONLLUDataset(dev_doc, pretrain, vocab=vocab, test=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBZSDFv6t_-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1592c1a7-4f21-4e17-a4aa-db0b9f2e8480"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1507, 1508, 695, 2], [9, 4, 2, 3], [719, 13335, 16854, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbhf2uYHvcJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_collate(batch):\n",
        "    (words, upos, pretrained) = zip(*batch)\n",
        "\n",
        "    word_lens = [len(w) for w in words]\n",
        "    upos_lens = [len(u) for u in upos]\n",
        "    pretrained_lens = [len(p) for p in pretrained]\n",
        "\n",
        "    words = [torch.LongTensor(w).to(device) for w in words]\n",
        "    upos = [torch.LongTensor(u).to(device) for u in upos]\n",
        "    pretrained = [torch.LongTensor(p).to(device) for p in pretrained]\n",
        "\n",
        "    word_pad = pad_sequence(words, batch_first=True, padding_value=PAD_ID)\n",
        "    upos_pad = pad_sequence(upos, batch_first=True, padding_value=PAD_ID)\n",
        "    pretrained_pad = pad_sequence(pretrained, batch_first=True, padding_value=PAD_ID)\n",
        "\n",
        "    word_pad = word_pad.to(device)\n",
        "    upos_pad = upos_pad.to(device)\n",
        "    pretrained_pad = pretrained_pad.to(device)\n",
        "\n",
        "    return word_pad, upos_pad, pretrained_pad, word_lens, upos_lens, pretrained_lens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMNbcGrQ7dGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li5RRsb6IrJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N-xVFW28FD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tagger(nn.Module):\n",
        "    def __init__(self, vocab, word_emb_dim, transformed_dim, emb_matrix, hidden_dim, upos_clf_hidden_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        input_size = 0\n",
        "\n",
        "        self.word_emb = nn.Embedding(len(vocab['word']), word_emb_dim, padding_idx=0)\n",
        "        input_size += word_emb_dim\n",
        "\n",
        "        self.pretrained_emb = nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True)\n",
        "        self.trans_pretrained = nn.Linear(emb_matrix.shape[1], transformed_dim, bias=False)\n",
        "        input_size += transformed_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.lstm_h_init = nn.Parameter(torch.zeros(2 * num_layers, 1, hidden_dim))\n",
        "        self.lstm_c_init = nn.Parameter(torch.zeros(2 * num_layers, 1, hidden_dim))\n",
        "\n",
        "        self.upos_hid = nn.Linear(2* hidden_dim, upos_clf_hidden_dim)\n",
        "        self.upos_clf = nn.Linear(upos_clf_hidden_dim, len(vocab['upos']))\n",
        "\n",
        "        self.crit = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "    def forward(self, word_pad, upos_pad, pretrained_pad, word_lens, upos_lens, pretrained_lens):\n",
        "        inputs = []\n",
        "\n",
        "        word_emb = self.word_emb(word_pad)\n",
        "        word_emb = pack_padded_sequence(word_emb, word_lens, batch_first=True, enforce_sorted=False)\n",
        "        inputs += [word_emb]\n",
        "\n",
        "        pretrained_emb = self.pretrained_emb(pretrained_pad)\n",
        "        pretrained_emb = self.trans_pretrained(pretrained_emb)\n",
        "        pretrained_emb = pack_padded_sequence(pretrained_emb, pretrained_lens, batch_first=True, enforce_sorted=False)\n",
        "        inputs += [pretrained_emb]\n",
        "\n",
        "        lstm_inputs = torch.cat([x.data for x in inputs], 1)\n",
        "        lstm_inputs = self.drop(lstm_inputs)\n",
        "        lstm_inputs = PackedSequence(lstm_inputs, inputs[0].batch_sizes)\n",
        "\n",
        "        lstm_outputs, (h0, c0) = self.lstm(\n",
        "            lstm_inputs, \n",
        "            (self.lstm_h_init.expand(2 * self.num_layers, word_pad.size(0), self.hidden_dim).contiguous(), \n",
        "             self.lstm_c_init.expand(2 * self.num_layers, word_pad.size(0), self.hidden_dim).contiguous())\n",
        "        )\n",
        "        lstm_outputs = lstm_outputs.data\n",
        "\n",
        "        upos_hid = F.relu(self.upos_hid(self.drop(lstm_outputs)))\n",
        "        upos_pred = self.upos_clf(self.drop(upos_hid))\n",
        "\n",
        "        pred = pad_packed_sequence(PackedSequence(upos_pred, word_emb.batch_sizes), batch_first=True)[0]\n",
        "        pred = pred.max(2)[1]\n",
        "        upos = pack_padded_sequence(upos_pad, upos_lens, batch_first=True, enforce_sorted=False).data\n",
        "        loss = self.crit(upos_pred, upos)\n",
        "\n",
        "        return loss, pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-iZlzjyF4no",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, vocab, word_emb_dim, transformed_dim, emb_matrix, hidden_dim, upos_clf_hidden_dim, num_layers, dropout, use_cuda=False):\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.model = Tagger(vocab, word_emb_dim, transformed_dim, emb_matrix, hidden_dim, upos_clf_hidden_dim, num_layers, dropout)\n",
        "        self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
        "\n",
        "        self.model.to(device)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters)\n",
        " \n",
        "    def update(self, batch, eval=False):\n",
        "        word_pad, upos_pad, pretrained_pad, word_lens, upos_lens, pretrained_lens = batch\n",
        "\n",
        "        if eval:\n",
        "            self.model.eval()\n",
        "        else:\n",
        "            self.model.train()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        loss, _ = self.model(word_pad, upos_pad, pretrained_pad, word_lens, upos_lens, pretrained_lens)\n",
        "        loss_val = loss.data.item()\n",
        "        if eval:\n",
        "            return loss_val\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss_val\n",
        "\n",
        "\n",
        "    def predict(self, batch):\n",
        "        word_pad, upos_pad, pretrained_pad, word_lens, upos_lens, pretrained_lens = batch\n",
        "\n",
        "        self.model.eval()\n",
        "        _, pred = self.model(word_pad, upos_pad, pretrained_pad, word_lens, upos_lens, pretrained_lens)\n",
        "        pred = [self.vocab['upos'].unmap(sent) for sent in pred.tolist()]\n",
        "\n",
        "        return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAu5ZF94JMvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_emb_dim = 75\n",
        "transformed_dim = 125\n",
        "emb_matrix = pretrain.emb\n",
        "hidden_dim = 200\n",
        "upos_clf_hidden_dim = 400\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "use_cuda = True if device == 'cuda' else False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiKLCdBaJG-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(vocab, word_emb_dim, transformed_dim, emb_matrix, hidden_dim, upos_clf_hidden_dim, num_layers, dropout, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjJm834cJ-kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = 0\n",
        "max_steps = 50000\n",
        "dev_score_history = []\n",
        "format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch)'\n",
        "last_best_step = 0\n",
        "\n",
        "log_step = 20\n",
        "eval_interval = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY5M8lS5Kibm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73869b8f-e98f-45d7-885c-c644dfb062e8"
      },
      "source": [
        "train_loss = 0\n",
        "while True:\n",
        "    do_break = False\n",
        "    for batch in train_loader:\n",
        "        start_time = time()\n",
        "        global_step += 1\n",
        "        loss = trainer.update(batch, eval=False)\n",
        "        train_loss += loss\n",
        "\n",
        "        if global_step % log_step == 0:\n",
        "            duration = time() - start_time\n",
        "            print(format_str.format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), global_step,\\\n",
        "                    max_steps, loss, duration))\n",
        "            \n",
        "        if global_step % eval_interval == 0:\n",
        "            print(\"Evaluating on dev set...\")\n",
        "            dev_preds = []\n",
        "            for batch in dev_loader:\n",
        "                preds = trainer.predict(batch)\n",
        "                dev_preds += preds\n",
        "            \n",
        "            train_loss = train_loss / eval_interval\n",
        "            print(\"step {}: train_loss = {:.6f}\".format(global_step, train_loss))\n",
        "            train_loss = 0\n",
        "\n",
        "        if global_step >= max_steps:\n",
        "            do_break = True\n",
        "            break\n",
        "\n",
        "        if do_break:\n",
        "            break"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-27 10:04:12: step 20/50000, loss = 2.312118 (0.026 sec/batch)\n",
            "2020-03-27 10:04:13: step 40/50000, loss = 1.894985 (0.026 sec/batch)\n",
            "2020-03-27 10:04:13: step 60/50000, loss = 1.607767 (0.035 sec/batch)\n",
            "2020-03-27 10:04:14: step 80/50000, loss = 1.212688 (0.037 sec/batch)\n",
            "2020-03-27 10:04:14: step 100/50000, loss = 1.016654 (0.034 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 100: train_loss = 1.760188\n",
            "2020-03-27 10:04:15: step 120/50000, loss = 0.937383 (0.028 sec/batch)\n",
            "2020-03-27 10:04:16: step 140/50000, loss = 0.836219 (0.034 sec/batch)\n",
            "2020-03-27 10:04:17: step 160/50000, loss = 0.796540 (0.025 sec/batch)\n",
            "2020-03-27 10:04:17: step 180/50000, loss = 0.762427 (0.031 sec/batch)\n",
            "2020-03-27 10:04:18: step 200/50000, loss = 0.807348 (0.032 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 200: train_loss = 0.844437\n",
            "2020-03-27 10:04:19: step 220/50000, loss = 0.643031 (0.026 sec/batch)\n",
            "2020-03-27 10:04:19: step 240/50000, loss = 0.705730 (0.026 sec/batch)\n",
            "2020-03-27 10:04:20: step 260/50000, loss = 0.668514 (0.023 sec/batch)\n",
            "2020-03-27 10:04:21: step 280/50000, loss = 0.541623 (0.028 sec/batch)\n",
            "2020-03-27 10:04:21: step 300/50000, loss = 0.587753 (0.036 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 300: train_loss = 0.638426\n",
            "2020-03-27 10:04:22: step 320/50000, loss = 0.591951 (0.068 sec/batch)\n",
            "2020-03-27 10:04:23: step 340/50000, loss = 0.487918 (0.024 sec/batch)\n",
            "2020-03-27 10:04:23: step 360/50000, loss = 0.516418 (0.021 sec/batch)\n",
            "2020-03-27 10:04:24: step 380/50000, loss = 0.552111 (0.025 sec/batch)\n",
            "2020-03-27 10:04:25: step 400/50000, loss = 0.431657 (0.018 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 400: train_loss = 0.535687\n",
            "2020-03-27 10:04:26: step 420/50000, loss = 0.479957 (0.028 sec/batch)\n",
            "2020-03-27 10:04:26: step 440/50000, loss = 0.415560 (0.026 sec/batch)\n",
            "2020-03-27 10:04:27: step 460/50000, loss = 0.500979 (0.023 sec/batch)\n",
            "2020-03-27 10:04:28: step 480/50000, loss = 0.444972 (0.026 sec/batch)\n",
            "2020-03-27 10:04:28: step 500/50000, loss = 0.430090 (0.022 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 500: train_loss = 0.452121\n",
            "2020-03-27 10:04:29: step 520/50000, loss = 0.386263 (0.026 sec/batch)\n",
            "2020-03-27 10:04:30: step 540/50000, loss = 0.423417 (0.025 sec/batch)\n",
            "2020-03-27 10:04:30: step 560/50000, loss = 0.323408 (0.028 sec/batch)\n",
            "2020-03-27 10:04:31: step 580/50000, loss = 0.348303 (0.067 sec/batch)\n",
            "2020-03-27 10:04:32: step 600/50000, loss = 0.337959 (0.036 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 600: train_loss = 0.387186\n",
            "2020-03-27 10:04:33: step 620/50000, loss = 0.346396 (0.087 sec/batch)\n",
            "2020-03-27 10:04:33: step 640/50000, loss = 0.281746 (0.032 sec/batch)\n",
            "2020-03-27 10:04:34: step 660/50000, loss = 0.320488 (0.025 sec/batch)\n",
            "2020-03-27 10:04:34: step 680/50000, loss = 0.352430 (0.029 sec/batch)\n",
            "2020-03-27 10:04:35: step 700/50000, loss = 0.299549 (0.037 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 700: train_loss = 0.331392\n",
            "2020-03-27 10:04:36: step 720/50000, loss = 0.300801 (0.033 sec/batch)\n",
            "2020-03-27 10:04:36: step 740/50000, loss = 0.274665 (0.039 sec/batch)\n",
            "2020-03-27 10:04:37: step 760/50000, loss = 0.296251 (0.020 sec/batch)\n",
            "2020-03-27 10:04:38: step 780/50000, loss = 0.294552 (0.035 sec/batch)\n",
            "2020-03-27 10:04:38: step 800/50000, loss = 0.250136 (0.032 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 800: train_loss = 0.278732\n",
            "2020-03-27 10:04:39: step 820/50000, loss = 0.262862 (0.019 sec/batch)\n",
            "2020-03-27 10:04:40: step 840/50000, loss = 0.256831 (0.026 sec/batch)\n",
            "2020-03-27 10:04:41: step 860/50000, loss = 0.190912 (0.031 sec/batch)\n",
            "2020-03-27 10:04:41: step 880/50000, loss = 0.209062 (0.026 sec/batch)\n",
            "2020-03-27 10:04:42: step 900/50000, loss = 0.241274 (0.021 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 900: train_loss = 0.234715\n",
            "2020-03-27 10:04:43: step 920/50000, loss = 0.212977 (0.021 sec/batch)\n",
            "2020-03-27 10:04:43: step 940/50000, loss = 0.195109 (0.024 sec/batch)\n",
            "2020-03-27 10:04:44: step 960/50000, loss = 0.203703 (0.024 sec/batch)\n",
            "2020-03-27 10:04:45: step 980/50000, loss = 0.217186 (0.031 sec/batch)\n",
            "2020-03-27 10:04:45: step 1000/50000, loss = 0.192632 (0.023 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1000: train_loss = 0.201748\n",
            "2020-03-27 10:04:46: step 1020/50000, loss = 0.175341 (0.031 sec/batch)\n",
            "2020-03-27 10:04:47: step 1040/50000, loss = 0.172139 (0.020 sec/batch)\n",
            "2020-03-27 10:04:47: step 1060/50000, loss = 0.194531 (0.024 sec/batch)\n",
            "2020-03-27 10:04:48: step 1080/50000, loss = 0.189957 (0.026 sec/batch)\n",
            "2020-03-27 10:04:49: step 1100/50000, loss = 0.146317 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1100: train_loss = 0.174277\n",
            "2020-03-27 10:04:50: step 1120/50000, loss = 0.141731 (0.028 sec/batch)\n",
            "2020-03-27 10:04:50: step 1140/50000, loss = 0.180629 (0.028 sec/batch)\n",
            "2020-03-27 10:04:51: step 1160/50000, loss = 0.158935 (0.023 sec/batch)\n",
            "2020-03-27 10:04:52: step 1180/50000, loss = 0.149094 (0.026 sec/batch)\n",
            "2020-03-27 10:04:52: step 1200/50000, loss = 0.143867 (0.021 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1200: train_loss = 0.152927\n",
            "2020-03-27 10:04:53: step 1220/50000, loss = 0.128406 (0.022 sec/batch)\n",
            "2020-03-27 10:04:54: step 1240/50000, loss = 0.128432 (0.025 sec/batch)\n",
            "2020-03-27 10:04:54: step 1260/50000, loss = 0.144015 (0.026 sec/batch)\n",
            "2020-03-27 10:04:55: step 1280/50000, loss = 0.107558 (0.027 sec/batch)\n",
            "2020-03-27 10:04:56: step 1300/50000, loss = 0.109611 (0.032 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1300: train_loss = 0.129676\n",
            "2020-03-27 10:04:57: step 1320/50000, loss = 0.117830 (0.031 sec/batch)\n",
            "2020-03-27 10:04:57: step 1340/50000, loss = 0.118084 (0.022 sec/batch)\n",
            "2020-03-27 10:04:58: step 1360/50000, loss = 0.121014 (0.029 sec/batch)\n",
            "2020-03-27 10:04:59: step 1380/50000, loss = 0.176143 (0.019 sec/batch)\n",
            "2020-03-27 10:04:59: step 1400/50000, loss = 0.119947 (0.065 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1400: train_loss = 0.115088\n",
            "2020-03-27 10:05:00: step 1420/50000, loss = 0.095364 (0.022 sec/batch)\n",
            "2020-03-27 10:05:01: step 1440/50000, loss = 0.110371 (0.030 sec/batch)\n",
            "2020-03-27 10:05:02: step 1460/50000, loss = 0.096987 (0.028 sec/batch)\n",
            "2020-03-27 10:05:02: step 1480/50000, loss = 0.102682 (0.021 sec/batch)\n",
            "2020-03-27 10:05:03: step 1500/50000, loss = 0.074185 (0.034 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1500: train_loss = 0.107543\n",
            "2020-03-27 10:05:04: step 1520/50000, loss = 0.089909 (0.030 sec/batch)\n",
            "2020-03-27 10:05:04: step 1540/50000, loss = 0.111413 (0.026 sec/batch)\n",
            "2020-03-27 10:05:05: step 1560/50000, loss = 0.095630 (0.032 sec/batch)\n",
            "2020-03-27 10:05:06: step 1580/50000, loss = 0.063469 (0.032 sec/batch)\n",
            "2020-03-27 10:05:06: step 1600/50000, loss = 0.085576 (0.025 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1600: train_loss = 0.093322\n",
            "2020-03-27 10:05:07: step 1620/50000, loss = 0.091875 (0.028 sec/batch)\n",
            "2020-03-27 10:05:08: step 1640/50000, loss = 0.099850 (0.027 sec/batch)\n",
            "2020-03-27 10:05:09: step 1660/50000, loss = 0.070032 (0.035 sec/batch)\n",
            "2020-03-27 10:05:09: step 1680/50000, loss = 0.076126 (0.023 sec/batch)\n",
            "2020-03-27 10:05:10: step 1700/50000, loss = 0.079897 (0.022 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1700: train_loss = 0.087458\n",
            "2020-03-27 10:05:11: step 1720/50000, loss = 0.061476 (0.028 sec/batch)\n",
            "2020-03-27 10:05:11: step 1740/50000, loss = 0.063376 (0.027 sec/batch)\n",
            "2020-03-27 10:05:12: step 1760/50000, loss = 0.075774 (0.021 sec/batch)\n",
            "2020-03-27 10:05:13: step 1780/50000, loss = 0.090967 (0.033 sec/batch)\n",
            "2020-03-27 10:05:13: step 1800/50000, loss = 0.087944 (0.029 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1800: train_loss = 0.078134\n",
            "2020-03-27 10:05:14: step 1820/50000, loss = 0.076864 (0.051 sec/batch)\n",
            "2020-03-27 10:05:15: step 1840/50000, loss = 0.086511 (0.024 sec/batch)\n",
            "2020-03-27 10:05:16: step 1860/50000, loss = 0.080007 (0.026 sec/batch)\n",
            "2020-03-27 10:05:16: step 1880/50000, loss = 0.060628 (0.024 sec/batch)\n",
            "2020-03-27 10:05:17: step 1900/50000, loss = 0.067547 (0.025 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1900: train_loss = 0.071548\n",
            "2020-03-27 10:05:18: step 1920/50000, loss = 0.074957 (0.023 sec/batch)\n",
            "2020-03-27 10:05:18: step 1940/50000, loss = 0.075539 (0.031 sec/batch)\n",
            "2020-03-27 10:05:19: step 1960/50000, loss = 0.051606 (0.033 sec/batch)\n",
            "2020-03-27 10:05:20: step 1980/50000, loss = 0.073506 (0.025 sec/batch)\n",
            "2020-03-27 10:05:20: step 2000/50000, loss = 0.071866 (0.022 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2000: train_loss = 0.066004\n",
            "2020-03-27 10:05:21: step 2020/50000, loss = 0.091978 (0.033 sec/batch)\n",
            "2020-03-27 10:05:22: step 2040/50000, loss = 0.065366 (0.027 sec/batch)\n",
            "2020-03-27 10:05:22: step 2060/50000, loss = 0.053909 (0.033 sec/batch)\n",
            "2020-03-27 10:05:23: step 2080/50000, loss = 0.066553 (0.022 sec/batch)\n",
            "2020-03-27 10:05:24: step 2100/50000, loss = 0.070077 (0.029 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2100: train_loss = 0.059669\n",
            "2020-03-27 10:05:25: step 2120/50000, loss = 0.055205 (0.031 sec/batch)\n",
            "2020-03-27 10:05:25: step 2140/50000, loss = 0.062362 (0.026 sec/batch)\n",
            "2020-03-27 10:05:26: step 2160/50000, loss = 0.057489 (0.020 sec/batch)\n",
            "2020-03-27 10:05:27: step 2180/50000, loss = 0.055539 (0.022 sec/batch)\n",
            "2020-03-27 10:05:27: step 2200/50000, loss = 0.072803 (0.026 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2200: train_loss = 0.056699\n",
            "2020-03-27 10:05:28: step 2220/50000, loss = 0.057737 (0.020 sec/batch)\n",
            "2020-03-27 10:05:29: step 2240/50000, loss = 0.042316 (0.033 sec/batch)\n",
            "2020-03-27 10:05:29: step 2260/50000, loss = 0.029450 (0.029 sec/batch)\n",
            "2020-03-27 10:05:30: step 2280/50000, loss = 0.035436 (0.030 sec/batch)\n",
            "2020-03-27 10:05:31: step 2300/50000, loss = 0.064178 (0.018 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2300: train_loss = 0.048659\n",
            "2020-03-27 10:05:32: step 2320/50000, loss = 0.056455 (0.020 sec/batch)\n",
            "2020-03-27 10:05:32: step 2340/50000, loss = 0.040805 (0.027 sec/batch)\n",
            "2020-03-27 10:05:33: step 2360/50000, loss = 0.053043 (0.022 sec/batch)\n",
            "2020-03-27 10:05:33: step 2380/50000, loss = 0.041098 (0.034 sec/batch)\n",
            "2020-03-27 10:05:34: step 2400/50000, loss = 0.040435 (0.029 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2400: train_loss = 0.049251\n",
            "2020-03-27 10:05:35: step 2420/50000, loss = 0.050041 (0.042 sec/batch)\n",
            "2020-03-27 10:05:35: step 2440/50000, loss = 0.027454 (0.026 sec/batch)\n",
            "2020-03-27 10:05:36: step 2460/50000, loss = 0.057000 (0.023 sec/batch)\n",
            "2020-03-27 10:05:37: step 2480/50000, loss = 0.031574 (0.034 sec/batch)\n",
            "2020-03-27 10:05:37: step 2500/50000, loss = 0.034131 (0.023 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2500: train_loss = 0.046516\n",
            "2020-03-27 10:05:38: step 2520/50000, loss = 0.043221 (0.028 sec/batch)\n",
            "2020-03-27 10:05:39: step 2540/50000, loss = 0.033512 (0.026 sec/batch)\n",
            "2020-03-27 10:05:40: step 2560/50000, loss = 0.039419 (0.022 sec/batch)\n",
            "2020-03-27 10:05:40: step 2580/50000, loss = 0.041872 (0.033 sec/batch)\n",
            "2020-03-27 10:05:41: step 2600/50000, loss = 0.034509 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2600: train_loss = 0.044161\n",
            "2020-03-27 10:05:42: step 2620/50000, loss = 0.034240 (0.024 sec/batch)\n",
            "2020-03-27 10:05:43: step 2640/50000, loss = 0.040600 (0.032 sec/batch)\n",
            "2020-03-27 10:05:43: step 2660/50000, loss = 0.038349 (0.024 sec/batch)\n",
            "2020-03-27 10:05:44: step 2680/50000, loss = 0.039732 (0.028 sec/batch)\n",
            "2020-03-27 10:05:44: step 2700/50000, loss = 0.066960 (0.021 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2700: train_loss = 0.039742\n",
            "2020-03-27 10:05:45: step 2720/50000, loss = 0.042117 (0.029 sec/batch)\n",
            "2020-03-27 10:05:46: step 2740/50000, loss = 0.039723 (0.027 sec/batch)\n",
            "2020-03-27 10:05:47: step 2760/50000, loss = 0.056219 (0.028 sec/batch)\n",
            "2020-03-27 10:05:47: step 2780/50000, loss = 0.049969 (0.043 sec/batch)\n",
            "2020-03-27 10:05:48: step 2800/50000, loss = 0.031739 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2800: train_loss = 0.040460\n",
            "2020-03-27 10:05:49: step 2820/50000, loss = 0.041722 (0.063 sec/batch)\n",
            "2020-03-27 10:05:49: step 2840/50000, loss = 0.046969 (0.037 sec/batch)\n",
            "2020-03-27 10:05:50: step 2860/50000, loss = 0.040131 (0.018 sec/batch)\n",
            "2020-03-27 10:05:51: step 2880/50000, loss = 0.051928 (0.026 sec/batch)\n",
            "2020-03-27 10:05:51: step 2900/50000, loss = 0.014822 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2900: train_loss = 0.040153\n",
            "2020-03-27 10:05:52: step 2920/50000, loss = 0.044817 (0.021 sec/batch)\n",
            "2020-03-27 10:05:53: step 2940/50000, loss = 0.025027 (0.030 sec/batch)\n",
            "2020-03-27 10:05:53: step 2960/50000, loss = 0.023723 (0.022 sec/batch)\n",
            "2020-03-27 10:05:54: step 2980/50000, loss = 0.031445 (0.032 sec/batch)\n",
            "2020-03-27 10:05:55: step 3000/50000, loss = 0.046014 (0.022 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3000: train_loss = 0.035986\n",
            "2020-03-27 10:05:56: step 3020/50000, loss = 0.031581 (0.029 sec/batch)\n",
            "2020-03-27 10:05:56: step 3040/50000, loss = 0.018766 (0.028 sec/batch)\n",
            "2020-03-27 10:05:57: step 3060/50000, loss = 0.054239 (0.025 sec/batch)\n",
            "2020-03-27 10:05:57: step 3080/50000, loss = 0.033540 (0.032 sec/batch)\n",
            "2020-03-27 10:05:58: step 3100/50000, loss = 0.042764 (0.030 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3100: train_loss = 0.032799\n",
            "2020-03-27 10:05:59: step 3120/50000, loss = 0.021508 (0.025 sec/batch)\n",
            "2020-03-27 10:06:00: step 3140/50000, loss = 0.030374 (0.026 sec/batch)\n",
            "2020-03-27 10:06:00: step 3160/50000, loss = 0.020291 (0.026 sec/batch)\n",
            "2020-03-27 10:06:01: step 3180/50000, loss = 0.031102 (0.022 sec/batch)\n",
            "2020-03-27 10:06:02: step 3200/50000, loss = 0.038862 (0.021 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3200: train_loss = 0.031666\n",
            "2020-03-27 10:06:03: step 3220/50000, loss = 0.045104 (0.026 sec/batch)\n",
            "2020-03-27 10:06:03: step 3240/50000, loss = 0.037268 (0.021 sec/batch)\n",
            "2020-03-27 10:06:04: step 3260/50000, loss = 0.032645 (0.024 sec/batch)\n",
            "2020-03-27 10:06:05: step 3280/50000, loss = 0.018567 (0.029 sec/batch)\n",
            "2020-03-27 10:06:05: step 3300/50000, loss = 0.045284 (0.021 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3300: train_loss = 0.033514\n",
            "2020-03-27 10:06:06: step 3320/50000, loss = 0.043598 (0.029 sec/batch)\n",
            "2020-03-27 10:06:07: step 3340/50000, loss = 0.040484 (0.062 sec/batch)\n",
            "2020-03-27 10:06:07: step 3360/50000, loss = 0.045248 (0.022 sec/batch)\n",
            "2020-03-27 10:06:08: step 3380/50000, loss = 0.048017 (0.029 sec/batch)\n",
            "2020-03-27 10:06:09: step 3400/50000, loss = 0.026784 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3400: train_loss = 0.031565\n",
            "2020-03-27 10:06:10: step 3420/50000, loss = 0.018548 (0.027 sec/batch)\n",
            "2020-03-27 10:06:10: step 3440/50000, loss = 0.024923 (0.026 sec/batch)\n",
            "2020-03-27 10:06:11: step 3460/50000, loss = 0.027019 (0.020 sec/batch)\n",
            "2020-03-27 10:06:11: step 3480/50000, loss = 0.029170 (0.030 sec/batch)\n",
            "2020-03-27 10:06:12: step 3500/50000, loss = 0.023032 (0.024 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3500: train_loss = 0.029406\n",
            "2020-03-27 10:06:13: step 3520/50000, loss = 0.037384 (0.029 sec/batch)\n",
            "2020-03-27 10:06:14: step 3540/50000, loss = 0.017911 (0.026 sec/batch)\n",
            "2020-03-27 10:06:14: step 3560/50000, loss = 0.034801 (0.032 sec/batch)\n",
            "2020-03-27 10:06:15: step 3580/50000, loss = 0.035651 (0.041 sec/batch)\n",
            "2020-03-27 10:06:15: step 3600/50000, loss = 0.030407 (0.021 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3600: train_loss = 0.028124\n",
            "2020-03-27 10:06:16: step 3620/50000, loss = 0.026907 (0.029 sec/batch)\n",
            "2020-03-27 10:06:17: step 3640/50000, loss = 0.033381 (0.020 sec/batch)\n",
            "2020-03-27 10:06:18: step 3660/50000, loss = 0.025764 (0.022 sec/batch)\n",
            "2020-03-27 10:06:18: step 3680/50000, loss = 0.023128 (0.031 sec/batch)\n",
            "2020-03-27 10:06:19: step 3700/50000, loss = 0.020843 (0.025 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3700: train_loss = 0.028919\n",
            "2020-03-27 10:06:20: step 3720/50000, loss = 0.037367 (0.027 sec/batch)\n",
            "2020-03-27 10:06:20: step 3740/50000, loss = 0.020478 (0.027 sec/batch)\n",
            "2020-03-27 10:06:21: step 3760/50000, loss = 0.019651 (0.022 sec/batch)\n",
            "2020-03-27 10:06:22: step 3780/50000, loss = 0.015320 (0.026 sec/batch)\n",
            "2020-03-27 10:06:22: step 3800/50000, loss = 0.030230 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3800: train_loss = 0.025786\n",
            "2020-03-27 10:06:23: step 3820/50000, loss = 0.067213 (0.022 sec/batch)\n",
            "2020-03-27 10:06:24: step 3840/50000, loss = 0.022542 (0.027 sec/batch)\n",
            "2020-03-27 10:06:25: step 3860/50000, loss = 0.024130 (0.030 sec/batch)\n",
            "2020-03-27 10:06:25: step 3880/50000, loss = 0.034053 (0.020 sec/batch)\n",
            "2020-03-27 10:06:26: step 3900/50000, loss = 0.025343 (0.022 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3900: train_loss = 0.027602\n",
            "2020-03-27 10:06:27: step 3920/50000, loss = 0.032232 (0.025 sec/batch)\n",
            "2020-03-27 10:06:27: step 3940/50000, loss = 0.023615 (0.025 sec/batch)\n",
            "2020-03-27 10:06:28: step 3960/50000, loss = 0.024605 (0.035 sec/batch)\n",
            "2020-03-27 10:06:29: step 3980/50000, loss = 0.039840 (0.027 sec/batch)\n",
            "2020-03-27 10:06:29: step 4000/50000, loss = 0.019422 (0.030 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4000: train_loss = 0.024292\n",
            "2020-03-27 10:06:30: step 4020/50000, loss = 0.020211 (0.028 sec/batch)\n",
            "2020-03-27 10:06:31: step 4040/50000, loss = 0.013331 (0.021 sec/batch)\n",
            "2020-03-27 10:06:32: step 4060/50000, loss = 0.045208 (0.027 sec/batch)\n",
            "2020-03-27 10:06:32: step 4080/50000, loss = 0.019123 (0.032 sec/batch)\n",
            "2020-03-27 10:06:33: step 4100/50000, loss = 0.026870 (0.031 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4100: train_loss = 0.024173\n",
            "2020-03-27 10:06:34: step 4120/50000, loss = 0.009216 (0.033 sec/batch)\n",
            "2020-03-27 10:06:34: step 4140/50000, loss = 0.012783 (0.029 sec/batch)\n",
            "2020-03-27 10:06:35: step 4160/50000, loss = 0.026789 (0.035 sec/batch)\n",
            "2020-03-27 10:06:36: step 4180/50000, loss = 0.029274 (0.029 sec/batch)\n",
            "2020-03-27 10:06:36: step 4200/50000, loss = 0.031977 (0.022 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4200: train_loss = 0.024689\n",
            "2020-03-27 10:06:37: step 4220/50000, loss = 0.032056 (0.025 sec/batch)\n",
            "2020-03-27 10:06:38: step 4240/50000, loss = 0.024467 (0.022 sec/batch)\n",
            "2020-03-27 10:06:38: step 4260/50000, loss = 0.016777 (0.021 sec/batch)\n",
            "2020-03-27 10:06:39: step 4280/50000, loss = 0.009469 (0.021 sec/batch)\n",
            "2020-03-27 10:06:40: step 4300/50000, loss = 0.026026 (0.032 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4300: train_loss = 0.022840\n",
            "2020-03-27 10:06:41: step 4320/50000, loss = 0.025474 (0.023 sec/batch)\n",
            "2020-03-27 10:06:41: step 4340/50000, loss = 0.022339 (0.029 sec/batch)\n",
            "2020-03-27 10:06:42: step 4360/50000, loss = 0.022023 (0.026 sec/batch)\n",
            "2020-03-27 10:06:42: step 4380/50000, loss = 0.019038 (0.020 sec/batch)\n",
            "2020-03-27 10:06:43: step 4400/50000, loss = 0.011266 (0.019 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4400: train_loss = 0.022673\n",
            "2020-03-27 10:06:44: step 4420/50000, loss = 0.014265 (0.022 sec/batch)\n",
            "2020-03-27 10:06:44: step 4440/50000, loss = 0.031657 (0.024 sec/batch)\n",
            "2020-03-27 10:06:45: step 4460/50000, loss = 0.019789 (0.032 sec/batch)\n",
            "2020-03-27 10:06:46: step 4480/50000, loss = 0.024644 (0.025 sec/batch)\n",
            "2020-03-27 10:06:46: step 4500/50000, loss = 0.011862 (0.024 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4500: train_loss = 0.021572\n",
            "2020-03-27 10:06:47: step 4520/50000, loss = 0.038026 (0.024 sec/batch)\n",
            "2020-03-27 10:06:48: step 4540/50000, loss = 0.025866 (0.023 sec/batch)\n",
            "2020-03-27 10:06:49: step 4560/50000, loss = 0.012679 (0.022 sec/batch)\n",
            "2020-03-27 10:06:49: step 4580/50000, loss = 0.033074 (0.020 sec/batch)\n",
            "2020-03-27 10:06:50: step 4600/50000, loss = 0.014353 (0.017 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4600: train_loss = 0.022415\n",
            "2020-03-27 10:06:51: step 4620/50000, loss = 0.018507 (0.022 sec/batch)\n",
            "2020-03-27 10:06:51: step 4640/50000, loss = 0.013956 (0.028 sec/batch)\n",
            "2020-03-27 10:06:52: step 4660/50000, loss = 0.018486 (0.023 sec/batch)\n",
            "2020-03-27 10:06:53: step 4680/50000, loss = 0.012238 (0.029 sec/batch)\n",
            "2020-03-27 10:06:53: step 4700/50000, loss = 0.017524 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4700: train_loss = 0.024493\n",
            "2020-03-27 10:06:54: step 4720/50000, loss = 0.021394 (0.026 sec/batch)\n",
            "2020-03-27 10:06:55: step 4740/50000, loss = 0.030663 (0.040 sec/batch)\n",
            "2020-03-27 10:06:55: step 4760/50000, loss = 0.012097 (0.031 sec/batch)\n",
            "2020-03-27 10:06:56: step 4780/50000, loss = 0.022797 (0.020 sec/batch)\n",
            "2020-03-27 10:06:57: step 4800/50000, loss = 0.048054 (0.019 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4800: train_loss = 0.022391\n",
            "2020-03-27 10:06:57: step 4820/50000, loss = 0.027501 (0.024 sec/batch)\n",
            "2020-03-27 10:06:58: step 4840/50000, loss = 0.017973 (0.034 sec/batch)\n",
            "2020-03-27 10:06:59: step 4860/50000, loss = 0.021529 (0.034 sec/batch)\n",
            "2020-03-27 10:06:59: step 4880/50000, loss = 0.033492 (0.022 sec/batch)\n",
            "2020-03-27 10:07:00: step 4900/50000, loss = 0.029704 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 4900: train_loss = 0.021051\n",
            "2020-03-27 10:07:01: step 4920/50000, loss = 0.027044 (0.028 sec/batch)\n",
            "2020-03-27 10:07:01: step 4940/50000, loss = 0.021987 (0.027 sec/batch)\n",
            "2020-03-27 10:07:02: step 4960/50000, loss = 0.011671 (0.030 sec/batch)\n",
            "2020-03-27 10:07:03: step 4980/50000, loss = 0.025697 (0.024 sec/batch)\n",
            "2020-03-27 10:07:03: step 5000/50000, loss = 0.043935 (0.032 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5000: train_loss = 0.021335\n",
            "2020-03-27 10:07:04: step 5020/50000, loss = 0.016154 (0.029 sec/batch)\n",
            "2020-03-27 10:07:05: step 5040/50000, loss = 0.028558 (0.065 sec/batch)\n",
            "2020-03-27 10:07:06: step 5060/50000, loss = 0.031615 (0.021 sec/batch)\n",
            "2020-03-27 10:07:06: step 5080/50000, loss = 0.026362 (0.027 sec/batch)\n",
            "2020-03-27 10:07:07: step 5100/50000, loss = 0.027071 (0.026 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5100: train_loss = 0.020439\n",
            "2020-03-27 10:07:08: step 5120/50000, loss = 0.023983 (0.026 sec/batch)\n",
            "2020-03-27 10:07:08: step 5140/50000, loss = 0.034889 (0.022 sec/batch)\n",
            "2020-03-27 10:07:09: step 5160/50000, loss = 0.028733 (0.024 sec/batch)\n",
            "2020-03-27 10:07:09: step 5180/50000, loss = 0.026181 (0.022 sec/batch)\n",
            "2020-03-27 10:07:10: step 5200/50000, loss = 0.019839 (0.022 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5200: train_loss = 0.019580\n",
            "2020-03-27 10:07:11: step 5220/50000, loss = 0.022012 (0.024 sec/batch)\n",
            "2020-03-27 10:07:12: step 5240/50000, loss = 0.024436 (0.030 sec/batch)\n",
            "2020-03-27 10:07:12: step 5260/50000, loss = 0.012722 (0.041 sec/batch)\n",
            "2020-03-27 10:07:13: step 5280/50000, loss = 0.030583 (0.027 sec/batch)\n",
            "2020-03-27 10:07:14: step 5300/50000, loss = 0.021264 (0.023 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5300: train_loss = 0.019774\n",
            "2020-03-27 10:07:14: step 5320/50000, loss = 0.013480 (0.019 sec/batch)\n",
            "2020-03-27 10:07:15: step 5340/50000, loss = 0.008237 (0.024 sec/batch)\n",
            "2020-03-27 10:07:16: step 5360/50000, loss = 0.013695 (0.022 sec/batch)\n",
            "2020-03-27 10:07:16: step 5380/50000, loss = 0.011634 (0.034 sec/batch)\n",
            "2020-03-27 10:07:17: step 5400/50000, loss = 0.008444 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5400: train_loss = 0.017769\n",
            "2020-03-27 10:07:18: step 5420/50000, loss = 0.017709 (0.027 sec/batch)\n",
            "2020-03-27 10:07:19: step 5440/50000, loss = 0.020227 (0.018 sec/batch)\n",
            "2020-03-27 10:07:19: step 5460/50000, loss = 0.028882 (0.038 sec/batch)\n",
            "2020-03-27 10:07:20: step 5480/50000, loss = 0.031778 (0.026 sec/batch)\n",
            "2020-03-27 10:07:20: step 5500/50000, loss = 0.017891 (0.021 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5500: train_loss = 0.018828\n",
            "2020-03-27 10:07:21: step 5520/50000, loss = 0.023005 (0.023 sec/batch)\n",
            "2020-03-27 10:07:22: step 5540/50000, loss = 0.011291 (0.023 sec/batch)\n",
            "2020-03-27 10:07:22: step 5560/50000, loss = 0.024967 (0.030 sec/batch)\n",
            "2020-03-27 10:07:23: step 5580/50000, loss = 0.008797 (0.029 sec/batch)\n",
            "2020-03-27 10:07:24: step 5600/50000, loss = 0.019882 (0.022 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5600: train_loss = 0.020179\n",
            "2020-03-27 10:07:25: step 5620/50000, loss = 0.014444 (0.021 sec/batch)\n",
            "2020-03-27 10:07:25: step 5640/50000, loss = 0.017078 (0.022 sec/batch)\n",
            "2020-03-27 10:07:26: step 5660/50000, loss = 0.034071 (0.026 sec/batch)\n",
            "2020-03-27 10:07:26: step 5680/50000, loss = 0.014150 (0.022 sec/batch)\n",
            "2020-03-27 10:07:27: step 5700/50000, loss = 0.019684 (0.025 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5700: train_loss = 0.018295\n",
            "2020-03-27 10:07:28: step 5720/50000, loss = 0.019721 (0.036 sec/batch)\n",
            "2020-03-27 10:07:29: step 5740/50000, loss = 0.004889 (0.021 sec/batch)\n",
            "2020-03-27 10:07:29: step 5760/50000, loss = 0.028358 (0.027 sec/batch)\n",
            "2020-03-27 10:07:30: step 5780/50000, loss = 0.034529 (0.025 sec/batch)\n",
            "2020-03-27 10:07:31: step 5800/50000, loss = 0.014546 (0.024 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5800: train_loss = 0.017906\n",
            "2020-03-27 10:07:31: step 5820/50000, loss = 0.024063 (0.031 sec/batch)\n",
            "2020-03-27 10:07:32: step 5840/50000, loss = 0.016997 (0.026 sec/batch)\n",
            "2020-03-27 10:07:33: step 5860/50000, loss = 0.024903 (0.025 sec/batch)\n",
            "2020-03-27 10:07:33: step 5880/50000, loss = 0.035262 (0.027 sec/batch)\n",
            "2020-03-27 10:07:34: step 5900/50000, loss = 0.025243 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 5900: train_loss = 0.017280\n",
            "2020-03-27 10:07:35: step 5920/50000, loss = 0.015384 (0.037 sec/batch)\n",
            "2020-03-27 10:07:36: step 5940/50000, loss = 0.017304 (0.024 sec/batch)\n",
            "2020-03-27 10:07:36: step 5960/50000, loss = 0.017212 (0.026 sec/batch)\n",
            "2020-03-27 10:07:37: step 5980/50000, loss = 0.032411 (0.019 sec/batch)\n",
            "2020-03-27 10:07:37: step 6000/50000, loss = 0.013923 (0.025 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6000: train_loss = 0.016805\n",
            "2020-03-27 10:07:38: step 6020/50000, loss = 0.011663 (0.031 sec/batch)\n",
            "2020-03-27 10:07:39: step 6040/50000, loss = 0.012247 (0.025 sec/batch)\n",
            "2020-03-27 10:07:40: step 6060/50000, loss = 0.019765 (0.022 sec/batch)\n",
            "2020-03-27 10:07:40: step 6080/50000, loss = 0.010120 (0.022 sec/batch)\n",
            "2020-03-27 10:07:41: step 6100/50000, loss = 0.040301 (0.029 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6100: train_loss = 0.016166\n",
            "2020-03-27 10:07:42: step 6120/50000, loss = 0.011828 (0.025 sec/batch)\n",
            "2020-03-27 10:07:43: step 6140/50000, loss = 0.018147 (0.023 sec/batch)\n",
            "2020-03-27 10:07:43: step 6160/50000, loss = 0.012146 (0.022 sec/batch)\n",
            "2020-03-27 10:07:44: step 6180/50000, loss = 0.014949 (0.027 sec/batch)\n",
            "2020-03-27 10:07:44: step 6200/50000, loss = 0.012553 (0.025 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6200: train_loss = 0.016038\n",
            "2020-03-27 10:07:45: step 6220/50000, loss = 0.016055 (0.020 sec/batch)\n",
            "2020-03-27 10:07:46: step 6240/50000, loss = 0.032918 (0.025 sec/batch)\n",
            "2020-03-27 10:07:47: step 6260/50000, loss = 0.015009 (0.037 sec/batch)\n",
            "2020-03-27 10:07:47: step 6280/50000, loss = 0.009853 (0.024 sec/batch)\n",
            "2020-03-27 10:07:48: step 6300/50000, loss = 0.010818 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6300: train_loss = 0.016385\n",
            "2020-03-27 10:07:49: step 6320/50000, loss = 0.028511 (0.026 sec/batch)\n",
            "2020-03-27 10:07:49: step 6340/50000, loss = 0.018555 (0.027 sec/batch)\n",
            "2020-03-27 10:07:50: step 6360/50000, loss = 0.023716 (0.023 sec/batch)\n",
            "2020-03-27 10:07:51: step 6380/50000, loss = 0.009977 (0.030 sec/batch)\n",
            "2020-03-27 10:07:51: step 6400/50000, loss = 0.022299 (0.066 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6400: train_loss = 0.016042\n",
            "2020-03-27 10:07:52: step 6420/50000, loss = 0.019861 (0.028 sec/batch)\n",
            "2020-03-27 10:07:53: step 6440/50000, loss = 0.019183 (0.023 sec/batch)\n",
            "2020-03-27 10:07:53: step 6460/50000, loss = 0.018152 (0.035 sec/batch)\n",
            "2020-03-27 10:07:54: step 6480/50000, loss = 0.018144 (0.021 sec/batch)\n",
            "2020-03-27 10:07:55: step 6500/50000, loss = 0.004754 (0.024 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6500: train_loss = 0.016036\n",
            "2020-03-27 10:07:56: step 6520/50000, loss = 0.004082 (0.026 sec/batch)\n",
            "2020-03-27 10:07:56: step 6540/50000, loss = 0.025702 (0.024 sec/batch)\n",
            "2020-03-27 10:07:57: step 6560/50000, loss = 0.022418 (0.032 sec/batch)\n",
            "2020-03-27 10:07:57: step 6580/50000, loss = 0.025532 (0.028 sec/batch)\n",
            "2020-03-27 10:07:58: step 6600/50000, loss = 0.012710 (0.024 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6600: train_loss = 0.017766\n",
            "2020-03-27 10:07:59: step 6620/50000, loss = 0.004914 (0.063 sec/batch)\n",
            "2020-03-27 10:07:59: step 6640/50000, loss = 0.011189 (0.028 sec/batch)\n",
            "2020-03-27 10:08:00: step 6660/50000, loss = 0.010283 (0.024 sec/batch)\n",
            "2020-03-27 10:08:01: step 6680/50000, loss = 0.006578 (0.029 sec/batch)\n",
            "2020-03-27 10:08:01: step 6700/50000, loss = 0.017521 (0.025 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6700: train_loss = 0.015847\n",
            "2020-03-27 10:08:02: step 6720/50000, loss = 0.005571 (0.033 sec/batch)\n",
            "2020-03-27 10:08:03: step 6740/50000, loss = 0.018201 (0.039 sec/batch)\n",
            "2020-03-27 10:08:04: step 6760/50000, loss = 0.015102 (0.022 sec/batch)\n",
            "2020-03-27 10:08:04: step 6780/50000, loss = 0.024685 (0.023 sec/batch)\n",
            "2020-03-27 10:08:05: step 6800/50000, loss = 0.001845 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6800: train_loss = 0.016663\n",
            "2020-03-27 10:08:06: step 6820/50000, loss = 0.011176 (0.026 sec/batch)\n",
            "2020-03-27 10:08:06: step 6840/50000, loss = 0.020279 (0.022 sec/batch)\n",
            "2020-03-27 10:08:07: step 6860/50000, loss = 0.009001 (0.031 sec/batch)\n",
            "2020-03-27 10:08:08: step 6880/50000, loss = 0.008680 (0.035 sec/batch)\n",
            "2020-03-27 10:08:08: step 6900/50000, loss = 0.011243 (0.016 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 6900: train_loss = 0.015955\n",
            "2020-03-27 10:08:09: step 6920/50000, loss = 0.009516 (0.029 sec/batch)\n",
            "2020-03-27 10:08:10: step 6940/50000, loss = 0.026669 (0.024 sec/batch)\n",
            "2020-03-27 10:08:10: step 6960/50000, loss = 0.006886 (0.027 sec/batch)\n",
            "2020-03-27 10:08:11: step 6980/50000, loss = 0.007435 (0.027 sec/batch)\n",
            "2020-03-27 10:08:12: step 7000/50000, loss = 0.020026 (0.038 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7000: train_loss = 0.015538\n",
            "2020-03-27 10:08:13: step 7020/50000, loss = 0.010744 (0.027 sec/batch)\n",
            "2020-03-27 10:08:13: step 7040/50000, loss = 0.019116 (0.056 sec/batch)\n",
            "2020-03-27 10:08:14: step 7060/50000, loss = 0.018349 (0.019 sec/batch)\n",
            "2020-03-27 10:08:14: step 7080/50000, loss = 0.029680 (0.021 sec/batch)\n",
            "2020-03-27 10:08:15: step 7100/50000, loss = 0.015938 (0.025 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7100: train_loss = 0.015863\n",
            "2020-03-27 10:08:16: step 7120/50000, loss = 0.016256 (0.026 sec/batch)\n",
            "2020-03-27 10:08:17: step 7140/50000, loss = 0.028135 (0.024 sec/batch)\n",
            "2020-03-27 10:08:17: step 7160/50000, loss = 0.011138 (0.020 sec/batch)\n",
            "2020-03-27 10:08:18: step 7180/50000, loss = 0.009778 (0.028 sec/batch)\n",
            "2020-03-27 10:08:18: step 7200/50000, loss = 0.007755 (0.022 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7200: train_loss = 0.014681\n",
            "2020-03-27 10:08:19: step 7220/50000, loss = 0.022949 (0.022 sec/batch)\n",
            "2020-03-27 10:08:20: step 7240/50000, loss = 0.018624 (0.021 sec/batch)\n",
            "2020-03-27 10:08:21: step 7260/50000, loss = 0.008700 (0.036 sec/batch)\n",
            "2020-03-27 10:08:21: step 7280/50000, loss = 0.007829 (0.028 sec/batch)\n",
            "2020-03-27 10:08:22: step 7300/50000, loss = 0.012207 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7300: train_loss = 0.016198\n",
            "2020-03-27 10:08:23: step 7320/50000, loss = 0.008097 (0.021 sec/batch)\n",
            "2020-03-27 10:08:23: step 7340/50000, loss = 0.008889 (0.020 sec/batch)\n",
            "2020-03-27 10:08:24: step 7360/50000, loss = 0.033541 (0.018 sec/batch)\n",
            "2020-03-27 10:08:24: step 7380/50000, loss = 0.009866 (0.020 sec/batch)\n",
            "2020-03-27 10:08:25: step 7400/50000, loss = 0.013848 (0.024 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7400: train_loss = 0.013867\n",
            "2020-03-27 10:08:26: step 7420/50000, loss = 0.014278 (0.025 sec/batch)\n",
            "2020-03-27 10:08:27: step 7440/50000, loss = 0.014452 (0.023 sec/batch)\n",
            "2020-03-27 10:08:27: step 7460/50000, loss = 0.003654 (0.020 sec/batch)\n",
            "2020-03-27 10:08:28: step 7480/50000, loss = 0.023076 (0.030 sec/batch)\n",
            "2020-03-27 10:08:28: step 7500/50000, loss = 0.012003 (0.022 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7500: train_loss = 0.016567\n",
            "2020-03-27 10:08:29: step 7520/50000, loss = 0.008587 (0.028 sec/batch)\n",
            "2020-03-27 10:08:30: step 7540/50000, loss = 0.010062 (0.036 sec/batch)\n",
            "2020-03-27 10:08:31: step 7560/50000, loss = 0.016003 (0.025 sec/batch)\n",
            "2020-03-27 10:08:31: step 7580/50000, loss = 0.010685 (0.027 sec/batch)\n",
            "2020-03-27 10:08:32: step 7600/50000, loss = 0.020492 (0.023 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7600: train_loss = 0.014362\n",
            "2020-03-27 10:08:33: step 7620/50000, loss = 0.021926 (0.020 sec/batch)\n",
            "2020-03-27 10:08:33: step 7640/50000, loss = 0.014310 (0.035 sec/batch)\n",
            "2020-03-27 10:08:34: step 7660/50000, loss = 0.015180 (0.026 sec/batch)\n",
            "2020-03-27 10:08:35: step 7680/50000, loss = 0.020750 (0.026 sec/batch)\n",
            "2020-03-27 10:08:35: step 7700/50000, loss = 0.009351 (0.027 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7700: train_loss = 0.014270\n",
            "2020-03-27 10:08:36: step 7720/50000, loss = 0.028632 (0.028 sec/batch)\n",
            "2020-03-27 10:08:37: step 7740/50000, loss = 0.013869 (0.028 sec/batch)\n",
            "2020-03-27 10:08:37: step 7760/50000, loss = 0.015366 (0.020 sec/batch)\n",
            "2020-03-27 10:08:38: step 7780/50000, loss = 0.009301 (0.024 sec/batch)\n",
            "2020-03-27 10:08:39: step 7800/50000, loss = 0.006283 (0.029 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7800: train_loss = 0.012590\n",
            "2020-03-27 10:08:40: step 7820/50000, loss = 0.015698 (0.017 sec/batch)\n",
            "2020-03-27 10:08:40: step 7840/50000, loss = 0.020272 (0.025 sec/batch)\n",
            "2020-03-27 10:08:41: step 7860/50000, loss = 0.020131 (0.024 sec/batch)\n",
            "2020-03-27 10:08:42: step 7880/50000, loss = 0.017342 (0.035 sec/batch)\n",
            "2020-03-27 10:08:42: step 7900/50000, loss = 0.013010 (0.033 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 7900: train_loss = 0.013130\n",
            "2020-03-27 10:08:43: step 7920/50000, loss = 0.023368 (0.026 sec/batch)\n",
            "2020-03-27 10:08:44: step 7940/50000, loss = 0.031089 (0.027 sec/batch)\n",
            "2020-03-27 10:08:44: step 7960/50000, loss = 0.006484 (0.065 sec/batch)\n",
            "2020-03-27 10:08:45: step 7980/50000, loss = 0.013201 (0.023 sec/batch)\n",
            "2020-03-27 10:08:46: step 8000/50000, loss = 0.005892 (0.022 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 8000: train_loss = 0.014154\n",
            "2020-03-27 10:08:46: step 8020/50000, loss = 0.002994 (0.022 sec/batch)\n",
            "2020-03-27 10:08:47: step 8040/50000, loss = 0.010182 (0.026 sec/batch)\n",
            "2020-03-27 10:08:48: step 8060/50000, loss = 0.012293 (0.024 sec/batch)\n",
            "2020-03-27 10:08:48: step 8080/50000, loss = 0.008892 (0.025 sec/batch)\n",
            "2020-03-27 10:08:49: step 8100/50000, loss = 0.012892 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 8100: train_loss = 0.013519\n",
            "2020-03-27 10:08:50: step 8120/50000, loss = 0.012285 (0.025 sec/batch)\n",
            "2020-03-27 10:08:50: step 8140/50000, loss = 0.013161 (0.020 sec/batch)\n",
            "2020-03-27 10:08:51: step 8160/50000, loss = 0.006106 (0.032 sec/batch)\n",
            "2020-03-27 10:08:52: step 8180/50000, loss = 0.007944 (0.021 sec/batch)\n",
            "2020-03-27 10:08:52: step 8200/50000, loss = 0.007760 (0.024 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 8200: train_loss = 0.013723\n",
            "2020-03-27 10:08:53: step 8220/50000, loss = 0.008049 (0.023 sec/batch)\n",
            "2020-03-27 10:08:54: step 8240/50000, loss = 0.019746 (0.021 sec/batch)\n",
            "2020-03-27 10:08:54: step 8260/50000, loss = 0.014539 (0.062 sec/batch)\n",
            "2020-03-27 10:08:55: step 8280/50000, loss = 0.019610 (0.020 sec/batch)\n",
            "2020-03-27 10:08:56: step 8300/50000, loss = 0.016046 (0.026 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 8300: train_loss = 0.014027\n",
            "2020-03-27 10:08:57: step 8320/50000, loss = 0.004093 (0.036 sec/batch)\n",
            "2020-03-27 10:08:57: step 8340/50000, loss = 0.013499 (0.032 sec/batch)\n",
            "2020-03-27 10:08:58: step 8360/50000, loss = 0.003804 (0.027 sec/batch)\n",
            "2020-03-27 10:08:58: step 8380/50000, loss = 0.011926 (0.032 sec/batch)\n",
            "2020-03-27 10:08:59: step 8400/50000, loss = 0.012366 (0.024 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 8400: train_loss = 0.012200\n",
            "2020-03-27 10:09:00: step 8420/50000, loss = 0.014142 (0.024 sec/batch)\n",
            "2020-03-27 10:09:01: step 8440/50000, loss = 0.015537 (0.021 sec/batch)\n",
            "2020-03-27 10:09:01: step 8460/50000, loss = 0.006472 (0.024 sec/batch)\n",
            "2020-03-27 10:09:02: step 8480/50000, loss = 0.007976 (0.021 sec/batch)\n",
            "2020-03-27 10:09:02: step 8500/50000, loss = 0.004434 (0.022 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 8500: train_loss = 0.013976\n",
            "2020-03-27 10:09:03: step 8520/50000, loss = 0.014851 (0.036 sec/batch)\n",
            "2020-03-27 10:09:04: step 8540/50000, loss = 0.012637 (0.031 sec/batch)\n",
            "2020-03-27 10:09:05: step 8560/50000, loss = 0.010078 (0.023 sec/batch)\n",
            "2020-03-27 10:09:05: step 8580/50000, loss = 0.018325 (0.025 sec/batch)\n",
            "2020-03-27 10:09:06: step 8600/50000, loss = 0.011834 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 8600: train_loss = 0.014096\n",
            "2020-03-27 10:09:07: step 8620/50000, loss = 0.016065 (0.028 sec/batch)\n",
            "2020-03-27 10:09:07: step 8640/50000, loss = 0.010920 (0.029 sec/batch)\n",
            "2020-03-27 10:09:08: step 8660/50000, loss = 0.016560 (0.021 sec/batch)\n",
            "2020-03-27 10:09:08: step 8680/50000, loss = 0.008916 (0.029 sec/batch)\n",
            "2020-03-27 10:09:09: step 8700/50000, loss = 0.021120 (0.023 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 8700: train_loss = 0.012829\n",
            "2020-03-27 10:09:10: step 8720/50000, loss = 0.009335 (0.023 sec/batch)\n",
            "2020-03-27 10:09:11: step 8740/50000, loss = 0.018927 (0.018 sec/batch)\n",
            "2020-03-27 10:09:11: step 8760/50000, loss = 0.015428 (0.033 sec/batch)\n",
            "2020-03-27 10:09:12: step 8780/50000, loss = 0.018369 (0.026 sec/batch)\n",
            "2020-03-27 10:09:12: step 8800/50000, loss = 0.009810 (0.021 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 8800: train_loss = 0.013547\n",
            "2020-03-27 10:09:13: step 8820/50000, loss = 0.012259 (0.028 sec/batch)\n",
            "2020-03-27 10:09:14: step 8840/50000, loss = 0.016685 (0.023 sec/batch)\n",
            "2020-03-27 10:09:15: step 8860/50000, loss = 0.012060 (0.029 sec/batch)\n",
            "2020-03-27 10:09:15: step 8880/50000, loss = 0.008026 (0.023 sec/batch)\n",
            "2020-03-27 10:09:16: step 8900/50000, loss = 0.006644 (0.020 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 8900: train_loss = 0.013163\n",
            "2020-03-27 10:09:17: step 8920/50000, loss = 0.008684 (0.034 sec/batch)\n",
            "2020-03-27 10:09:18: step 8940/50000, loss = 0.004675 (0.026 sec/batch)\n",
            "2020-03-27 10:09:18: step 8960/50000, loss = 0.018705 (0.022 sec/batch)\n",
            "2020-03-27 10:09:19: step 8980/50000, loss = 0.009961 (0.024 sec/batch)\n",
            "2020-03-27 10:09:19: step 9000/50000, loss = 0.008739 (0.030 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 9000: train_loss = 0.013527\n",
            "2020-03-27 10:09:20: step 9020/50000, loss = 0.023733 (0.027 sec/batch)\n",
            "2020-03-27 10:09:21: step 9040/50000, loss = 0.007791 (0.035 sec/batch)\n",
            "2020-03-27 10:09:22: step 9060/50000, loss = 0.006912 (0.024 sec/batch)\n",
            "2020-03-27 10:09:22: step 9080/50000, loss = 0.008505 (0.027 sec/batch)\n",
            "2020-03-27 10:09:23: step 9100/50000, loss = 0.024918 (0.028 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 9100: train_loss = 0.012174\n",
            "2020-03-27 10:09:24: step 9120/50000, loss = 0.006894 (0.033 sec/batch)\n",
            "2020-03-27 10:09:24: step 9140/50000, loss = 0.007660 (0.028 sec/batch)\n",
            "2020-03-27 10:09:25: step 9160/50000, loss = 0.018839 (0.030 sec/batch)\n",
            "2020-03-27 10:09:26: step 9180/50000, loss = 0.006983 (0.028 sec/batch)\n",
            "2020-03-27 10:09:26: step 9200/50000, loss = 0.037059 (0.018 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 9200: train_loss = 0.014344\n",
            "2020-03-27 10:09:27: step 9220/50000, loss = 0.008375 (0.033 sec/batch)\n",
            "2020-03-27 10:09:28: step 9240/50000, loss = 0.010747 (0.027 sec/batch)\n",
            "2020-03-27 10:09:28: step 9260/50000, loss = 0.016820 (0.025 sec/batch)\n",
            "2020-03-27 10:09:29: step 9280/50000, loss = 0.015215 (0.025 sec/batch)\n",
            "2020-03-27 10:09:30: step 9300/50000, loss = 0.010763 (0.036 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 9300: train_loss = 0.013470\n",
            "2020-03-27 10:09:30: step 9320/50000, loss = 0.017121 (0.022 sec/batch)\n",
            "2020-03-27 10:09:31: step 9340/50000, loss = 0.015183 (0.027 sec/batch)\n",
            "2020-03-27 10:09:32: step 9360/50000, loss = 0.015887 (0.021 sec/batch)\n",
            "2020-03-27 10:09:32: step 9380/50000, loss = 0.007288 (0.020 sec/batch)\n",
            "2020-03-27 10:09:33: step 9400/50000, loss = 0.009696 (0.025 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 9400: train_loss = 0.013707\n",
            "2020-03-27 10:09:34: step 9420/50000, loss = 0.004957 (0.027 sec/batch)\n",
            "2020-03-27 10:09:34: step 9440/50000, loss = 0.006652 (0.021 sec/batch)\n",
            "2020-03-27 10:09:35: step 9460/50000, loss = 0.017826 (0.021 sec/batch)\n",
            "2020-03-27 10:09:36: step 9480/50000, loss = 0.016418 (0.031 sec/batch)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-235-48246a23682c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-232-a292b2e7a63f>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batch, eval)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unozGBA9MHaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}