{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab5_SequenceTagging.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB1YDdMM8TIe",
        "colab_type": "text"
      },
      "source": [
        "# Lab 5. Sequence Tagging with LSTM\n",
        "\n",
        "Last time we looked at the text classification with the [Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network). While this concept worked quite well for this task, it would be very difficult to apply it to the sequence tagging task. \n",
        "\n",
        "First, let's find out what **sequence tagging** or **sequence labeling** stands for. Sequence tagging is a task of assigning some tag, or label, to each element in the sequence. It can be a name entitiy tag or a pos-tag. You can already see that CNNs are not really applicable for this task since they transform the whole input into some feature vector. \n",
        "\n",
        "To assign a label for each element in a sequence, we need to apply some transformation for each element as well. However, the _context_ can be very important for choosing the correct tag. For example, a noun is usually preceded by an article. We can achieve this with [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network).\n",
        "\n",
        "To have a more illustative example, let's look at the picture below:\n",
        "\n",
        "![image.png](img1.png)\n",
        "\n",
        "Here, a square with the diagonal stripes is an **RNN unit**. You may notice, that it has one input and two outputs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUerAOp1Ctj4",
        "colab_type": "text"
      },
      "source": [
        "To better understand what these outputs mean and how we can use them, let's look inside one of the RNN units, or **RNN cells**.\n",
        "\n",
        "![RNN cell](http://dprogrammer.org/wp-content/uploads/2019/04/RNN_Core2-768x491.png)\n",
        "\n",
        "This simple cell consists of only one TanH activation function (it can be replaced with ReLU). As an input, it takes the **hidden state** from the previous cell and the input itself which can be a word embedding, for example. In this case, the output hidden state is the same as the output of the cell. Passing this hidden state as an input to the next cell will transfer some information from the context.\n",
        "\n",
        "Now, the cell above is very simple. There are more efficient variations like [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) or [GRU](https://en.wikipedia.org/wiki/Gated_recurrent_unit). \n",
        "\n",
        "Let's look inside an LSTM cell.\n",
        "\n",
        "<a title=\"Guillaume Chevalier / CC BY (https://creativecommons.org/licenses/by/4.0)\" href=\"https://commons.wikimedia.org/wiki/File:The_LSTM_cell.png\"><img width=\"512\" alt=\"The LSTM cell\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/512px-The_LSTM_cell.png\"></a>\n",
        "\n",
        "We can see, that it has more inputs and outputs, as well as more layers inside the cell, but the main concept stays the same. This time $h_t$ is a hidden state at the timestamp $t$, and $c_t$ is a **cell state** at the timestep $t$. \n",
        "\n",
        "As we are going to impliment an LSTM model in this Lab, we need to understand how to route the inputs and outputs correctly. The inside magic of the network will be done for us by PyTorch. Our job is just to plug in the wires into the correct slots.\n",
        "\n",
        "You can also read more about LSTMs [in this blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVRSVyJhH9Xt",
        "colab_type": "text"
      },
      "source": [
        "## POS tagging for your language\n",
        "\n",
        "In this Lab, you are going to create a pos tagger for the language of your choice. I encourage you to try to use your native language. \n",
        "\n",
        "First, let's import everything that we might need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yygt6gOJevIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "from typing import List, Dict\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIDOXHRpIWAH",
        "colab_type": "text"
      },
      "source": [
        "We are going to need the pretrained word vectors. For this network, we can use the [Fasttext](https://fasttext.cc/docs/en/crawl-vectors.html) vectors since they have a lot of languages available. I am going to download the vectors for my native language which is Russian. To download the vectors for your language, you need to replace the link with your **text vectors** from the table at the link above.\n",
        "\n",
        "For example, that link for Estonian word vectors will be: `https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.et.300.vec.gz`\n",
        "\n",
        "Run the two cells below to download and unpack the vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1dBJUSQe6eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J3TIUs0fc0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip cc.ru.300.vec.gz\n",
        "!mkdir vector_cache/\n",
        "!mv cc.ru.300.vec vector_cache/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxuSASQaJNAz",
        "colab_type": "text"
      },
      "source": [
        "Next, we need the data to train on. For this task, we are going to use [Universal Dependencies](https://universaldependencies.org/) data. It has labelled corpora for morphological tagging and syntax parsing for over than 70 languages. You need to choose your language from the official UD page, choose the treebank that you like and follow the GitHub link to it. Then, from GitHub, copy the link from the green \"Clone or download\" button and replace it in the cell below. \n",
        "\n",
        "Also, replace the name of your treebank in the `!mv` command.\n",
        "\n",
        "For example, if I choose the EDT treebank for Estonian from [here](https://universaldependencies.org/#estonian-treebanks), the GitHub link is going to be `https://github.com/UniversalDependencies/UD_Estonian-EDT.git` and the name of the treebank is `UD_Estonian-EDT`, which is the name of the repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJBLZXIEgbbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_Russian-Taiga.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJqCd9yQggJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data/\n",
        "!mv UD_Russian-Taiga/ data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8KqQMYpKzUJ",
        "colab_type": "text"
      },
      "source": [
        "This part is moslty the same as in the [Lab 4](https://github.com/501Good/tartu-nlp-2020/blob/master/labs/lab4/Lab4_TextClassificationCNN.ipynb). \n",
        "\n",
        "**Don't forget to change the `VEC_PATH` and `DATA_PATH` variables to match your data!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb4fsehMgkBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD = '<PAD>'\n",
        "PAD_ID = 0\n",
        "UNK = '<UNK>'\n",
        "UNK_ID = 1\n",
        "VOCAB_PREFIX = [PAD, UNK]\n",
        "\n",
        "VEC_PATH = Path('vector_cache') / 'cc.ru.300.vec'\n",
        "DATA_PATH = Path('data') / 'UD_Russian-Taiga'\n",
        "MAX_VOCAB = 25000\n",
        "\n",
        "batch_size = 64\n",
        "validation_split = .3\n",
        "shuffle_dataset = True\n",
        "random_seed = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5DyDzC6ORzY",
        "colab_type": "text"
      },
      "source": [
        "## Building the Vocabs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGsPh253hDIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaseVocab:\n",
        "    def __init__(self, data, idx=0, lower=False):\n",
        "        self.data = data\n",
        "        self.lower = lower\n",
        "        self.idx = idx\n",
        "        self.build_vocab()\n",
        "        \n",
        "    def normalize_unit(self, unit):\n",
        "        if self.lower:\n",
        "            return unit.lower()\n",
        "        else:\n",
        "            return unit\n",
        "        \n",
        "    def unit2id(self, unit):\n",
        "        unit = self.normalize_unit(unit)\n",
        "        if unit in self._unit2id:\n",
        "            return self._unit2id[unit]\n",
        "        else:\n",
        "            return self._unit2id[UNK]\n",
        "    \n",
        "    def id2unit(self, id):\n",
        "        return self._id2unit[id]\n",
        "    \n",
        "    def map(self, units):\n",
        "        return [self.unit2id(unit) for unit in units]\n",
        "\n",
        "    def unmap(self, ids):\n",
        "        return [self.id2unit(idx) for idx in ids]\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        NotImplementedError()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._unit2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMvi6s3lsFA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PretrainedWordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        self._id2unit = VOCAB_PREFIX + self.data\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4g3xj6iOXvW",
        "colab_type": "text"
      },
      "source": [
        "In addition to the pretrained vocab, we are going to create a word vocab that we are going to use to encode the words and the pos tags. It extends the same `BaseVocab` class but has a different way of initialization. \n",
        "\n",
        "The `self.data` in this vocab is going to be a list of list of lists, like that: \n",
        "\n",
        "`[[['I', 'PRON'], ['like', 'VERB'], ['cats', 'NOUN'], ['.', 'PUNCT']]]`.\n",
        "\n",
        "We also added the `idx` parameter for the `BaseVocab` to select the appropriate element for each word array in out data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9mn3bWHhXER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        if self.lower:\n",
        "            counter = Counter([w[self.idx].lower() for sent in self.data for w in sent])\n",
        "        else:\n",
        "            counter = Counter([w[self.idx] for sent in self.data for w in sent])\n",
        "\n",
        "        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPOQxl5mPdK8",
        "colab_type": "text"
      },
      "source": [
        "## Building the Dataset\n",
        "\n",
        "The `Pretrain` class is the same as in the [Lab 4](https://github.com/501Good/tartu-nlp-2020/blob/master/labs/lab4/Lab4_TextClassificationCNN.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOefas-mheLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pretrain:\n",
        "    def __init__(self, vec_filename, max_vocab=-1):\n",
        "        self._vec_filename = vec_filename\n",
        "        self._max_vocab = max_vocab\n",
        "        \n",
        "    @property\n",
        "    def vocab(self):\n",
        "        if not hasattr(self, '_vocab'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._vocab\n",
        "    \n",
        "    @property\n",
        "    def emb(self):\n",
        "        if not hasattr(self, '_emb'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._emb\n",
        "        \n",
        "    def read(self):\n",
        "        if self._vec_filename is None:\n",
        "            raise Exception(\"Vector file is not provided.\")\n",
        "        print(f\"Reading pretrained vectors from {self._vec_filename}...\")\n",
        "        \n",
        "        words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n",
        "        \n",
        "        if failed > 0: # recover failure\n",
        "            emb = emb[:-failed]\n",
        "        if len(emb) - len(VOCAB_PREFIX) != len(words):\n",
        "            raise Exception(\"Loaded number of vectors does not match number of words.\")\n",
        "            \n",
        "        # Use a fixed vocab size\n",
        "        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):\n",
        "            words = words[:self._max_vocab - len(VOCAB_PREFIX)]\n",
        "            emb = emb[:self._max_vocab]\n",
        "                \n",
        "        vocab = PretrainedWordVocab(words, lower=True)\n",
        "        \n",
        "        return vocab, emb\n",
        "        \n",
        "    def read_from_file(self, filename, open_func=open):\n",
        "        \"\"\"\n",
        "        Open a vector file using the provided function and read from it.\n",
        "        \"\"\"\n",
        "        first = True\n",
        "        words = []\n",
        "        failed = 0\n",
        "        with open_func(filename, 'rb') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    line = line.decode()\n",
        "                except UnicodeDecodeError:\n",
        "                    failed += 1\n",
        "                    continue\n",
        "                if first:\n",
        "                    # the first line contains the number of word vectors and the dimensionality\n",
        "                    first = False\n",
        "                    line = line.strip().split(' ')\n",
        "                    rows, cols = [int(x) for x in line]\n",
        "                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n",
        "                    continue\n",
        "\n",
        "                line = line.rstrip().split(' ')\n",
        "                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n",
        "                words.append(' '.join(line[:-cols]))\n",
        "        return words, emb, failed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygF5ivdMPwmK",
        "colab_type": "text"
      },
      "source": [
        "Since our data is in the [CoNLL-U format](https://universaldependencies.org/format.html) that is basically as tab-separated text document, we are going to create `Word`, `Sentence`, `Document` classes to store all the information about words, sentences, documents.\n",
        "\n",
        "`Word` class is just going to store all 10 fields with their respecitve names. These are `id`, `text`, `lemma`, `upos`, `xpos`, `feats`, `head`, `deprel`, `deps`, and `misc`. For this model, we are only interested in `text` and `upos` fields, but it's good idea to have all the information nicely stored in case we want to expand our model (that you are going to do in the homework).\n",
        "\n",
        "`Sentence` class just stores the list of `Word` objects.\n",
        "\n",
        "`Document` class stores the list of `Sentence` objects and actually reads the text file in the `load_conll` method. `get` method returns a document as a list of lists of lists that we will need to build a `WordVocab`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8i2PuW6iafP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIELD_NUM = 10\n",
        "\n",
        "class Word:\n",
        "    def __init__(self, word: List[str]):\n",
        "        self._id = word[0]\n",
        "        self._text = word[1]\n",
        "        self._lemma = word[2]\n",
        "        self._upos = word[3]\n",
        "        self._xpos = word[4]\n",
        "        self._feats = word[5]\n",
        "        self._head = word[6]\n",
        "        self._deprel = word[7]\n",
        "        self._deps = word[8]\n",
        "        self._misc = word[9]\n",
        "\n",
        "    @property\n",
        "    def id(self):\n",
        "        return self._id\n",
        "\n",
        "    @property\n",
        "    def text(self):\n",
        "        return self._text\n",
        "\n",
        "    @property\n",
        "    def lemma(self):\n",
        "        return self._lemma\n",
        "\n",
        "    @property\n",
        "    def upos(self):\n",
        "        return self._upos\n",
        "\n",
        "    @property\n",
        "    def xpos(self):\n",
        "        return self._xpos\n",
        "\n",
        "    @property\n",
        "    def feats(self):\n",
        "        return self._feats\n",
        "\n",
        "    @property\n",
        "    def head(self):\n",
        "        return self._head\n",
        "\n",
        "    @property\n",
        "    def deprel(self):\n",
        "        return self._deprel\n",
        "\n",
        "    @property\n",
        "    def deps(self):\n",
        "        return self._deps\n",
        "\n",
        "    @property\n",
        "    def misc(self):\n",
        "        return self._misc\n",
        "\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, words: List[List[str]]):\n",
        "        self._words = [Word(w) for w in words]\n",
        "\n",
        "    @property\n",
        "    def words(self):\n",
        "        return self._words\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, file_path):\n",
        "        self._sentences = []\n",
        "        self.load_conll(open(file_path, encoding='utf-8'))\n",
        "\n",
        "\n",
        "    def load_conll(self, f, ignore_gapping=True):\n",
        "        \"\"\" Load the file or string into the CoNLL-U format data.\n",
        "        Input: file or string reader, where the data is in CoNLL-U format.\n",
        "        Output: a list of list of list for each token in each sentence in the data, where the innermost list represents \n",
        "        all fields of a token.\n",
        "\n",
        "        Taken and modified from Stanza: https://github.com/stanfordnlp/stanza/blob/master/stanza/utils/conll.py\n",
        "        Stanza is released under the Apache License, Version 2.0.\n",
        "        \"\"\"\n",
        "        # f is open() or io.StringIO()\n",
        "        doc, sent = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if len(line) == 0:\n",
        "                if len(sent) > 0:\n",
        "                    doc.append(Sentence(sent))\n",
        "                    sent = []\n",
        "            else:\n",
        "                if line.startswith('#'): # skip comment line\n",
        "                    continue\n",
        "                array = line.split('\\t')\n",
        "                if ignore_gapping and '.' in array[0]:\n",
        "                    continue\n",
        "                assert len(array) == FIELD_NUM, \\\n",
        "                        f\"Cannot parse CoNLL line: expecting {FIELD_NUM} fields, {len(array)} found.\"\n",
        "                sent += [array]\n",
        "        if len(sent) > 0:\n",
        "            doc.append(Sentence(sent))\n",
        "        self._sentences = doc\n",
        "\n",
        "    \n",
        "    @property\n",
        "    def sentences(self):\n",
        "        return self._sentences\n",
        "\n",
        "\n",
        "    def get(self, fields, as_sentences=False):\n",
        "        \"\"\"Taken and modified from Stanza: https://github.com/stanfordnlp/stanza/blob/master/stanza/models/common/doc.py\n",
        "        Stanza is released under the Apache License, Version 2.0.\n",
        "        \"\"\"\n",
        "        assert isinstance(fields, list), \"Must provide field names as a list.\"\n",
        "        assert len(fields) >= 1, \"Must have at least one field.\"\n",
        "\n",
        "        results = []\n",
        "        for sentence in self.sentences:\n",
        "            cursent = []\n",
        "            units = sentence.words\n",
        "            for unit in units:\n",
        "                if len(fields) == 1:\n",
        "                    cursent += [getattr(unit, fields[0])]\n",
        "                else:\n",
        "                    cursent += [[getattr(unit, field) for field in fields]]\n",
        "\n",
        "            # decide whether append the results as a sentence or a whole list\n",
        "            if as_sentences:\n",
        "                results.append(cursent)\n",
        "            else:\n",
        "                results += cursent\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azsMK8kTSZfO",
        "colab_type": "text"
      },
      "source": [
        "Finally, to read the data, we are going to create `CONLLUDataset` class that implements the [`Dataset`](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset) class from PyTorch.\n",
        "\n",
        "First, we are going to initialize the pretrain vocab by calling `pretrain.vocab` and saving it in the `self.pretrain_vocab` variable. \n",
        "\n",
        "Next, we read the data with `self.load_doc()` method. We are going to pass a `Document` object that we created before to this method.\n",
        "\n",
        "Then, we initialize the vocabs with the data that we just read with the `self.init_vocab` method and store them inside the `self.vocab` variable.\n",
        "\n",
        "Finally, we preprocess the data with `self.preprocess` method. In this method, we are turning the words and upos tags into the respective indices in their vocabs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7UVFATAhsEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CONLLUDataset(Dataset):\n",
        "    def __init__(self, doc: Document, pretrain: Pretrain, \n",
        "                 vocab: Dict[str, BaseVocab] = None, test: bool = False):\n",
        "        self.pretrain_vocab = pretrain.vocab\n",
        "        self.test = test\n",
        "        data = self.load_doc(doc)\n",
        "\n",
        "        if vocab is None:\n",
        "            self.vocab = self.init_vocab(data)\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "        self.data = self.preprocess(data, self.vocab, self.pretrain_vocab)\n",
        "\n",
        "    def init_vocab(self, data: List) -> Dict:\n",
        "        wordvocab = WordVocab(data, idx=0)\n",
        "        uposvocab = WordVocab(data, idx=1)\n",
        "        vocab = {\n",
        "            'word': wordvocab,\n",
        "            'upos': uposvocab}\n",
        "        return vocab\n",
        "\n",
        "    def preprocess(self, data: List, vocab: Dict[str, BaseVocab], \n",
        "                   pretrain_vocab: PretrainedWordVocab) -> List[List[int]]:\n",
        "        processed = []\n",
        "        for sent in data:\n",
        "            processed_sent = [vocab['word'].map([w[0] for w in sent])]\n",
        "            processed_sent += [vocab['upos'].map([w[1] for w in sent])]\n",
        "            processed_sent += [pretrain_vocab.map([w[0].lower() for w in sent])]\n",
        "            processed.append(processed_sent)\n",
        "        return processed\n",
        "        \n",
        "    def load_doc(self, doc: Document) -> List:\n",
        "        data = doc.get(['text', 'upos'], as_sentences=True)\n",
        "        return data\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9L6KBcOkVu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QraxfFjKs-jZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_doc = Document(DATA_PATH / 'ru_taiga-ud-train.conllu')\n",
        "train_dataset = CONLLUDataset(train_doc, pretrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAM3HOs7ViFE",
        "colab_type": "text"
      },
      "source": [
        "When creating a development data set, we should use the same vocabs as in the training set so that the mappings from words to ids are the same. We can extract the vocabs from the `train_dataset` by addressing the `vocab` attribute. The pretrained vocab is stored in the `pretrain` object already."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdk0QJJheLpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = train_dataset.vocab\n",
        "dev_doc = Document(DATA_PATH / 'ru_taiga-ud-dev.conllu')\n",
        "dev_dataset = CONLLUDataset(dev_doc, pretrain, vocab=vocab, test=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBZSDFv6t_-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-A6qEIiWPQn",
        "colab_type": "text"
      },
      "source": [
        "As usual, we need to specify the `collate_fn` for the [`DataLoader`](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader). \n",
        "\n",
        "Last time, we padded all the sentences with `0` by hand to make them all equal length and allow constructing the batch matrix. We can do that now but it will create a lot of unnecessary computations for the paddings. \n",
        "\n",
        "Luckily for us, PyTorch has a [`PackedSequence`](https://pytorch.org/docs/stable/nn.html?highlight=packedsequence#torch.nn.utils.rnn.PackedSequence) class as well as [`pad_sequence`](https://pytorch.org/docs/stable/nn.html?highlight=pad_sequence#torch.nn.utils.rnn.pad_sequence), [`pack_padded_sequence`](https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence) and [`pad_packed_sequence`](https://pytorch.org/docs/stable/nn.html?highlight=pad_packed_sequence#torch.nn.utils.rnn.pad_packed_sequence) objects. Let's see what each of them does and how we can benefit from them in our model. \n",
        "\n",
        "`pad_sequence` function is prettry straightforward. As the name suggests, it pads the sequences to the same length. So, let's say we have $64$ sentences in out batch, all of which are of different length. `pad_sequence` is going to take them as a list of 64 tensors and return a tensor of size $B \\times T$, where $B$ is the batch size and $T$ is the length of the longest sentence in the batch.\n",
        "\n",
        "`pack_padded_sequence` function is going to take our padded sequence as the original lengths of the sequences (i.e. before paddind) and return a `PackedSequence` object that we are going to feed to the neural network. As you may notice, we saved the lengths of each sequence in the `sent_lens` before padding. This `PackedSequence` object is going to ensure that our model doesn't perform tons of useless computations on the padding symbols. We can use `PackedSequence` object in the loss function as well.\n",
        "\n",
        "`pad_packed_sequence` function does the complete opposite of what `pack_padded_sequence` does. It takes a `PackedSequence` object as an input and returns a padded tensor of size $B \\times T$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbhf2uYHvcJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_collate(batch):\n",
        "    (sents, upos, pretrained) = zip(*batch)\n",
        "\n",
        "    sent_lens = [len(s) for s in sents]\n",
        "\n",
        "    sents = [torch.LongTensor(w).to(device) for w in sents]\n",
        "    upos = [torch.LongTensor(u).to(device) for u in upos]\n",
        "    pretrained = [torch.LongTensor(p).to(device) for p in pretrained]\n",
        "\n",
        "    sent_pad = pad_sequence(sents, batch_first=True, padding_value=PAD_ID)\n",
        "    upos_pad = pad_sequence(upos, batch_first=True, padding_value=PAD_ID)\n",
        "    pretrained_pad = pad_sequence(pretrained, batch_first=True, padding_value=PAD_ID)\n",
        "\n",
        "    sent_pad = sent_pad.to(device)\n",
        "    upos_pad = upos_pad.to(device)\n",
        "    pretrained_pad = pretrained_pad.to(device)\n",
        "\n",
        "    return sent_pad, upos_pad, pretrained_pad, sent_lens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMNbcGrQ7dGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li5RRsb6IrJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLNjeeQUa0U3",
        "colab_type": "text"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "To better understand the model architecture, it's better to make a visual representation of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fv9BK0qc0IA",
        "colab_type": "text"
      },
      "source": [
        "![image.png](img2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1JJgSQmc1tO",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the model will have two embedding layers, one is going to be trainable and another one is going to stay frozen and contained already pretrained word vectors that we downloaded before. \n",
        "\n",
        "Then, we have a BiLSTM layer which stands for a Bidirectional LSTM. You can see it simply as two LSTM layers but one is passing the hidden states from left to right and another one from right to left. You can see the comparison of LSTM and BiLSTM on the picture below:\n",
        "\n",
        "![BiLSTM vs LSTM](https://www.researchgate.net/profile/Arvind_Mohan/publication/324769532/figure/fig2/AS:619510805561344@1524714294669/LSTM-and-BiLSTM-Architectures.png)\n",
        "Image from: *Mohan, Arvind & Gaitonde, Datta. (2018). A Deep Learning based Approach to Reduced Order Modeling for Turbulent Flow Control using LSTM Neural Networks.*\n",
        "\n",
        "Finally, we have two [Linear layers](https://pytorch.org/docs/stable/nn.html?highlight=linear#torch.nn.Linear) with a [ReLU activation function](https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU) in-between that are going to map the output of the LSTM layer to the probabilities of each pos tag. Since each word can have one and only one pos tag, we will just take the index of the maximum probability to make the prediction.\n",
        "\n",
        "To optimize the model, we use [cross-entropy loss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropy#torch.nn.CrossEntropyLoss) fucntion. Since we give a `PackedSequence` as an input to the model, the output will also be a `PackedSequence`. That means that we also need to transform the correct tags to a `PackedSequence` and take the `data` attribute from it to match the output of he model.\n",
        "\n",
        "Also, the model has many tricky transformations to fit the data into the correct modules. Let's go line by line in the `forward()` method.\n",
        "\n",
        "First, we create an `inputs = []` list to store our inputs. Then, we get the trainable word embeddings for our first input with `self.word_emb`. We repeat the same for the `self.pretrained_emb` with the only difference that we put the embeddings through the linear layer to transform them to the dimension that we want.\n",
        "\n",
        "After that, we need to concatenate the inputs. We basically add a pretrained embedding to the end of the word embedding. Finally, we pack the inputs to put the into the LSTM. Pay attention that we specify `enforce_sorted = False` flag because we didn't sort the sentences by length inside of the batch. \n",
        "\n",
        "Now, we can finally put all of that into the `self.lstm` layer. We take the `data` from the output, which is also a `PackedSequence`. We do it because we don't want to perform extra operations on the padding in the linear layers and `PackedSequence` inputs are only supported by RNNs. To overcome this, we create a `PackedSequence` from the linear output manually. Even though the PyTorch documentation [strongly recommends against creaing a `PackedSequence` maunally](https://pytorch.org/docs/stable/nn.html?highlight=pack_sequence#torch.nn.utils.rnn.PackedSequence), it seems to be the only workaround for now. It is important that you don't forget to put all the parameters from the original packed sequence like `batch_sizes`, `sorted_indices`, and `unsorted_indices`. If you don't specify the last two, `pad_packed_sequence()` function is going to sort your predictions automatically, and thus you will lose the correct order of your predictions and they become practically useless.\n",
        "\n",
        "Finally, we take the index with maximum probability for each prediction which is going to be the index of the pos tag in the vocabulary. The last step is calculate the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N-xVFW28FD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tagger(nn.Module):\n",
        "    def __init__(self, vocab: Dict[str, BaseVocab], word_emb_dim: int,\n",
        "                 transformed_dim: int, emb_matrix: np.ndarray, hidden_dim: int,\n",
        "                 upos_clf_hidden_dim: int, num_layers: int, dropout: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        input_size = 0\n",
        "\n",
        "        self.word_emb = nn.Embedding(len(vocab['word']), word_emb_dim, padding_idx=0)\n",
        "        input_size += word_emb_dim\n",
        "\n",
        "        self.pretrained_emb = nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True)\n",
        "        self.trans_pretrained = nn.Linear(emb_matrix.shape[1], transformed_dim, bias=False)\n",
        "        input_size += transformed_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.lstm_h_init = nn.Parameter(torch.zeros(2 * num_layers, 1, hidden_dim))\n",
        "        self.lstm_c_init = nn.Parameter(torch.zeros(2 * num_layers, 1, hidden_dim))\n",
        "\n",
        "        self.upos_hid = nn.Linear(2* hidden_dim, upos_clf_hidden_dim)\n",
        "        self.upos_clf = nn.Linear(upos_clf_hidden_dim, len(vocab['upos']))\n",
        "\n",
        "        self.crit = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "    def forward(self, sent_pad, upos_pad, pretrained_pad, sent_lens):\n",
        "        inputs = []\n",
        "\n",
        "        word_emb = self.word_emb(sent_pad)\n",
        "        inputs += [word_emb]\n",
        "\n",
        "        pretrained_emb = self.pretrained_emb(pretrained_pad)\n",
        "        pretrained_emb = self.trans_pretrained(pretrained_emb)\n",
        "        inputs += [pretrained_emb]\n",
        "\n",
        "        lstm_inputs = torch.cat([x for x in inputs], 2)\n",
        "        lstm_inputs = self.drop(lstm_inputs)\n",
        "        lstm_inputs = pack_padded_sequence(lstm_inputs, sent_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        lstm_outputs, _ = self.lstm(\n",
        "            lstm_inputs, \n",
        "            (self.lstm_h_init.expand(2 * self.num_layers, sent_pad.size(0), self.hidden_dim).contiguous(), \n",
        "             self.lstm_c_init.expand(2 * self.num_layers, sent_pad.size(0), self.hidden_dim).contiguous())\n",
        "        )\n",
        "        lstm_outputs = lstm_outputs.data\n",
        "\n",
        "        upos_hid = F.relu(self.upos_hid(self.drop(lstm_outputs)))\n",
        "        upos_pred = self.upos_clf(self.drop(upos_hid))\n",
        "\n",
        "        pred = PackedSequence(upos_pred, lstm_inputs.batch_sizes,\n",
        "                              lstm_inputs.sorted_indices, lstm_inputs.unsorted_indices)\n",
        "        pred = pad_packed_sequence(pred, batch_first=True)[0]\n",
        "        pred = pred.max(2)[1]\n",
        "        upos = pack_padded_sequence(upos_pad, sent_lens, batch_first=True, enforce_sorted=False).data\n",
        "        loss = self.crit(upos_pred, upos)\n",
        "\n",
        "        return loss, pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_jdDSOspTVn",
        "colab_type": "text"
      },
      "source": [
        "In the `Trainer` class, we are going to update the model's weights in the `update()` method. This method performs the forward pass of the model, backward pass on the loss and updates the optimizer.\n",
        "\n",
        "`predict()` method is used for inference. It only performs the forward pass on the model and trims the predictions to their original lengths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-iZlzjyF4no",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, vocab, word_emb_dim, transformed_dim, emb_matrix, hidden_dim, upos_clf_hidden_dim, num_layers, dropout, use_cuda=False):\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.model = Tagger(vocab, word_emb_dim, transformed_dim, emb_matrix, hidden_dim, upos_clf_hidden_dim, num_layers, dropout)\n",
        "        self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
        "\n",
        "        self.model.to(device)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters)\n",
        " \n",
        "    def update(self, batch, eval=False):\n",
        "        sent_pad, upos_pad, pretrained_pad, sent_lens = batch\n",
        "\n",
        "        if eval:\n",
        "            self.model.eval()\n",
        "        else:\n",
        "            self.model.train()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        loss, _ = self.model(sent_pad, upos_pad, pretrained_pad, sent_lens)\n",
        "        loss_val = loss.data.item()\n",
        "        if eval:\n",
        "            return loss_val\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss_val\n",
        "\n",
        "\n",
        "    def predict(self, batch):\n",
        "        sent_pad, upos_pad, pretrained_pad, sent_lens = batch\n",
        "\n",
        "        self.model.eval()\n",
        "        batch_size = sent_pad.size(0)\n",
        "        _, pred = self.model(sent_pad, upos_pad, pretrained_pad, sent_lens)\n",
        "        # Transform the indices to the pos tags\n",
        "        pred = [self.vocab['upos'].unmap(sent) for sent in pred.tolist()]\n",
        "        # Trim the predictions to their original lengths\n",
        "        pred_tokens = [[pred[i][j] for j in range(sent_lens[i])] for i in range(batch_size)]\n",
        "\n",
        "        return pred_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAu5ZF94JMvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_emb_dim = 75\n",
        "transformed_dim = 125\n",
        "emb_matrix = pretrain.emb\n",
        "hidden_dim = 200\n",
        "upos_clf_hidden_dim = 400\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "use_cuda = True if device == 'cuda' else False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiKLCdBaJG-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(vocab, word_emb_dim, transformed_dim, emb_matrix, hidden_dim, upos_clf_hidden_dim, num_layers, dropout, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjJm834cJ-kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = 0\n",
        "max_steps = 50000\n",
        "dev_score_history = []\n",
        "format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch)'\n",
        "last_best_step = 0\n",
        "\n",
        "log_step = 20\n",
        "eval_interval = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY5M8lS5Kibm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = 0\n",
        "while True:\n",
        "    do_break = False\n",
        "    for batch in train_loader:\n",
        "        start_time = time()\n",
        "        global_step += 1\n",
        "        loss = trainer.update(batch, eval=False)\n",
        "        train_loss += loss\n",
        "\n",
        "        if global_step % log_step == 0:\n",
        "            duration = time() - start_time\n",
        "            print(format_str.format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), global_step,\\\n",
        "                    max_steps, loss, duration))\n",
        "            \n",
        "        if global_step % eval_interval == 0:\n",
        "            print(\"Evaluating on dev set...\")\n",
        "            dev_preds = []\n",
        "            dev_words = []\n",
        "            for batch in dev_loader:\n",
        "                batch_size = batch[0].size(0)\n",
        "                preds = trainer.predict(batch)\n",
        "                dev_preds += preds\n",
        "                # Keep the original sentence\n",
        "                pred_sents = [[batch[0][i][j] for j in range(batch[3][i])] for i in range(batch_size)]\n",
        "                dev_words += [vocab['word'].unmap(sent) for sent in [sent for sent in pred_sents]]\n",
        "            \n",
        "            train_loss = train_loss / eval_interval\n",
        "            print(\"step {}: train_loss = {:.6f}\".format(global_step, train_loss))\n",
        "            # Shows one prediction for a sanity check\n",
        "            print(f\"Preds: {list(zip(dev_preds[0], dev_words[0]))}\")\n",
        "            train_loss = 0\n",
        "\n",
        "        if global_step >= max_steps:\n",
        "            do_break = True\n",
        "            break\n",
        "\n",
        "        if do_break:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unozGBA9MHaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}