{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB1YDdMM8TIe",
        "colab_type": "text"
      },
      "source": [
        "# Homework 4. Sequence Tagging with LSTM\n",
        "\n",
        "Welcome to Homework 4! \n",
        "\n",
        "The homework contains several tasks. You can find the amount of points that you get for the correct solution in the task header. Maximum amount of points for each homework is _six_.\n",
        "\n",
        "The **grading** for each task is the following:\n",
        "- correct answer - **full points**\n",
        "- insufficient solution or solution resulting in the incorrect output - **half points**\n",
        "- no answer or completely wrong solution - **no points**\n",
        "\n",
        "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\n",
        "\n",
        "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\n",
        "\n",
        "When writing code, make it readable. Choose appropriate names for your variables (`a = 'cat'` - not good, `word = 'cat'` - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\n",
        "\n",
        "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\n",
        "\n",
        "<font color='red'>**Important!:**</font> **before sending your solution, do the `Kernel -> Restart & Run All` to ensure that all your code works.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVRSVyJhH9Xt",
        "colab_type": "text"
      },
      "source": [
        "## Task 1. Download resourses for your language (0.5 points)\n",
        "\n",
        "In this homework, you are going to improve the pos tagger for your native language that we have built during the Lab. In particular, you are going to add a character level model to capture the inner structure of the word. This should help to better predict a correct tag.\n",
        "\n",
        "__What is your native language?__\n",
        "\n",
        "<font color='red'>Your answer here</font>\n",
        "\n",
        "To start with, import all the packages below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yygt6gOJevIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "from typing import List, Dict\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIDOXHRpIWAH",
        "colab_type": "text"
      },
      "source": [
        "Get the pretrained word vectors for your native language from [Fasttext](https://fasttext.cc/docs/en/crawl-vectors.html). For this model, you need to choose __text__ vectors (not bin!). If you don't have them available for your language, you can choose any other language that you want. Put the link instead of the `...` in the cell below.\n",
        "\n",
        "__Are there FastText vectors available for your language? If yes, provide the link to it below.__\n",
        "\n",
        "<font color='red'>Your answer here</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1dBJUSQe6eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dmuzm7tD4EF",
        "colab_type": "text"
      },
      "source": [
        "Replace `...` with the filename of your vector in the first line. Put the same name but without a `.gz` extension instead of the `...` in the last line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J3TIUs0fc0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip ...\n",
        "!mkdir vector_cache/\n",
        "!mv ... vector_cache/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxuSASQaJNAz",
        "colab_type": "text"
      },
      "source": [
        "Next, we need the data to train on. For this task, we are going to use [Universal Dependencies](https://universaldependencies.org/) data. It has labelled corpora for morphological tagging and syntax parsing for over than 70 languages. You need to choose your language from the official UD page, choose the treebank that you like and follow the GitHub link to it. Then, from GitHub, copy the link from the green \"Clone or download\" button and replace it in the cell below. \n",
        "\n",
        "Also, replace the name of your treebank in the `!mv` command.\n",
        "\n",
        "For example, if I choose the EDT treebank for Estonian from [here](https://universaldependencies.org/#estonian-treebanks), the GitHub link is going to be `https://github.com/UniversalDependencies/UD_Estonian-EDT.git` and the name of the treebank is `UD_Estonian-EDT`, which is the name of the repository.\n",
        "\n",
        "__Is there a UD treebank available for your language? If yes, provide the link to it below.__\n",
        "\n",
        "<font color='red'>Your answer here</font>\n",
        "\n",
        "Replace the `...` with the GitHub link to the repository that you've chosen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJBLZXIEgbbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n0J8ZoFEoMb",
        "colab_type": "text"
      },
      "source": [
        "Replace the `...` with the name of the treebank that you've chosen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJqCd9yQggJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data/\n",
        "!mv .../ data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8KqQMYpKzUJ",
        "colab_type": "text"
      },
      "source": [
        "This part is moslty the same as in the [Lab 4](https://github.com/501Good/tartu-nlp-2020/blob/master/labs/lab4/Lab4_TextClassificationCNN.ipynb). \n",
        "\n",
        "**Don't forget to change the `VEC_PATH` and `DATA_PATH` variables to match your data!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb4fsehMgkBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD = '<PAD>'\n",
        "PAD_ID = 0\n",
        "UNK = '<UNK>'\n",
        "UNK_ID = 1\n",
        "\n",
        "VOCAB_PREFIX = [PAD, UNK]\n",
        "\n",
        "VEC_PATH = Path('vector_cache') / '...'\n",
        "DATA_PATH = Path('data') / '...'\n",
        "MAX_VOCAB = 25000\n",
        "\n",
        "batch_size = 64\n",
        "validation_split = .3\n",
        "shuffle_dataset = True\n",
        "random_seed = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5DyDzC6ORzY",
        "colab_type": "text"
      },
      "source": [
        "You can consult the Lab materials if you have any questions about the vocab classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGsPh253hDIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaseVocab:\n",
        "    def __init__(self, data, idx=0, lower=False):\n",
        "        self.data = data\n",
        "        self.lower = lower\n",
        "        self.idx = idx\n",
        "        self.build_vocab()\n",
        "        \n",
        "    def normalize_unit(self, unit):\n",
        "        if self.lower:\n",
        "            return unit.lower()\n",
        "        else:\n",
        "            return unit\n",
        "        \n",
        "    def unit2id(self, unit):\n",
        "        unit = self.normalize_unit(unit)\n",
        "        if unit in self._unit2id:\n",
        "            return self._unit2id[unit]\n",
        "        else:\n",
        "            return self._unit2id[UNK]\n",
        "    \n",
        "    def id2unit(self, id):\n",
        "        return self._id2unit[id]\n",
        "    \n",
        "    def map(self, units):\n",
        "        return [self.unit2id(unit) for unit in units]\n",
        "\n",
        "    def unmap(self, ids):\n",
        "        return [self.id2unit(idx) for idx in ids]\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        NotImplementedError()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._unit2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMvi6s3lsFA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PretrainedWordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        self._id2unit = VOCAB_PREFIX + self.data\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9mn3bWHhXER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        if self.lower:\n",
        "            counter = Counter([w[self.idx].lower() for sent in self.data for w in sent])\n",
        "        else:\n",
        "            counter = Counter([w[self.idx] for sent in self.data for w in sent])\n",
        "\n",
        "        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_dTkVH7GBwO",
        "colab_type": "text"
      },
      "source": [
        "Here, we introduce a character vocab that is going to store the mappings for individual characters rather than words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgIck3Z61HY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        counter = Counter([c for sent in self.data for w in sent for c in w[self.idx]])\n",
        "        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: (counter[k], k), reverse=True))\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPOQxl5mPdK8",
        "colab_type": "text"
      },
      "source": [
        "You can consult the Lab materials if you have any questions about building the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOefas-mheLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pretrain:\n",
        "    def __init__(self, vec_filename, max_vocab=-1):\n",
        "        self._vec_filename = vec_filename\n",
        "        self._max_vocab = max_vocab\n",
        "        \n",
        "    @property\n",
        "    def vocab(self):\n",
        "        if not hasattr(self, '_vocab'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._vocab\n",
        "    \n",
        "    @property\n",
        "    def emb(self):\n",
        "        if not hasattr(self, '_emb'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._emb\n",
        "        \n",
        "    def read(self):\n",
        "        if self._vec_filename is None:\n",
        "            raise Exception(\"Vector file is not provided.\")\n",
        "        print(f\"Reading pretrained vectors from {self._vec_filename}...\")\n",
        "        \n",
        "        words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n",
        "        \n",
        "        if failed > 0: # recover failure\n",
        "            emb = emb[:-failed]\n",
        "        if len(emb) - len(VOCAB_PREFIX) != len(words):\n",
        "            raise Exception(\"Loaded number of vectors does not match number of words.\")\n",
        "            \n",
        "        # Use a fixed vocab size\n",
        "        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):\n",
        "            words = words[:self._max_vocab - len(VOCAB_PREFIX)]\n",
        "            emb = emb[:self._max_vocab]\n",
        "                \n",
        "        vocab = PretrainedWordVocab(words, lower=True)\n",
        "        \n",
        "        return vocab, emb\n",
        "        \n",
        "    def read_from_file(self, filename, open_func=open):\n",
        "        \"\"\"\n",
        "        Open a vector file using the provided function and read from it.\n",
        "        \"\"\"\n",
        "        first = True\n",
        "        words = []\n",
        "        failed = 0\n",
        "        with open_func(filename, 'rb') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    line = line.decode()\n",
        "                except UnicodeDecodeError:\n",
        "                    failed += 1\n",
        "                    continue\n",
        "                if first:\n",
        "                    # the first line contains the number of word vectors and the dimensionality\n",
        "                    first = False\n",
        "                    line = line.strip().split(' ')\n",
        "                    rows, cols = [int(x) for x in line]\n",
        "                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n",
        "                    continue\n",
        "\n",
        "                line = line.rstrip().split(' ')\n",
        "                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n",
        "                words.append(' '.join(line[:-cols]))\n",
        "        return words, emb, failed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8i2PuW6iafP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIELD_NUM = 10\n",
        "\n",
        "class Word:\n",
        "    def __init__(self, word: List[str]):\n",
        "        self._id = word[0]\n",
        "        self._text = word[1]\n",
        "        self._lemma = word[2]\n",
        "        self._upos = word[3]\n",
        "        self._xpos = word[4]\n",
        "        self._feats = word[5]\n",
        "        self._head = word[6]\n",
        "        self._deprel = word[7]\n",
        "        self._deps = word[8]\n",
        "        self._misc = word[9]\n",
        "\n",
        "    @property\n",
        "    def id(self):\n",
        "        return self._id\n",
        "\n",
        "    @property\n",
        "    def text(self):\n",
        "        return self._text\n",
        "\n",
        "    @property\n",
        "    def lemma(self):\n",
        "        return self._lemma\n",
        "\n",
        "    @property\n",
        "    def upos(self):\n",
        "        return self._upos\n",
        "\n",
        "    @property\n",
        "    def xpos(self):\n",
        "        return self._xpos\n",
        "\n",
        "    @property\n",
        "    def feats(self):\n",
        "        return self._feats\n",
        "\n",
        "    @property\n",
        "    def head(self):\n",
        "        return self._head\n",
        "\n",
        "    @property\n",
        "    def deprel(self):\n",
        "        return self._deprel\n",
        "\n",
        "    @property\n",
        "    def deps(self):\n",
        "        return self._deps\n",
        "\n",
        "    @property\n",
        "    def misc(self):\n",
        "        return self._misc\n",
        "\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, words: List[List[str]]):\n",
        "        self._words = [Word(w) for w in words]\n",
        "\n",
        "    @property\n",
        "    def words(self):\n",
        "        return self._words\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, file_path):\n",
        "        self._sentences = []\n",
        "        self.load_conll(open(file_path, encoding='utf-8'))\n",
        "\n",
        "\n",
        "    def load_conll(self, f, ignore_gapping=True):\n",
        "        \"\"\" Load the file or string into the CoNLL-U format data.\n",
        "        Input: file or string reader, where the data is in CoNLL-U format.\n",
        "        Output: a list of list of list for each token in each sentence in the data, where the innermost list represents \n",
        "        all fields of a token.\n",
        "\n",
        "        Taken and modified from Stanza: https://github.com/stanfordnlp/stanza/blob/master/stanza/utils/conll.py\n",
        "        Stanza is released under the Apache License, Version 2.0.\n",
        "        \"\"\"\n",
        "        # f is open() or io.StringIO()\n",
        "        doc, sent = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if len(line) == 0:\n",
        "                if len(sent) > 0:\n",
        "                    doc.append(Sentence(sent))\n",
        "                    sent = []\n",
        "            else:\n",
        "                if line.startswith('#'): # skip comment line\n",
        "                    continue\n",
        "                array = line.split('\\t')\n",
        "                if ignore_gapping and '.' in array[0]:\n",
        "                    continue\n",
        "                assert len(array) == FIELD_NUM, \\\n",
        "                        f\"Cannot parse CoNLL line: expecting {FIELD_NUM} fields, {len(array)} found.\"\n",
        "                sent += [array]\n",
        "        if len(sent) > 0:\n",
        "            doc.append(Sentence(sent))\n",
        "        self._sentences = doc\n",
        "\n",
        "    \n",
        "    @property\n",
        "    def sentences(self):\n",
        "        return self._sentences\n",
        "\n",
        "\n",
        "    def get(self, fields, as_sentences=False):\n",
        "        \"\"\"Taken and modified from Stanza: https://github.com/stanfordnlp/stanza/blob/master/stanza/models/common/doc.py\n",
        "        Stanza is released under the Apache License, Version 2.0.\n",
        "        \"\"\"\n",
        "        assert isinstance(fields, list), \"Must provide field names as a list.\"\n",
        "        assert len(fields) >= 1, \"Must have at least one field.\"\n",
        "\n",
        "        results = []\n",
        "        for sentence in self.sentences:\n",
        "            cursent = []\n",
        "            units = sentence.words\n",
        "            for unit in units:\n",
        "                if len(fields) == 1:\n",
        "                    cursent += [getattr(unit, fields[0])]\n",
        "                else:\n",
        "                    cursent += [[getattr(unit, field) for field in fields]]\n",
        "\n",
        "            # decide whether append the results as a sentence or a whole list\n",
        "            if as_sentences:\n",
        "                results.append(cursent)\n",
        "            else:\n",
        "                results += cursent\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azsMK8kTSZfO",
        "colab_type": "text"
      },
      "source": [
        "For the dataset, we are going to add the new `CharVocab` and preprocess each word character by character with it. For example, if you have a character vocabulary like this:\n",
        "\n",
        "`{'a': 0, 'b': 1, ..., 'y': 24, 'z': 25, 'A': 26, 'B': 27, ..., 'Y': 50, 'Z': 51}`\n",
        "\n",
        "Then a sentence `['I', 'like', 'cats']` is going to be transformed into `[[35], [11, 8, 10, 4], [2, 0, 19, 18]]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7UVFATAhsEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CONLLUDataset(Dataset):\n",
        "    def __init__(self, doc: Document, pretrain: Pretrain, \n",
        "                 vocab: Dict[str, BaseVocab] = None, test: bool = False):\n",
        "        self.pretrain_vocab = pretrain.vocab\n",
        "        self.test = test\n",
        "        data = self.load_doc(doc)\n",
        "\n",
        "        if vocab is None:\n",
        "            self.vocab = self.init_vocab(data)\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "        self.data = self.preprocess(data, self.vocab, self.pretrain_vocab)\n",
        "\n",
        "    def init_vocab(self, data: List) -> Dict:\n",
        "        wordvocab = WordVocab(data, idx=0)\n",
        "        charvocab = CharVocab(data, idx=0)\n",
        "        uposvocab = WordVocab(data, idx=1)\n",
        "        vocab = {\n",
        "            'word': wordvocab,\n",
        "            'char': charvocab,\n",
        "            'upos': uposvocab}\n",
        "        return vocab\n",
        "\n",
        "    def preprocess(self, data: List, vocab: Dict[str, BaseVocab], \n",
        "                   pretrain_vocab: PretrainedWordVocab) -> List[List[int]]:\n",
        "        processed = []\n",
        "        for sent in data:\n",
        "            processed_sent = [vocab['word'].map([w[0] for w in sent])]\n",
        "            processed_sent += [[vocab['char'].map([char for char in w[0]]) for w in sent]]\n",
        "            processed_sent += [vocab['upos'].map([w[1] for w in sent])]\n",
        "            processed_sent += [pretrain_vocab.map([w[0].lower() for w in sent])]\n",
        "            processed.append(processed_sent)\n",
        "        return processed\n",
        "        \n",
        "    def load_doc(self, doc: Document) -> List:\n",
        "        data = doc.get(['text', 'upos'], as_sentences=True)\n",
        "        return data\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9L6KBcOkVu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLhAK0nzIOhI",
        "colab_type": "text"
      },
      "source": [
        "Put the correct path to your train file here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QraxfFjKs-jZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_doc = Document(DATA_PATH / '...')\n",
        "train_dataset = CONLLUDataset(train_doc, pretrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4hOIwqtIUQq",
        "colab_type": "text"
      },
      "source": [
        "Put the correct path to your dev file here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdk0QJJheLpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = train_dataset.vocab\n",
        "dev_doc = Document(DATA_PATH / '...')\n",
        "dev_dataset = CONLLUDataset(dev_doc, pretrain, vocab=vocab, test=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPFocs3RIYTA",
        "colab_type": "text"
      },
      "source": [
        "You can look inside the first sentence to see how the preprocessed data looks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBZSDFv6t_-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1_mmSj9Ipzv",
        "colab_type": "text"
      },
      "source": [
        "We are going to pad the characters and save the original lengths for each word in a sentence to reconstruct the correct order later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbhf2uYHvcJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_collate(batch):\n",
        "    (sents, chars, upos, pretrained) = zip(*batch)\n",
        "\n",
        "    sent_lens = [len(s) for s in sents]\n",
        "    word_lens = [len(c) for w in chars for c in w]\n",
        "\n",
        "    sents = [torch.LongTensor(w).to(device) for w in sents]\n",
        "    chars = [torch.LongTensor(c).to(device) for w in chars for c in w]\n",
        "    upos = [torch.LongTensor(u).to(device) for u in upos]\n",
        "    pretrained = [torch.LongTensor(p).to(device) for p in pretrained]\n",
        "\n",
        "    sent_pad = pad_sequence(sents, batch_first=True, padding_value=PAD_ID)\n",
        "    chars_pad = pad_sequence(chars, batch_first=True, padding_value=PAD_ID)\n",
        "    upos_pad = pad_sequence(upos, batch_first=True, padding_value=PAD_ID)\n",
        "    pretrained_pad = pad_sequence(pretrained, batch_first=True, padding_value=PAD_ID)\n",
        "\n",
        "    sent_pad = sent_pad.to(device)\n",
        "    chars_pad = chars_pad.to(device)\n",
        "    upos_pad = upos_pad.to(device)\n",
        "    pretrained_pad = pretrained_pad.to(device)\n",
        "\n",
        "    return sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMNbcGrQ7dGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li5RRsb6IrJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLNjeeQUa0U3",
        "colab_type": "text"
      },
      "source": [
        "## Task 2. Build a character-level LSTM model (3 points)\n",
        "\n",
        "You already know that we can have a vector representation of a word that is learned from its context. We can use these vectors to capture sematical relations between the words. \n",
        "\n",
        "In addition to that we can learn an inner representation of a word, or its character-level embedding. This can help to capture morphological information about a word.\n",
        "\n",
        "We can do it by building another LSTM model and taking its last hidden state which is going to be the character-level representation of a word. The input to the model is going to be a stream for characters for each word in a batch.\n",
        "\n",
        "### Task 2.1. Define an Embedding layer (0.5 points)\n",
        "\n",
        "Create an [`nn.Embedding`](https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding) layer. The input size, or the number of embeddings, should be the size of our vocabulary (`CONLLUDataset` class shows how to access them). The output size, or the size of each vector, is `char_emb_dim`. Padding index is `0`. \n",
        "\n",
        "### Task 2.2. Define an LSTM layer (0.5 points)\n",
        "\n",
        "Create an [`nn.LSTM`](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM) layer. The input size should be the output size of the embedding layer. The hidden size is `char_hidden_dim`. The number of layers is `char_num_layers`. Set the `batch_first` parameter to `True`. Make the droupout to be `0` if number of layers is `1` or `droupout` otherwise.\n",
        "\n",
        "### Task 2.3. Embed the input (0.5 points)\n",
        "\n",
        "Now, you will implement a forward pass.\n",
        "\n",
        "Run the character input (`chars_pad`) through the embedding layer. Apply the dropout to the embeddings and save the result into the `char_emb` variable.\n",
        "\n",
        "### Task 2.4. Apply the LSTM layer (0.5 points)\n",
        "\n",
        "Put the embeddings into the LSTM layer. You are going to save only the last hidden state that is going to be the representation of the word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSY9a1DGHU8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab: Dict[str, BaseVocab], char_emb_dim: int,\n",
        "                 char_hidden_dim: int, char_num_layers: int, dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        ### Task 2.1 starts here ###\n",
        "        self.char_emb = ...\n",
        "        ### Task 2.1 ends here ###\n",
        "\n",
        "        ### Task 2.2 starts here ###\n",
        "        self.char_lstm = ...\n",
        "        self.char_lstm_h_init = nn.Parameter(torch.zeros(char_num_layers, 1, char_hidden_dim))\n",
        "        self.char_lstm_c_init = nn.Parameter(torch.zeros(char_num_layers, 1, char_hidden_dim))\n",
        "        ### Task 2.2 ends here ###\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, chars_pad, sent_lens, word_lens):\n",
        "        ### Task 2.3 starts here ###\n",
        "        char_emb = ...\n",
        "        ### Task 2.3 ends here ###\n",
        "\n",
        "        batch_size = char_emb.size(0)\n",
        "        char_emb = pack_padded_sequence(char_emb, word_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        ### Task 2.4 starts here ###\n",
        "        _, (h, _) = self.char_lstm(\n",
        "            ...,\n",
        "            (self.char_lstm_h_init.expand(char_num_layers, batch_size, char_hidden_dim).contiguous(),\n",
        "             self.char_lstm_c_init.expand(char_num_layers, batch_size, char_hidden_dim).contiguous())\n",
        "        )\n",
        "        ### Task 2.4 ends here ###\n",
        "\n",
        "        # Remove an empty dimension\n",
        "        result = h.squeeze(0)\n",
        "        # Chunk the output back into words\n",
        "        result = pack_sequence(result.split(sent_lens), enforce_sorted=False)\n",
        "\n",
        "        return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXaj5bXEQxAw",
        "colab_type": "text"
      },
      "source": [
        "### Task 2.5. Incorporate the CharLSTM into the Tagger model (0.5 points)\n",
        "\n",
        "Now your new model should have the following architecture:\n",
        "\n",
        "![img](img1.png)\n",
        "\n",
        "Initialize the CharLSTM model that you've just created. Pass `vocab`, `char_emb_dim`, `char_hidden_dim`, `char_num_layers`, `dropout` to the class constructor. Save it to `self.char_model`.\n",
        "\n",
        "Create another [`nn.Linear`](https://pytorch.org/docs/stable/nn.html?highlight=linear#torch.nn.Linear) layer to transform the character representations. It should work similar to the linear layer for the transformation of the pretrained vectors. The input size should be `char_hidden_dim` and the output size is `transformed_dim`. Also, set the `bias` parameter to `False`.\n",
        "\n",
        "Add the `transformed_dim` to the `input_size`.\n",
        "\n",
        "### Task 2.6. Get the character embeddings in the forward pass (0.5 points)\n",
        "\n",
        "Perform the forward pass on your CharLSTM and save the results into the `char_emb` variable. Pass all the appropriate parameters to the CharLSTM.\n",
        "\n",
        "Apply the dropout to the `char_emb`. After that transform it with the `self.trans_char` linear layer. \n",
        "\n",
        "**NB!** Since the output of the CharLSTM is a `PackedSequence` object, you need to apply the dropout to `char_emb.data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N-xVFW28FD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tagger(nn.Module):\n",
        "    def __init__(self, vocab: Dict[str, BaseVocab], word_emb_dim: int,\n",
        "                 char_emb_dim: int, transformed_dim: int, emb_matrix: np.ndarray,\n",
        "                 hidden_dim: int, char_hidden_dim: int,\n",
        "                 upos_clf_hidden_dim: int, num_layers: int, char_num_layers: int,\n",
        "                 dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        input_size = 0\n",
        "\n",
        "        self.word_emb = nn.Embedding(len(vocab['word']), word_emb_dim, padding_idx=0)\n",
        "        input_size += word_emb_dim\n",
        "\n",
        "        ### Task 2.5 starts here ###\n",
        "        self.char_model = ...\n",
        "        self.trans_char = ...\n",
        "        input_size += transformed_dim\n",
        "        ### Task 2.5 ends here ###\n",
        "\n",
        "        self.pretrained_emb = nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True)\n",
        "        self.trans_pretrained = nn.Linear(emb_matrix.shape[1], transformed_dim, bias=False)\n",
        "        input_size += transformed_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.lstm_h_init = nn.Parameter(torch.zeros(2 * num_layers, 1, hidden_dim))\n",
        "        self.lstm_c_init = nn.Parameter(torch.zeros(2 * num_layers, 1, hidden_dim))\n",
        "\n",
        "        self.upos_hid = nn.Linear(2* hidden_dim, upos_clf_hidden_dim)\n",
        "        self.upos_clf = nn.Linear(upos_clf_hidden_dim, len(vocab['upos']))\n",
        "\n",
        "        self.crit = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "    def forward(self, sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens):\n",
        "        inputs = []\n",
        "\n",
        "        word_emb = self.word_emb(sent_pad)\n",
        "        inputs += [word_emb]\n",
        "\n",
        "        pretrained_emb = self.pretrained_emb(pretrained_pad)\n",
        "        pretrained_emb = self.trans_pretrained(pretrained_emb)\n",
        "        inputs += [pretrained_emb]\n",
        "\n",
        "        ### Task 2.6 starts here ###\n",
        "        char_emb = ...\n",
        "        char_emb_trans = ...\n",
        "        ### Task 2.6 ends here ###\n",
        "        # Creating a PackedSequence from the embeddings to restore the original order\n",
        "        char_emb = PackedSequence(char_emb_trans, char_emb.batch_sizes, \n",
        "                                  char_emb.sorted_indices, char_emb.unsorted_indices)\n",
        "        char_emb = pad_packed_sequence(char_emb, batch_first=True)[0]\n",
        "        inputs += [char_emb]\n",
        "\n",
        "        lstm_inputs = torch.cat([x for x in inputs], 2)\n",
        "        lstm_inputs = self.drop(lstm_inputs)\n",
        "        lstm_inputs = pack_padded_sequence(lstm_inputs, sent_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        lstm_outputs, _ = self.lstm(\n",
        "            lstm_inputs, \n",
        "            (self.lstm_h_init.expand(2 * self.num_layers, sent_pad.size(0), self.hidden_dim).contiguous(), \n",
        "             self.lstm_c_init.expand(2 * self.num_layers, sent_pad.size(0), self.hidden_dim).contiguous())\n",
        "        )\n",
        "        lstm_outputs = lstm_outputs.data\n",
        "\n",
        "        upos_hid = F.relu(self.upos_hid(self.drop(lstm_outputs)))\n",
        "        upos_pred = self.upos_clf(self.drop(upos_hid))\n",
        "\n",
        "        pred = PackedSequence(upos_pred, lstm_inputs.batch_sizes,\n",
        "                              lstm_inputs.sorted_indices, lstm_inputs.unsorted_indices)\n",
        "        pred = pad_packed_sequence(pred, batch_first=True)[0]\n",
        "        pred = pred.max(2)[1]\n",
        "        upos = pack_padded_sequence(upos_pad, sent_lens, batch_first=True, enforce_sorted=False).data\n",
        "        loss = self.crit(upos_pred, upos)\n",
        "\n",
        "        return loss, pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-iZlzjyF4no",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, vocab, word_emb_dim, char_emb_dim, transformed_dim,\n",
        "                 emb_matrix, hidden_dim, char_hidden_dim, upos_clf_hidden_dim, \n",
        "                 num_layers, char_num_layers, dropout):\n",
        "        self.vocab = vocab\n",
        "        self.model = Tagger(vocab, word_emb_dim, char_emb_dim, transformed_dim, \n",
        "                            emb_matrix, hidden_dim, char_hidden_dim, \n",
        "                            upos_clf_hidden_dim, num_layers, char_num_layers, \n",
        "                            dropout)\n",
        "        self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
        "\n",
        "        self.model.to(device)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters)\n",
        " \n",
        "    def update(self, batch, eval=False):\n",
        "        sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens = batch\n",
        "\n",
        "        if eval:\n",
        "            self.model.eval()\n",
        "        else:\n",
        "            self.model.train()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        loss, _ = self.model(sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens)\n",
        "        loss_val = loss.data.item()\n",
        "        if eval:\n",
        "            return loss_val\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss_val\n",
        "\n",
        "\n",
        "    def predict(self, batch):\n",
        "        sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens = batch\n",
        "\n",
        "        self.model.eval()\n",
        "        batch_size = sent_pad.size(0)\n",
        "        _, pred = self.model(sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens)\n",
        "        # Transform the indices to the pos tags\n",
        "        pred = [self.vocab['upos'].unmap(sent) for sent in pred.tolist()]\n",
        "        # Trim the predictions to their original lengths\n",
        "        pred_tokens = [[pred[i][j] for j in range(sent_lens[i])] for i in range(batch_size)]\n",
        "\n",
        "        gold_upos = [vocab['upos'].unmap(upos) for upos in [upos for upos in upos_pad]]\n",
        "        gold_tokens = [[gold_upos[i][j] for j in range(sent_lens[i])] for i in range(batch_size)]\n",
        "\n",
        "        return pred_tokens, gold_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAu5ZF94JMvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_emb_dim = 75\n",
        "char_emb_dim = 100\n",
        "transformed_dim = 125\n",
        "emb_matrix = pretrain.emb\n",
        "hidden_dim = 200\n",
        "char_hidden_dim = 400\n",
        "upos_clf_hidden_dim = 400\n",
        "num_layers = 2\n",
        "char_num_layers = 1\n",
        "dropout = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiKLCdBaJG-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(vocab, word_emb_dim, char_emb_dim, transformed_dim,\n",
        "                  emb_matrix, hidden_dim, char_hidden_dim, upos_clf_hidden_dim,\n",
        "                  num_layers, char_num_layers, dropout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjJm834cJ-kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = 0\n",
        "max_steps = 50000\n",
        "dev_score_history = []\n",
        "format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch)'\n",
        "last_best_step = 0\n",
        "\n",
        "log_step = 20\n",
        "eval_interval = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT2AjVnth0_L",
        "colab_type": "text"
      },
      "source": [
        "## Task 3. Train your model (0.5 points)\n",
        "\n",
        "Run the cell below to start training the model. After each 100 steps, it is going to print out the average training loss and dev score, which is a simple accuracy in our case. You should see the training loss decreasing and the dev score increasing.\n",
        "\n",
        "Train the model until you don't see the increase in the dev score anymore. Report the score that you've got.\n",
        "\n",
        "__My maximum dev_score is:__\n",
        "\n",
        "<font color='red'>Your answer here</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY5M8lS5Kibm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = 0\n",
        "while True:\n",
        "    do_break = False\n",
        "    for batch in train_loader:\n",
        "        start_time = time()\n",
        "        global_step += 1\n",
        "        loss = trainer.update(batch, eval=False)\n",
        "        train_loss += loss\n",
        "\n",
        "        if global_step % log_step == 0:\n",
        "            duration = time() - start_time\n",
        "            print(format_str.format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), global_step,\\\n",
        "                    max_steps, loss, duration))\n",
        "            \n",
        "        if global_step % eval_interval == 0:\n",
        "            print(\"Evaluating on dev set...\")\n",
        "            dev_preds = []\n",
        "            dev_words = []\n",
        "            dev_correct = 0\n",
        "            dev_total = 0\n",
        "            for batch in dev_loader:\n",
        "                batch_size = batch[0].size(0)\n",
        "                preds, gold = trainer.predict(batch)\n",
        "                dev_correct += sum([1 for sent in zip(preds, gold) for pair in zip(*sent) if pair[0] == pair[1]])\n",
        "                dev_total += sum([len(sent) for sent in gold])\n",
        "                dev_preds += preds\n",
        "                # Keep the original sentence\n",
        "                pred_sents = [[batch[0][i][j] for j in range(batch[4][i])] for i in range(batch_size)]\n",
        "                dev_words += [vocab['word'].unmap(sent) for sent in [sent for sent in pred_sents]]\n",
        "            \n",
        "            dev_score = dev_correct / dev_total\n",
        "            train_loss = train_loss / eval_interval\n",
        "            print(\"step {}: train_loss = {:.6f}, dev_score = {:.6f}\".format(global_step, train_loss, dev_score))\n",
        "            # Shows one prediction for a sanity check\n",
        "            print(f\"Preds: {list(zip(dev_preds[0], dev_words[0]))}\")\n",
        "            train_loss = 0\n",
        "\n",
        "        if global_step >= max_steps:\n",
        "            do_break = True\n",
        "            break\n",
        "\n",
        "        if do_break:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIVnti9di2k9",
        "colab_type": "text"
      },
      "source": [
        "## Task 4. Propose a new approach (2 points)\n",
        "\n",
        "So far, you should have an idea on how the sequence tagging is done. Now you will have to propose an approach for a new problem.\n",
        "\n",
        "Imagine that you need to add morphological tagging to your model. The tags are stored in the [_feats_ field](https://universaldependencies.org/format.html#morphological-annotation) of the CoNLL-U format. From the official description, thay have the following format:\n",
        "\n",
        "> The FEATS field contains a list of morphological features, with vertical bar (|) as list separator and with underscore to represent the empty list. All features should be represented as attribute-value pairs, with an equals sign (=) separating the attribute from the value.\n",
        "\n",
        "Your task is to describe how you will modify the current POS tagger model to include FEATS tagger as well. Your description must answer the following questions:\n",
        "\n",
        "- Should you add a new vocab to read the feats? How will you build this vocab?\n",
        "- POS tagging can be treated as a multi-class classification task, i.e. we assign each word to exaclty one POS tag (which is a class in our situation). What kind of task is morphological tagging (multi-label, binary classification, multiple multi-class tasks)?\n",
        "- For example, nouns have case and gender attributes but verbs don't have them. How are you going to tackle this?\n",
        "- Which new layers are you going to introduce into the model?\n",
        "- What metrics are you going to use to measure the performance of morphological tagging?\n",
        "\n",
        "You can also try to visualize the model to make it easier to see your concept.\n",
        "\n",
        "<font color='red'>Your answer here</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unozGBA9MHaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}