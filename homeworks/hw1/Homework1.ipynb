{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1\n",
    "### NLP Basics & NLP Pipelines\n",
    "\n",
    "Welcome to Homework 1! \n",
    "\n",
    "The homework contains several tasks. You can find the amount of points that you get for the correct solution in the task header. Maximum amount of points for each homework is _six_.\n",
    "\n",
    "The **grading** for each task is the following:\n",
    "- correct answer - **full points**\n",
    "- insufficient solution or solution resulting in the incorrect output - **half points**\n",
    "- no answer or completely wrong solution - **no points**\n",
    "\n",
    "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\n",
    "\n",
    "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\n",
    "\n",
    "When writing code, make it readable. Choose appropriate names for your variables (`a = 'cat'` - not good, `word = 'cat'` - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\n",
    "\n",
    "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\n",
    "\n",
    "<font color='red'>**Important!:**</font> **before sending your solution, do the `Kernel -> Restart & Run All` to ensure that all your code works.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "import spacy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Find the data (0.5 points)\n",
    "\n",
    "Find large enough text data in English or [any other language supported by Spacy](https://spacy.io/usage/models). If the resources for your language are very limited, you may use English or other language of your preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the language of your data?**\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "**Where did you get the text data?**\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "**What kind of text is it? (books, magazines, news articles, etc.)**\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "**What style(s) of text does your data have? (user commetaries, scientific, neutral, etc.)**\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "**Was it easy to download the data? If no, desribe what difficulties you had and how you resolved them.**\n",
    "\n",
    "<font color='red'>Your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Tokenize and count statistics (0.5 points)\n",
    "\n",
    "Using either NLTK or Spacy tools, tokenize your text data that you found in the previous exercise.\n",
    "\n",
    "P.S. if you are using Spacy, don't forget to load an appropriate module for it\n",
    "\n",
    "Compute and output the following:\n",
    "- number of sentences \n",
    "- number of tokens \n",
    "- number of unique tokens (or types)\n",
    "- average length of a sentence\n",
    "- average length of a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the path with the name of your data file\n",
    "data_path = \"path_to_your_text_data.txt\"\n",
    "\n",
    "data = open(data_path, encoding='utf-8').read()\n",
    "\n",
    "# Split the data into sentences and tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = ...\n",
    "num_tokens = ...\n",
    "num_unique_tokens = ...\n",
    "avg_sentence_len = ...\n",
    "avg_token_len = ...\n",
    "\n",
    "print(\"Number of sentences:\", num_sentences)\n",
    "print(\"Number of tokens:\", num_tokens)\n",
    "print(\"Number of unique tokens (or types):\", num_unique_tokens)\n",
    "print(\"Average sentence length:\", avg_sentence_len)\n",
    "print(\"Average token length:\", avg_token_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Byte pair encoding (BPE) tokenization (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.1 (0.25 points)\n",
    "\n",
    "[Byte pair encoding (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding) is a simple algorithm of data compression. It looks for the most frequent pair of bytes in the data and replaces it with a new byte which is not seen in the data. \n",
    "\n",
    "Recently, this idea became [used in the tokenization](https://www.aclweb.org/anthology/P16-1162.pdf). Let's say that we want to train a network that captures the meaning of words. We can have in out data the following words: `low`, `lower`, `lowest`. If we tokenize the text in a simple way by splitting the words as a whole, the model will probably learn the relation between `low`, `lower`, `lowest`. Now, imagine that we get some new text that the model didn't see during training and it has the words `small`, `smaller`, `smallest` and in the training data we had only the word `small`. Since the model didn't see `smaller` and `smallest` during the training, it will most likely fail to capture the relation.\n",
    "\n",
    "One of the ways to solve this is BPE tokenization. It learns the most frequent sequences and can split an unknown word into **subwords**. In our case, it can split `smaller` into `['small', 'er']` since we had `small` in the training data and probably many other words ending with -er. Now. instead of one unknown word, the model have two known subwords from which it can take the information.\n",
    "\n",
    "The code below builds the subwords from the text data. For the purpose of time saving, we set the number of merges to 1000. \n",
    "\n",
    "Study the code below and answer the questions after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    \"\"\"Gets the text from a file and splits it with spaces.\"\"\"\n",
    "    \n",
    "    vocab = Counter()\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Computes the frequencies for each pair of characters in the vocab.\"\"\"\n",
    "\n",
    "    pairs = Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, in_vocab):\n",
    "    \"\"\"Merges the most frequent pair.\n",
    "\n",
    "    Arguments:\n",
    "    pair -- the most frequent word pair (tuple(str, str))\n",
    "    in_vocab -- vocabulary with frequencies (dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    out_vocab = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in in_vocab:\n",
    "        out_word = p.sub(''.join(pair), word)\n",
    "        out_vocab[out_word] = in_vocab[word]\n",
    "    return out_vocab\n",
    "\n",
    "def get_tokens_from_vocab(vocab):\n",
    "    tokens_frequencies = Counter()\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n",
    "\n",
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)\n",
    "\n",
    "vocab = get_vocab(data_path)\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')\n",
    "\n",
    "num_merges = 1000\n",
    "for i in tqdm(range(num_merges)):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "**Study the subwords from your data. Do you see any subwords that make sense from the liguistic point of view? (e.g. suffixes, prefixes, common roots etc.). Provide examples.**\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "**What will happen if you increase the number of merges?**\n",
    "\n",
    "<font color='red'>Your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.2 (0.75 points)\n",
    "\n",
    "Now, you are going to implement the function that splits the an unknown word into subwords using the vocab that we built above. \n",
    "\n",
    "One way to do it is the following:\n",
    "1. Sort our vocab by the length in the descending order.\n",
    "2. Find the boundaries of the \"window\" that is going to search if a candidate word has a corresponding subword in the vocab. In the beginning, the starting index is 0, since we start to scan the word from the first characher. The end index is the length of the longest subword in the vocab or the length of the word if it is smaller.\n",
    "3. In a while loop, start looking at the possible subwords. If the subword you are looking at is in the vocab, append it to the result. Now, your new starting index is your previous end index. Your new end index is your new start index plus the length of the longest subword in the vocab or the length of the word if it is smaller than the resulting sum. If the subword is not in the vocab, we reduce the end index by one thus narrowing our search window. Finally, is the length of our window is equal to one, we put an unknown subword in the result and update our window as above.\n",
    "4. End the loop when we reach the end of the word.\n",
    "\n",
    "After you finish with the function, test the tokenizer on a very common word and on a very unusual word (you can even try to invent a word yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing word: mountains</w>...\n",
      "['mountain', 's</w>']\n"
     ]
    }
   ],
   "source": [
    "# Sorting the subwords by the length in the descending order\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    \"\"\"\n",
    "    Tokenizes the word into subword using learned BPE vocab\n",
    "    \n",
    "    Arguments:\n",
    "    string -- a word to tokenize. Must end with </w>\n",
    "    sorted_tokens -- sorted vocab by frequency in descending order\n",
    "    unknown_token -- a token to replace the words not found in the vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    # We are going to store our subwords here\n",
    "    string_tokens = []\n",
    "    \n",
    "    # Find the maximum length of the ngram in vocab\n",
    "    ngram_max_len = ...\n",
    "    # End index is the maximum lenth of the ngram or the length of the string is it's smaller\n",
    "    end_idx = ...\n",
    "    # Starting index is 0 in the beginning\n",
    "    start_idx = 0\n",
    "    \n",
    "    while start_idx < len(string):\n",
    "        subword = string[start_idx:end_idx]\n",
    "        if subword in sorted_tokens:\n",
    "            ...\n",
    "        elif len(subword) == 1:\n",
    "            ...\n",
    "        else:\n",
    "            ...\n",
    "            \n",
    "    return string_tokens\n",
    "\n",
    "# The word should end with \"</w>\". For example, \"cat</w>\".\n",
    "word_known = '...</w>'\n",
    "word_unknown = '...</w>'\n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_known))\n",
    "if word_known in vocab_tokenization:\n",
    "    print(vocab_tokenization[word_known])\n",
    "else:\n",
    "    print(tokenize_word(string=word_known, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
    "    \n",
    "\n",
    "print('Tokenizing word: {}...'.format(word_unknown))\n",
    "if word_unknown in vocab_tokenization:\n",
    "    print(vocab_tokenization[word_unknown])\n",
    "else:\n",
    "    print(tokenize_word(string=word_unknown, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Lemmatization and normalization (1 point)\n",
    "\n",
    "#### Task 4.1 (0.5 points)\n",
    "\n",
    "Using either NTLK or Spacy, lemmatize your data.\n",
    "Make a copy of your data but this time transform all the tokens and lemmas into the lowercase.\n",
    "\n",
    "Provide the following statistics:\n",
    "- Number of unique lemmas (original case)\n",
    "- Number of unique lemmas (lower case)\n",
    "- Number of unique tokens (original case)\n",
    "- Number of unique tokens (lower case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize your data\n",
    "...\n",
    "\n",
    "\n",
    "# Make a copy of your tokens but in lowercase\n",
    "...\n",
    "\n",
    "\n",
    "# Count statistics (no need to calculate the number of unique tokens in original case since we did it in Task 2)\n",
    "num_unique_lemmas = ...\n",
    "num_unique_lemmas_lower = ...\n",
    "num_unique_tokens_lower = ...\n",
    "\n",
    "# Print out the numbers\n",
    "print(\"Number of unique lemmas (original case):\", num_unique_lemmas)\n",
    "print(\"Number of unique lemmas (lower case):\", num_unique_lemmas_lower)\n",
    "print(\"Number of unique tokens (original case):\", num_unique_tokens)\n",
    "print(\"Number of unique tokens (lower case):\", num_unique_tokens_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2 (0.5 points)\n",
    "\n",
    "Look at the numbers you got. \n",
    "\n",
    "**Imagine that you want to use your data to train a network that captures the meaning of the words. Do you want to use tokens or lemmas? Original or lowercase? Explain your choice.**\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "**Imagine that you want to use your data to train a system that detects named entities, i.e. names of people, places, companies etc. Do you want to use tokens or lemmas? Original or lowercase? Explain your choice.**\n",
    "\n",
    "<font color='red'>Your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Choose your pipeline (0.5 points)\n",
    "\n",
    "Choose the pipeline between [Spacy](https://spacy.io/) and [StanfordNLP](https://github.com/stanfordnlp/stanfordnlp).\n",
    "\n",
    "**Which pipeline did you choose? Why?**\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "**What components does the pipeline have?**\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "**What languages does the pipeline support?**\n",
    "\n",
    "<font color='red'>Your answer here</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your pipeline here\n",
    "import ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6. Process your text (1.5 points)\n",
    "\n",
    "#### Task 6.1 (1 point)\n",
    "\n",
    "Process the text data from the first task with the pipeline of your choice. \n",
    "\n",
    "Select one sentence from the processed document and print out all the results (tokens, pos-tags, lemmas, depparse, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text\n",
    "...\n",
    "\n",
    "\n",
    "# Print out the results\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6.2 (0.5 points)\n",
    "\n",
    "**Look at your output above. Are the results correct? If no, provide the examples of the mistakes.**\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "**What is the difference between a POS tag and morphological tag?**\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "**What is the difference between tagging and parsing?**\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "**Analyze the dependency parsing result. Does it make sense? Briefly describe the meaning behind the relations.**\n",
    "\n",
    "<font color='red'>Your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7. Statistics (1 point)\n",
    "\n",
    "In your processed output, compute and print out (in a human readable format) the following stats:\n",
    "- POS tag frequency for each tag (in descending order)\n",
    "- 50 most frequent lemmas\n",
    "- 10 least frequent lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print out POS tag frequency\n",
    "...\n",
    "\n",
    "\n",
    "# Compute and print out 50 most frequent lemmas\n",
    "...\n",
    "\n",
    "\n",
    "# Compute and print out 10 least frequent lemmas\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
