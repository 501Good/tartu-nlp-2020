{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "Homework3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UEDYQfp6SkIf"
      },
      "source": [
        "# Homework 3. Text Classification with CNN\n",
        "\n",
        "Welcome to Homework 3! \n",
        "\n",
        "The homework contains several tasks. You can find the amount of points that you get for the correct solution in the task header. Maximum amount of points for each homework is _six_.\n",
        "\n",
        "The **grading** for each task is the following:\n",
        "- correct answer - **full points**\n",
        "- insufficient solution or solution resulting in the incorrect output - **half points**\n",
        "- no answer or completely wrong solution - **no points**\n",
        "\n",
        "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\n",
        "\n",
        "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\n",
        "\n",
        "When writing code, make it readable. Choose appropriate names for your variables (`a = 'cat'` - not good, `word = 'cat'` - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\n",
        "\n",
        "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\n",
        "\n",
        "<font color='red'>**Important!:**</font> **before sending your solution, do the `Kernel -> Restart & Run All` to ensure that all your code works.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mT16Bz_-LBmg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "import time\n",
        "import json\n",
        "\n",
        "# from: https://spacy.io/api/tokenizer\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "# Create a Tokenizer with the default settings for English\n",
        "# including punctuation rules and exceptions\n",
        "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "\n",
        "# Check if we are running on a CPU or GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c0z6OPiAi8X-"
      },
      "source": [
        "## About the homework\n",
        "\n",
        "This homework is based on the [Lab 4](https://github.com/501Good/tartu-nlp-2020). You can freely use all the code and materials from this Lab to solve this homework. \n",
        "\n",
        "In the Lab, we looked at the binary classification problem. In other words, we had to predict only two classes. In this homework, you are going to learn about multi-label classification. This means that predictions can contain various number of labels. For exapmle, one movie can be drama, action, and murder at the same time while some other movie is just comedy. We achieve this by transforming the task into multiple binary classification problems. Basically, we are saying for each label if it is present or not.\n",
        "\n",
        "This will require for you to change the data loader and the model. We are going to introduce new metrics to evaluate the model as well.\n",
        "\n",
        "This time, you are going to work with [MPST: A Corpus of Movie Plot Synopses with Tags](http://ritual.uh.edu/mpst-2018/), the same dataset that we used in the previous Homework.\n",
        "\n",
        "__We strongly recommend you to do this homework in Google Colab if you don't have a GPU on your machine!__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gRmsO5GV5S39"
      },
      "source": [
        "## Task 0. Download the data (0 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UdftAyx8ZBRu"
      },
      "source": [
        "Let's download the vector file and unpack in to the `vector_cache/` folder. You can skip this step if you have already done it yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e7WKqu2yMcXr"
      },
      "outputs": [],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QlLJUBe6Mx7z"
      },
      "outputs": [],
      "source": [
        "!unzip wiki-news-300d-1M.vec.zip -d vector_cache/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CETGAQ_YZ3mW"
      },
      "source": [
        "Let's download the dataset and unpack in to the `MPST/` folder. You can skip this step if you have already done it yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7cXrxb9WNQfq"
      },
      "outputs": [],
      "source": [
        "!wget http://ritual.uh.edu/wp-content/uploads/projects/mpst_2018/MPST.7z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mFyKVu5cqqX4"
      },
      "outputs": [],
      "source": [
        "!7z x MPST.7z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1OyMQ_KEaMfR"
      },
      "source": [
        "We are going to define some variables that we are going to need later. \n",
        "\n",
        "We will need the `<PAD>` and `<UNK>` symbols. `<PAD>` is needed to make the sentences in one batch have the same length. We are going to prepend this symbol to the end of each sentence to equalize the lengths. `<UNK>` is needed to replace the words for which we don't have a pretrained vector.\n",
        "\n",
        "We are also going to define the paths for our vector file and data folder, as well as maximum numer of vectors that we want to store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "md73ChlvLBnc"
      },
      "outputs": [],
      "source": [
        "PAD = '<PAD>'\n",
        "PAD_ID = 0\n",
        "UNK = '<UNK>'\n",
        "UNK_ID = 1\n",
        "VOCAB_PREFIX = [PAD, UNK]\n",
        "\n",
        "VEC_PATH = Path('vector_cache') / 'wiki-news-300d-1M.vec'\n",
        "DATA_PATH = Path('MPST')\n",
        "MAX_VOCAB = 25000\n",
        "\n",
        "batch_size = 64\n",
        "validation_split = .3\n",
        "shuffle_dataset = True\n",
        "random_seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2yUjLPW1bK-q"
      },
      "source": [
        "First, let's prepare a vocabulary for our pretrained vectors and labels. Since the input to our model should be an index of a word, we need to build it to map from words to indices.\n",
        "\n",
        "Below, we define a `BaseVocab` class that is going to be a base class for our pretrained and label vocabularies. We also define some methods that we are going to use:\n",
        "\n",
        "- `normalize_unit()` to put the word to lowercase if `lower` argument is set to `True`.\n",
        "- `unit2id()` to return the index of a word in the vocab or an `<UNK>` index otherwise.\n",
        "- `id2unit()` to return a word given its index in the vocab.\n",
        "- `map()` to return a list of indeces given a list of words.\n",
        "- `build_vocab()` to initialize the vocab (is going to be implemented in respective classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AWQIf4Nx6QZO"
      },
      "outputs": [],
      "source": [
        "class BaseVocab:\n",
        "    def __init__(self, data, lower=False):\n",
        "        self.data = data\n",
        "        self.lower = lower\n",
        "        self.build_vocab()\n",
        "        \n",
        "    def normalize_unit(self, unit):\n",
        "        if self.lower:\n",
        "            return unit.lower()\n",
        "        else:\n",
        "            return unit\n",
        "        \n",
        "    def unit2id(self, unit):\n",
        "        unit = self.normalize_unit(unit)\n",
        "        if unit in self._unit2id:\n",
        "            return self._unit2id[unit]\n",
        "        else:\n",
        "            return self._unit2id[UNK]\n",
        "    \n",
        "    def id2unit(self, id):\n",
        "        return self._id2unit[id]\n",
        "    \n",
        "    def map(self, units):\n",
        "        return [self.unit2id(unit) for unit in units]\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        NotImplementedError()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._unit2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "KB0tPL5DLBnn"
      },
      "outputs": [],
      "source": [
        "class PretrainedWordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        self._id2unit = VOCAB_PREFIX + self.data\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "D8tKL56U6pdm"
      },
      "outputs": [],
      "source": [
        "class LabelVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        self._id2unit = self.data\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "maj8w5GEcykb"
      },
      "source": [
        "Next, we need to create the `Pretrain` class to store the pretrained vectors and vocab that we defined above. The vectors are going to be stored in as a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ryz6vtNoLBnt"
      },
      "outputs": [],
      "source": [
        "class Pretrain:\n",
        "    def __init__(self, vec_filename, max_vocab=-1):\n",
        "        self._vec_filename = vec_filename\n",
        "        self._max_vocab = max_vocab\n",
        "        \n",
        "    @property\n",
        "    def vocab(self):\n",
        "        if not hasattr(self, '_vocab'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._vocab\n",
        "    \n",
        "    @property\n",
        "    def emb(self):\n",
        "        if not hasattr(self, '_emb'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._emb\n",
        "        \n",
        "    def read(self):\n",
        "        if self._vec_filename is None:\n",
        "            raise Exception(\"Vector file is not provided.\")\n",
        "        print(f\"Reading pretrained vectors from {self._vec_filename}...\")\n",
        "        \n",
        "        words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n",
        "        \n",
        "        if failed > 0: # recover failure\n",
        "            emb = emb[:-failed]\n",
        "        if len(emb) - len(VOCAB_PREFIX) != len(words):\n",
        "            raise Exception(\"Loaded number of vectors does not match number of words.\")\n",
        "            \n",
        "        # Use a fixed vocab size\n",
        "        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):\n",
        "            words = words[:self._max_vocab - len(VOCAB_PREFIX)]\n",
        "            emb = emb[:self._max_vocab]\n",
        "                \n",
        "        vocab = PretrainedWordVocab(words, lower=True)\n",
        "        \n",
        "        return vocab, emb\n",
        "        \n",
        "    def read_from_file(self, filename, open_func=open):\n",
        "        \"\"\"\n",
        "        Open a vector file using the provided function and read from it.\n",
        "        \"\"\"\n",
        "        first = True\n",
        "        words = []\n",
        "        failed = 0\n",
        "        with open_func(filename, 'rb') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    line = line.decode()\n",
        "                except UnicodeDecodeError:\n",
        "                    failed += 1\n",
        "                    continue\n",
        "                if first:\n",
        "                    # the first line contains the number of word vectors and the dimensionality\n",
        "                    first = False\n",
        "                    line = line.strip().split(' ')\n",
        "                    rows, cols = [int(x) for x in line]\n",
        "                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n",
        "                    continue\n",
        "\n",
        "                line = line.rstrip().split(' ')\n",
        "                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n",
        "                words.append(' '.join(line[:-cols]))\n",
        "        return words, emb, failed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3ktSIbcqdKWO"
      },
      "source": [
        "## Task 1. Define the dataset (3 points)\n",
        "\n",
        "Finally, we need to define the dataset class `MPSTDataSet` that is going to load and preprocess our data files. Inside the data folder, we have different files that are going to help us.\n",
        "\n",
        "### Task 1.1. Read the file names (0.25 points)\n",
        "First, there are `test_ids.txt` and `train_ids.txt` files that contain the names of the files for test and train splits correspondingly. We are going to read them in the `get_filenames()` method and return as a list of strings.\n",
        "\n",
        "### Task 1.2. Read the labels (0.25 points)\n",
        "Then, we have all the names of the labels inside the `tag_assignment_data/tag_list.txt` file. Read this file line by line and return all the labels as a list of strings in the `get_labels()` method.\n",
        "\n",
        "### Task 1.3. Read the label mappings (0.25 points)\n",
        "Next, we have all the corresponding labels for the filename inside the `tag_assignment_data/movie_to_label_data.json` file. Read this file with `json.load()` and return the resulting dictionary in the `get_movie_label_mapping()` method.\n",
        "\n",
        "### Task 1.4. Read the dataset (1.75 points)\n",
        "Finally, read the dataset in the `load()` method. The texts are stored in the `final_plots_wiki_imdb_combined/cleaned/` folder. Each file has the name that we got in the `get_filenames()` method with `.txt` extension. For each file name, returned by the `get_filenames()` method, get the corresponding label from the `self.label_mappings` dictionary. Then, open the corresponding file by adding `.txt` to the end of the filename. Read all the contents and tokenize it with `tokenizer()` function defined in the beginning of this notebook. After that, convert all the tokens into indicies with `self.pretrain_vocab.map()` and store it in the `text` variable. Convert the indicies to the `torch.LongTensor`. Do the same with the labels and `self.label_vocab.map()` and store it in the `label` variable. Convert the label indicies to the `torch.FloatTensor`. \n",
        "\n",
        "### Task 1.5. Transform the labels (0.5 points)\n",
        "Since we have a multi-label classification task, we need to convert the labels into a one-hot tensor. This tensor is going to have the length of our label vocabulary. It will have `0s` in all positions excepts for `1s` in the labels positions. For example, if our labels are `[3, 5]` and we have `7` labels in total, the resulting vector should be `[0, 0, 0, 1, 0, 1, 0]`. \n",
        "\n",
        "_Hint_: You can initialize a zero vector with `torch.zeros` and then fill it with `torch.scatter_`, using the `label` tensor defined before.\n",
        "\n",
        "We also need our custom class to inherit from the `torch.utils.data.Dataset` class. Finally, we need to define the `__len__()` method to know how big is our dataset and `__getitem__()` method to get one sample at a given index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AHnV7tRJLBnx"
      },
      "outputs": [],
      "source": [
        "class MPSTDataSet(Dataset):\n",
        "    def __init__(self, pretrain, label_vocab=None, data_path='.data', test=False):\n",
        "        self.pretrain_vocab = pretrain.vocab\n",
        "        self.data_path = data_path\n",
        "        self.test = test\n",
        "        self.data = []\n",
        "\n",
        "        if label_vocab is None:\n",
        "            labels = self.get_labels()\n",
        "            self.label_vocab = LabelVocab(labels)\n",
        "        else:\n",
        "            self.label_vocab = label_vocab\n",
        "        \n",
        "        self.label_mappings = self.get_movie_label_mapping()\n",
        "        \n",
        "        if self.data_path.exists():\n",
        "            self.load()\n",
        "        else:\n",
        "            raise ValueError(\"Data path doesn't exist!\")\n",
        "\n",
        "        self.data = sorted(self.data, key=lambda x: len(x[0]), reverse=True)\n",
        "        \n",
        "    def get_filenames(self):\n",
        "        if self.test:\n",
        "            ...\n",
        "        else:\n",
        "            ...\n",
        "  \n",
        "    def get_labels(self):\n",
        "        ...\n",
        "\n",
        "    def get_movie_label_mapping(self):\n",
        "        ...\n",
        "\n",
        "    def load(self):\n",
        "        text_path = self.data_path / 'final_plots_wiki_imdb_combined' / 'cleaned'\n",
        "        for fname in self.get_filenames():\n",
        "            text = ...\n",
        "            label = ...\n",
        "            self.data.append((text, label))\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QBTTvrcgewAE"
      },
      "source": [
        "Additionally, we need to define a funciton to pad all the sentences in the batch to the same length. To do this, we are going to first find the longest sequence in the batch and use its length to create a torch tensor of size `(batch_size, max_len)` filled with `0` that is our padding id. Later, we are just going to append each sequence in the beginning of the corresponding row of our new batch tensor. Don't forget that `nn.Embedding` layer that we are going to use later requires indices to be of type `long`. We are also going to put the labels to the `labels` tensor of size `(batch_size, n_labels)`. To be able to use them to calculate the loss, each label must be of type `float`.\n",
        "\n",
        "Finally, don't forget to convert all the tensors to the current device with `.to(device)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xe34jVx2LBn0"
      },
      "outputs": [],
      "source": [
        "def pad_sequences(batch):\n",
        "    max_len = max([len(x[0]) for x in batch])\n",
        "    padded_sequences = torch.zeros((len(batch), max_len), dtype=torch.long)\n",
        "    labels = torch.zeros((len(batch), len(batch[0][1])), dtype=torch.float)\n",
        "\n",
        "    for i, sample in enumerate(batch):\n",
        "      padded_sequences[i, :len(sample[0])] = sample[0]\n",
        "      labels[i, :] = sample[1]\n",
        "\n",
        "    padded_sequences = padded_sequences.to(device)\n",
        "    labels = labels.to(device)\n",
        "    \n",
        "    return padded_sequences, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z-O_HtNqgQfA"
      },
      "source": [
        "Now, we can finally load our data and pretrained vectors. It will take some time..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rRPKP4YfLBn8"
      },
      "outputs": [],
      "source": [
        "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eCMyYcj1LBoC"
      },
      "outputs": [],
      "source": [
        "train_data = MPSTDataSet(pretrain, data_path=DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "U674r0iXLBoJ"
      },
      "outputs": [],
      "source": [
        "test_data = MPSTDataSet(pretrain, train_data.label_vocab, DATA_PATH, test=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CTiKgSxdhMkO"
      },
      "source": [
        "The last step in our data preparation is to define the train and validation splits. We are going to use the validation set to see how the model performs during the training. It is important to be able to see if the model is overfitting or not.\n",
        "\n",
        "To do that, we will just create a range of indices from `0` to the size of the training data. Then, we are going to define an index on which we are going to splite the data. Optionally, we can shuffle our indices before splitting.\n",
        "\n",
        "With these indices for train and validation datasets, we are going to create two corresponding `torch.utils.data.SubsetRandomSampler` objects that we are going to pass to the `torch.utils.data.DataLoader` objects in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "F_6Y55v9LBn4"
      },
      "outputs": [],
      "source": [
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset:\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zuyq7dLjiRZ1"
      },
      "source": [
        "Here, for each set, we are going to create a `DataLoader` object that is going to create a batch iterator for us. We will pass to it out `MPSTDataSet` object as a source of data. Batch size as a `batch_size` argument. To specify train and validation splits, we are going to pass the corresponding `SubsetRandomSampler` objects as a `sampler` argument for the training set. Finally, we need to pass our `pad_sequences()` function as a `collate_fn` argument to tell the data loader how to prepare the batches so that they have the same length. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wgn22jTLLBoN"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, collate_fn=pad_sequences)\n",
        "validation_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, collate_fn=pad_sequences)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=pad_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JyqT79e3jL4r"
      },
      "source": [
        "## Task 2. Define the model (0.5 points)\n",
        "\n",
        "In this task, you just need to change the `OUTPUT_DIM` variable to match our task.\n",
        "\n",
        "_Hint_: Use `train_data.label_vocab`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DnEhkstjLBoQ"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, pretrain, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "                \n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.from_numpy(pretrain.emb), \n",
        "            padding_idx=pad_idx, \n",
        "            freeze=True\n",
        "        )\n",
        "        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):           \n",
        "        #text = [batch size, sent len]\n",
        "\n",
        "        embedded = self.embedding(text)     \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)  \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]    \n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "                \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]     \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lrzLk7fTLBoV"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(pretrain.vocab)\n",
        "EMBEDDING_DIM = pretrain.emb.shape[1]\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = ...\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = CNN(pretrain, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XtCisPr9LBoc"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5NxltvyeA5Ye"
      },
      "source": [
        "## Task 3. Define your metrics (1.5 points)\n",
        "\n",
        "### Task 3.1. Coding (1 point)\n",
        "In the Lab, we used just accuracy as a metric to measure the performance of our model. This can work fine for a binary classification task, since we only have two classes, but it can backfire on a milti-label classification task. Imagine, the we have 100 labels in total. For some example, the correct labels are only of the index 10 and 20. The model output is all `0s`. Now, if we calculate the accuracy as \n",
        "\n",
        "$\\text{accuracy} = \\frac{\\text{true positives } + \\text{ true negatives}}{\\text{true positives } + \\text{ true negatives } + \\text{ false positives } + \\text{ false negatives}} = \\frac{0 + 98}{0 + 98 + 0 + 2} = 0.98$. \n",
        "\n",
        "That's not what we really want, because in this case the model didn't output anything at all.\n",
        "\n",
        "To get a more accurate idea about the model's performance, we are going to calculate additional metrics. Define the following metrics in the function bellow:\n",
        "\n",
        "$\\text{accuracy} = \\frac{\\text{true positives } + \\text{ true negatives}}{\\text{true positives } + \\text{ true negatives } + \\text{ false positives } + \\text{ false negatives}}$\n",
        "\n",
        "$\\text{precision} = \\frac{\\text{true positives}}{\\text{true positives } + \\text{ false positives}}$\n",
        "\n",
        "$\\text{recall} = \\frac{\\text{true positives}}{\\text{true positives } + \\text{ false negatives}}$\n",
        "\n",
        "$F_1 = 2 \\cdot \\frac{\\text{precision } \\cdot \\text{ recall}}{\\text{precision } + \\text{ recall}}$\n",
        "\n",
        "In this function, we are going to round our predictions to the closest integer, i.e. everything greater than $0.5$ will become $1$ and everything less than $0.5$ will become $0$. This is probably not the best threshold for our model. Selecting a good threshold can drastically improve the performance of your model, especially if the labels are unballanced (as in our case). You can find more about it in [this presentation](https://users.ics.aalto.fi/jesse/talks/Multilabel-Part01.pdf). However, in this Homework, we are not going to estimate the best threshold, since it takes additional time and effort.\n",
        "\n",
        "To find true positives, false positives, false negatives, and true negatives, we can build a $\\text{confusion vector} = \\frac{\\text{rounded preds}}{y}$. You can use the following properties of the `confusion_vector` to find the stats:\n",
        "\n",
        "    1     where prediction and truth are 1\n",
        "    inf   where prediction is 1 and truth is 0\n",
        "    nan   where prediction and truth are 0\n",
        "    0     where prediction is 0 and truth is 1\n",
        "\n",
        "_Hint_: You might want to use `torch.isinf` and `torch.isnan`.\n",
        "\n",
        "In these metrics, we are going to use _micro-averaging_. It means that we need to sum and the positives and negatives before calculating the metrics. You can read more about it [here](https://datascience.stackexchange.com/a/24051).\n",
        "\n",
        "_Hint_: When summing the positives and negatives, don't forget to specify `dtype=torch.float` to make sure that the division will be correct when you are going to calculate the metrics.\n",
        "\n",
        "### Task 3.2. Sigmoid vs Softmax (0.5 points)\n",
        "For this task, we are using the Sigmoid activation function again, as in the Lab on binary classification. This is because we treat our current task as $n$ binary classification tasks, where $n$ is the number of labels. However, there is another popular fucntion called _Sotfmax_. \n",
        "\n",
        "Read more about the Softmax function [here](https://pytorch.org/docs/stable/nn.html?highlight=softmax#torch.nn.Softmax) and [here](https://en.wikipedia.org/wiki/Softmax_function) and answer the following questions. \n",
        "\n",
        "__Why we do not use the Softmax fuction for a multi-label classification task?__\n",
        "\n",
        "<font color='red'>Your answer here</font>\n",
        "\n",
        "__What kind of classification task should we have to justify the use of the Softmax function?__\n",
        "\n",
        "<font color='red'>Your answer here</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "a0sYlvYpLBoi"
      },
      "outputs": [],
      "source": [
        "def multi_label_accuracy(preds, y):\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    confusion_vector = rounded_preds / y\n",
        "    \n",
        "    true_positives = ...\n",
        "    false_positives = ...\n",
        "    false_negatives = ...\n",
        "    true_negatives = ...\n",
        "\n",
        "    accuracy = ...\n",
        "    precision = ...\n",
        "    recall = ...\n",
        "    f_score = ...\n",
        "    return accuracy, precision, recall, f_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eEvv1JenLBon"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f_score = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch[0]).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch[1])\n",
        "        \n",
        "        acc, precision, recall, f_score = multi_label_accuracy(predictions, batch[1])\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        epoch_precision += precision.item()\n",
        "        epoch_recall += recall.item()\n",
        "        epoch_f_score += f_score.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), \\\n",
        "        epoch_precision / len(iterator), epoch_recall / len(iterator), \\\n",
        "        epoch_f_score / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7yw_-9W8LBot"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f_score = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch[0]).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch[1])\n",
        "            \n",
        "            acc, precision, recall, f_score = multi_label_accuracy(predictions, batch[1])\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            epoch_precision += precision.item()\n",
        "            epoch_recall += recall.item()\n",
        "            epoch_f_score += f_score.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), \\\n",
        "        epoch_precision / len(iterator), epoch_recall / len(iterator), \\\n",
        "        epoch_f_score / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dzP7wNcALBoz"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OLbyeMPqdURd"
      },
      "source": [
        "### Training and testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sSmBPJNqLBo5"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc, train_precision, train_recall, train_f_score \\\n",
        "        = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc, valid_precision, valid_recall, valid_f_score \\\n",
        "        = evaluate(model, validation_loader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'mpst_cnn_classifier.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | ' +\n",
        "          f'Train Precision: {train_precision*100:.2f}% | Train Recall: {train_recall*100:.2f}% | ' +\n",
        "          f'Train F1-score: {train_f_score*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | ' +\n",
        "          f'Val. Precision: {valid_precision*100:.2f}% | Val. Recall: {valid_recall*100:.2f}% | ' +\n",
        "          f'Val. F1-score: {valid_f_score*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3C-LI6oGrRF-"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "test_loss, test_acc, test_precision, test_recall, test_f_score \\\n",
        "    = evaluate(model, test_loader, criterion)\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "print(f'Epoch: test | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | ' +\n",
        "      f'Test Precision: {test_precision*100:.2f}% | Test Recall: {test_recall*100:.2f}% | ' +\n",
        "      f'Test F1-score: {test_f_score*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VnQ9uy8NhDHj"
      },
      "source": [
        "## Task 4. Analyze the model's performance (0.5 points)\n",
        "\n",
        "Look at the metrics while training and testing. Briefly describe what they tell you about the model:\n",
        "\n",
        "<font color='red'>Your answer here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bJ2k7s81Dony"
      },
      "source": [
        "## Task 5. Test the model yourself (0.5 points)\n",
        "\n",
        "Once we trained our model, we can try to predict the sentiment of our own input. We are going to define the `predict_sentiment()` function that is going to take our trained model and a text as an argument. \n",
        "\n",
        "First, we need to switch the model to evaluation mode be calling `model.eval()` on it. Then, we are going to tokenize the sentence the same way as we tokenized the input. If the sentence is less than `min_len` parameter, we are going to add the padding symbols to it, so our model doesn't throw an error. After that, we turn the words into indices with the same vocabulary that we built for training. Finally, we transform the output into tenson and adding an empty dimention in the beginning, imitating a batch of size 1.\n",
        "\n",
        "Here, we are not going to use the $0.5$ threshold since it can result in all the label probabilities being `0s`. Instead, we are going to look at top-3 labels for each prediction.\n",
        "\n",
        "Below, I've provided plot synopses for [Joker](https://www.imdb.com/title/tt7286456/?ref_=nv_sr_srsg_0) and [Madagaskar](https://www.imdb.com/title/tt0351283/?ref_=fn_ft_tt_1). With my model, I've got `['murder', 'violence', 'cult']` for Joker and `['psychedelic', 'comedy', 'humor']` for Madagaskar. Those predictions may be not perfect but they seem plausible to me. \n",
        "\n",
        "You can try to put synopses for your own favourite (or not) movies and share the predictions that you've got.\n",
        "\n",
        "<font color='red'>Your answer here</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CS4ZMuw-LBo_"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(model, sentence, min_len = 5):\n",
        "    model.eval()\n",
        "    tokenized = [token.text for token in tokenizer(sentence)]\n",
        "    if len(tokenized) < min_len:\n",
        "      tokenized += [PAD] * (min_len - len(tokenized))\n",
        "    indexed = pretrain.vocab.map(tokenized)\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    \n",
        "    prediction, indices = torch.topk(torch.sigmoid(model(tensor)), 3)\n",
        "    labels = [train_data.label_vocab.id2unit(idx) for idx in indices[0]]\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "v_EUiairzA0y"
      },
      "outputs": [],
      "source": [
        "joker_synopsis = \"\"\"The story takes place in Gotham City, 1981.\n",
        "\n",
        "Arthur Fleck (Joaquin Phoenix) works as a clown-for-hire for a company called Ha-Ha's. He struggles with severe depression personally but finds some form of optimism in performing for others and trying to make people laugh. He is tasked with advertising a store by dancing and waving a sign around. On one such occasion, the sign gets snatched by a group of punk teens, forcing Arthur to chase them into an alley. They smash the sign against his face and proceed to mercilessly kick him while he's down.\n",
        "\n",
        "In this era, Gotham is struggling with crime, unemployment, and poverty. Arthur visits a social worker for his medication, as well as his ongoing mental health issues. On the bus ride home, a small child looks at Arthur. He makes silly faces that amuse the boy, but his mother tells Arthur to leave him alone. Arthur begins to laugh hysterically and uncontrollably. When the mother questions him, he hands her a card that explains that he has a mental condition that causes him to laugh the way that he does.\n",
        "\n",
        "Arthur returns home, where he lives with his ailing mother, Penny (Frances Conroy). They sit and watch a talk show with host Murray Franklin (Robert DeNiro). Arthur imagines himself being on the show and getting Murray's attention. In his fantasy, Arthur charms the audience and Murray by telling them that he takes care of his mother. Murray relates to Arthur and invites him up on stage in front of everyone, where they share a familial embrace. It is revealed that Penny used to work for Thomas Wayne (Brett Cullen) and is obsessed with the millionaire and has been currently writing to him to try and better their living situation.\n",
        "\n",
        "At Ha-Ha's, Arthur is given a gun for protection by his co-worker Randall (Glenn Fleschler) after he hears about the mugging incident. Arthur is both reluctant and relieved to receive such a gift as firearms are outlawed at work but soon finds his confidence growing after receiving the weapon. However, soon after this, he is confronted by his cold and unfeeling boss, who reprimands him for losing the sign and takes the cost of it out of his pay. Arthur responds only by smiling bitterly.\n",
        "\n",
        "Arthur is infatuated with his neighbor, single mother, Sophie Dumond (Zazie Beetz). She speaks to him politely about relating issues that he can relate to. However, while trying to make an impression with her, he appears awkward and weird around her. At one point, he spends his day following her. Later, she comes by his apartment and asks if he was following her, and he admits that he was, but she doesn't seem put off by it. He invites her to a stand-up comedy show that he is performing at. She is hesitant but is won over by his charm and sense of humor. Arthur watches comedians perform to help him gain some insight into the craft, but feels more awkward and out of place as his over-the-top laughter is not genuine.\n",
        "\n",
        "Arthur goes to the comedy club for his performance. His nervousness consumes him and, as a coping mechanism, unintentionally finds himself laughing so hard that he can barely speak. He then begins going off into his routine, which isn't very funny. Sophie appears to be in the audience... the only person who is laughing at Arthur's jokes. This gives him the comfort he needs to continue to joke despite his inner torment and turmoil.\n",
        "\n",
        "Arthur later goes to a children's hospital to entertain them as a clown. He brought his gun with him, and it falls out on the floor. Arthur's boss later chews him out for this. Arthur pleads for a second chance, but his boss refuses and fires him on the spot. To top things off, Randall throws Arthur under the bus by claiming that Arthur got the gun himself. On the subway train ride home from Ha-Ha's in full clown getup, Arthur spots three drunk young Wall Street types working for Wayne Enterprises harassing a woman. Arthur starts laughing unintentionally and draws the attention of the men, while the woman wisely flees from that car. The men approach Arthur and mock him and his laughter before they start to beat him. Arthur fights back in self-defense, but they team up, and relentlessly beat him to the floor. Having had enough, Arthur then pulls out his gun and shoots two of them dead in self-defense before following the last guy out of the train and murdering him on the stairs.\n",
        "\n",
        "In shock over what he just did, Arthur retreats into a bathroom. After a moment of frantic contemplation, he finds a force rising within him, and he begins to dance by himself. At this moment, he sees himself in the dirty mirror as a battered and smeared and yet powerful clown and begins to embrace it. He hides the gun and then returns to the apartment where he meets and kisses Sophie for the first time.\n",
        "\n",
        "The news of the three murders spreads, with some seeing it as an attack on the wealthy, while others support the act. Thomas Wayne speaks out and condemns it, labeling the lower class as \"clowns,\" which becomes a symbol they readily embrace. The next day, Arthur cleanse out his locker at Ha Ha's but not before confronting Randall about betraying him and breaking the time punching machine. He then leaves, feeling high-spirited and free. News reports show clown rioters protesting through the city and wreaking trouble, condemning the higher privileged. Arthur sees that he has inadvertently caused this and begins to see his true potential, which makes him genuinely delighted.\n",
        "\n",
        "Arthur later finds one of Penny's letters to Thomas, which indicates that Arthur is Thomas's son. Arthur goes to Wayne Manor, where he meets young Bruce (Dante Pereira-Olson). After performing a magic trick for Bruce, he sticks his hands through the gate and forces Bruce to smile, realizing deep within that they may or may not be brothers. However, Alfred (Douglas Hodge) comes to intervene and tell Arthur to leave. Arthur mentions his mother and her involvement with Thomas, but Alfred says he remembers Penny and that she was lying to him. Arthur attacks and nearly strangles Alfred but then notices that Bruce is watching. Arthur then gets hold of himself and flees the Wayne premises.\n",
        "\n",
        "Arthur finds Thomas at a public art theater event and tries to confront him with the potential of him being his father. Arthur mentions Penny, whom Thomas also remembers. He says she was delusional and that there's no way Arthur could be his son. Thomas also explains that Penny never told Arthur that he was adopted, which Arthur strongly rejects before uncontrollably laughing in Thomas's face. Thomas, unaware of Arthur's condition, becomes defensive and punches Arthur in the face before having the man is thrown out of the building. Arthur returns home, where he tortures himself with the fridge in a fit of depression and longing.\n",
        "\n",
        "Two police detectives, Burke (Shea Wigham) and Garrity (Bill Camp) go to Arthur's apartment to question him on the subway murders due to the word that the suspect was wearing clown make-up, and they know Arthur lost his job earlier that day. Arthur denies any involvement and gets the detectives to leave. Not long after, Penny falls ill and is hospitalized. Sophie sits by Arthur as he tends to his mother. In the hospital, Arthur sees that Murray's show is playing a clip from his stand-up routine, but he is hurt to see that Murray only played it to mock Arthur.\n",
        "\n",
        "Arthur later receives a phone call from a rep for Murray's show. He is invited to appear as a guest, which Arthur reluctantly accepts. After studying other interviews on the comedy show, Arthur decides to commit suicide in front of the live audience, thinking it will make them laugh.\n",
        "\n",
        "Seeking hard proof, Arthur goes to Arkham Asylum and speaks to a clerk, Carl (Brian Tyree Henry), who has a file on Penny. When Carl says he can't give Arthur the info he wants, Arthur snatches the file and runs away to read it. Once away, Arthur opens the documents and reads them, finding that Thomas was telling the truth- according to the documents. The reality is that Penny adopted Arthur after he was found abandoned, and she abused him, tying him to a radiator and beating him alongside her abusive boyfriend. One part of the file mentions Arthur having a head injury, which is most likely what caused his laughing condition. Arthur returns to the hospital and tells Penny that he thought his life was a tragedy, but he sees it's a \"fucking comedy.\" With that, he smothers Penny to death.\n",
        "\n",
        "Arthur goes back home and breaks into Sophie's apartment. She sees him and is terrified, asking him to leave for the sake of her daughter. Arthur asks her if she has ever had \"a really bad day,\" to which she replies that she doesn't even know him. Through this, it is revealed that every other moment featuring Sophie was just in Arthur's head. A broken and frustrated Arthur leaves Sophie alone, storming out of the apartment.\n",
        "\n",
        "Arthur starts to get ready for his appearance on Murray's show and paints his face white. He is visited in his apartment by Randall and another former co-worker, named Gary (Leigh Gill). They offer condolences after they hear about Penny's death, but then Randall begins mentioning Burke and Garrity going to their apartments to question them about the subway murders. Arthur realizes that Randall is only seeking a way to use Arthur in order to cover his own butt and then snaps, brutally stabbing Randall twice in the face before smashing his head against the wall. A terrified Gary questions Arthur's deeds and begs to be let go. Arthur agrees to before playfully scaring him as a prank. Gary tries to undo the lock on Arthur's door but is unable to due to his height. He asks Arthur to open the door for him to which Arthur immediately agrees, pausing once to thank Gary for being the only person in his life who was nice to him. Arthur kisses Gary on the forehead and lets him go.\n",
        "\n",
        "Arthur then dyes his hair green, puts on full clown make-up, and dons a burgundy suit. He then dances down the stairways, fully embracing his insanity and carefree life. Burke and Garrity find Arthur dancing in the street and move in to arrest him. Arthur runs, and they chase him into the subway train where dozens of other Gotham citizens are dressed like clowns after being inspired by the murders. Arthur hides his face with a clown mask, which he steals from a protester and inadvertently starts a brawl in the train cars. As the detectives pursue Arthur, one clown gets in the way, and Burke accidentally shoots him dead when they struggle with his gun. The clowns pull the detectives out of the subway and start beating them relentlessly, allowing Arthur to get away, moving smoothly through the police forces which swarm the area.\n",
        "\n",
        "At the TV station, Arthur meets Murray and his agent Gene (Marc Maron). Before he goes on, Arthur asks Murray to introduce him as \"Joker,\" since Murray referred to him as such when playing his clip. Murray asks Arthur if his clown make-up has political agendas behind it to which Arthur replies, \"I don't believe in that. I don't believe in anything.\" While waiting to be introduced, Arthur sees Murray broadcasting a clip of a struggling Arthur trying to tell a joke. This causes Arthur's mind and plans to change, and then he dances out into the spotlight.\n",
        "\n",
        "Arthur goes out as the show begins. He awkwardly tells Murray a joke, which he finds funny for its dark humor though nobody else does. After being confronted with this, Arthur continues by admitting to the subway murders. Murray and the audience slowly realize that Arthur is serious. Arthur argues that the audience only cares for the victims because Thomas Wayne spoke for them, but anyone else like Arthur would be ignored and walked over. Murray and the audience grow angrier with Arthur, but so does he. Murray scolds Arthur, which escalates into Arthur snapping and telling another joke, grinning giddily. \"What do you get when you cross a mentally ill loner with a society that abandons him and treats him like trash?!\" he asks, only for Murray to try shutting him off before calling for the police. An enraged Arthur then screams, \"You get what you fucking' deserve!\" before blowing Murray's brains out in front of everyone. The audience runs away in terror, and the news of the murder immediately hits the airwaves. Arthur then laughs genuinely for the first time in his life.\n",
        "\n",
        "Gotham is now overrun by rioting citizens dressed as clowns after hearing about what Arthur did. The Waynes leave a movie theater to find the chaos in the streets. Thomas takes Martha (Carrie Louise Putrello) and Bruce into an alley, but one clown follows them and tells Thomas he is getting what he deserves using the punchline that Arthur used on the Murray Franklin show. With that, he shoots Thomas and Martha dead in front of Bruce. Meanwhile, Arthur has been arrested and is being taken by the police. Arthur looks out the window and laughs gleefully as he sees the destruction he has caused. Just then, the clowns in an ambulance run into the car, killing the cops and freeing Arthur, who is injured and unconscious. When he awakes, Arthur finds himself surrounded by a mob of cheering mobsters in clown masks. The rioters then cheer Arthur on as he stands on a car and embraces their admiration, now that he has gotten the recognition he has long desired. He dances to their cheering and then pauses, finding that his nose is bleeding profusely. He then spreads the blood across his upper lip and grins before standing before them, elevated like a god.\n",
        "\n",
        "Sometime later, Arthur is locked up in Arkham. He laughs after telling this story and visualizes a young Bruce standing over his parents in the alley. Realizing that he has, in a way, turned Bruce into himself, Arthur laughs some more, finding this genuinely hilarious. He meets a new social worker (April Grace) and says he wants to tell her a joke, but she wouldn't get it. A few minutes later, Arthur then steps out of the room, leaving a trail of bloody footprints behind before he is chased around by orderlies.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aToaCvbyC7fW"
      },
      "outputs": [],
      "source": [
        "predict_sentiment(model, joker_synopsis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WHlmfiEm2So1"
      },
      "outputs": [],
      "source": [
        "madagascar_synopsis = \"\"\"At New York City's Central Park Zoo, Marty the zebra (Chris Rock) is walking on a treadmill, daydreaming about running free in the wild. He swings on vines, jumps, does flips and runs through a bunch of singing penguins. Marty is jolted back to reality by his best friend, Alex the lion (Ben Stiller), who gets in his face and roars.\n",
        "\n",
        "Alex tells Marty that there's something in his teeth, so Marty tells him to open his mouth and let Dr. Marty, D.D.S., have a look. Marty reaches in and extracts a glass ball with a red ribbon on top. It's a snow globe, with a miniature Alex figure in the middle. It's a tenth birthday present for Marty from Alex. Marty puts the globe among a bunch of other \"Alex\" themed items that he'd been given over time. The gift did not excite him. In fact, he's bored and upset that his life at the zoo limits his ability to enjoy life more fully by keeping his movements restricted.\n",
        "\n",
        "Today was not only Marty's birthday, but \"Field Trip Day\" at the zoo, when all the school kids come. It was one of Alex's favorite days, and he excitedly woke up his other friends, Gloria the hippopotamus and Melman the giraffe, so they could get themselves ready. That's difficult for Melman, as he's a hypochondriac and worries about all sorts of ailments he thinks he may have.\n",
        "\n",
        "Alex quickly counsels Marty to work on changing his attitude, suggesting that he approach every day in a \"fresh\" way. So, Marty decides he's going to be \"fresh\" today.\n",
        "\n",
        "Mason the Chimpanzee (Conrad Vernon) starts his day off by rummaging through the waste can and coming up with a cup of coffee, bagel and newspaper, which he shares with his companion, Phil, another chimpanzee, who can't speak and just uses sign language.\n",
        "\n",
        "Once the gates to the zoo open, Gloria, Marty and Alex go into action, putting on an exhibition of dancing, posing, and doing acrobatics for the visiting people. They were very popular.\n",
        "\n",
        "There are four penguins at the zoo who are in the process of digging an escape tunnel. They are using plastic spoons and popsicle sticks to dig with, but their tools keep breaking and slowing them down.\n",
        "\n",
        "Marty is surprised when there's a small area of the grass in his compound that suddenly bulges upward, and the head of Skipper the penguin (Tom McGrath) appears, followed by those of his fellow penguins: Private (Christopher Knights), Kowalski (Chris Miller), and Rico. One of them asks what continent this is, and Marty says, \"Manhattan.\" They realize that they've somewhat missed the mark, as their destination was Antarctica, so they go back down to continue digging, but first swearing Marty to silence.\n",
        "\n",
        "Marty, Gloria, Melman and Alex all get V.I.P. treatments after the zoo closes, with the freshest of whatever foods they prefer, massages, and acupuncture. It's about as good as a life of confinement can be.\n",
        "\n",
        "Gloria's birthday gift to Marty is a cake, while Melman gives him a thermometer, not telling him it's his old rectal thermometer until Marty had put it in his mouth to try it out.\n",
        "\n",
        "Marty's birthday wish is to go to the wild. When Marty tells his skeptical friends that the penguins are trying to escape to the wild, Alex replies, \"the penguins are psychotic.\" They engage in a discussion about where the nearest wild might be, and Gloria says she heard there are wild places in Connecticut.\n",
        "\n",
        "Alex tells Marty that there wouldn't be things like the fresh steaks he likes so much out in the wild. When Marty asks his friends if they aren't bothered by not knowing about the world outside the zoo, they simultaneously say, \"no.\"\n",
        "\n",
        "Marty continues to be depressed, so Gloria makes Alex go and attempt to give him a pep talk. Alex knows that if he sings \"New York, New York,\" that Marty won't be able to resist joining in, so he does that. Marty does join in, but the noise they make causes the other animals to start waking up and they shout out for Marty and Alex to \"shut up.\"\n",
        "\n",
        "Marty tries to convince Alex to join him in breaking out and traveling north to Connecticut. Alex isn't interested, besides tomorrow is \"Senior's Day,\" at the zoo and he doesn't want to miss that.\n",
        "\n",
        "Later that night, after the animals have all gotten back to sleep, Alex is wakened by Melman, who normally wakes up every two hours to pee. Melman tells Alex that Marty wasn't in his compound. Gloria comes over too, and they all wonder where Marty might be. Alex grabs a nearby phone and calls 911, before he realizes \"we can't call the people.\"\n",
        "\n",
        "Marty is sauntering down a main street in downtown New York, headed for Grand Central Station. He stares at a woman walking by who is wearing a zebra striped outfit. He spends some time ice skating at an ice rink, then stops and talks to a police horse (David Cowgill), who gives him directions to Grand Central. The police officer (Stephen Apostolina), riding the horse calls into the precinct and asks if he can shoot the zebra. He is told no, that it's animal control's responsibility.\n",
        "\n",
        "Melman lifts Alex over the zoo wall and lowers him to the street. Gloria just busts through the brick wall, and Melman follows her out. Mason and Phil also go out through the hole in the wall. They all go to the nearest subway station and get in one of the cars, scaring and upsetting all the passengers. Before boarding the train, Melway goes into a restroom and comes out with one of those blue deodorizers in his mouth (he liked how it tasted).\n",
        "\n",
        "The animals ride the subway down to Grand Central Station, which is the same place Marty went. Wherever Alex goes, the humans all freak out and run, which he doesn't understand because he's not after them at all. There's an old lady (Elisa Gabrielli) who isn't afraid of him, and actually calls him a \"bad kitty\" and proceeds to start beating on him with her purse.\n",
        "\n",
        "When they find Marty, Alex rushes forward and tackles him, then hugs him, and finally chokes him, alternating in his emotions of concern, relief and anger. Marty tells Alex that he was going to come back to the zoo in the morning, after his little excursion to Connecticut.\n",
        "\n",
        "Hundreds of police show up and surround the animals. By then, the penguins and chimps have arrived and joined Marty, Alex, Gloria and Melman at the station. All the humans are on edge, and very afraid, especially of Alex. Except the old woman, who kicks Alex in the groin.\n",
        "\n",
        "Alex attempts to speak to the police and reason with them, but they don't understand him. So, he roars, in imitation of his popular daily zoo performance. The animal control officer is finally able to steady his nerves enough to shoot Alex in the butt with a tranquilizer dart. That puts Alex out. When Alex starts coming to, he's back at the zoo and there's an animal rights activist making a speech to a crowd about how the zoo animals should be returned to the wild. When they see Alex coming awake, they all get scared and run. Animal Control again responds by shooting at Alex with multiple darts. One of them hits him in the paw and he goes back to sleep.\n",
        "\n",
        "The next time Alex wakes up, he's in a wooden crate, as are Marty, Gloria, Melman, the penguins and the chimps. They are on a large ship that is sailing to Africa. The boxes that the animals are in are all labeled: \"Kenyan Wildlife Preserve.\" Phil can read, so he signs that bit of information to Mason, who informs the rest of them.\n",
        "\n",
        "Rico coughs up a paper clip and uses it to pick the lock to the box holding the penguins. The four of them then waddle their way up to the bridge, disabling a crewman along the way, and administering a karate chop to the back of the neck of the captain of the ship, taking him out.\n",
        "\n",
        "Alex, Marty and Gloria all start arguing about their predicament and what they should do. Meanwhile, the penguins are on the bridge of the ship and struggling to figure out how to steer and navigate it. They more or less accidentally figure it out and Skipper orders hard right rudder. When the ship lurches, the crates holding Alex, Marty, Gloria and Melman, all fall overboard and are set adrift as the ship moves away.\n",
        "\n",
        "After traveling some distance, the crate holding Alex starts rolling and bouncing. It comes to a sudden stop and breaks open, sending Alex head over heels onto a sandy beach. He comes up coughing, with a mouthful of sand. He's all alone and spends a long time roaming the beach, calling out to his friends, and at one point, he even calls for Regis, Kelly, Matt, Katie, and Al.\n",
        "\n",
        "Suddenly, there's the sound of another voice and Alex looks up to see a crate with four legs sticking out of the bottom, running around the beach. It's Melman. Alex hurries over and attempts to free Melman from the crate. He pulls Melman's neck way out, but that doesn't work, so he grabs a coconut tree log and prepares to ram it into Melman's stomach to force him out of the crate. He steps way back, points the log, and starts running. Just before he slams into Melman, something distracts him and he stops short. Coming onto the beach through the surf is a large crate containing Gloria. Once the crate hits the beach, Gloria kicks one side of her crate out, freeing herself and at the same time sending Alex flying through the air and crashing down on top of Melman, smashing his crate and freeing him. There are two starfish and a crab covering Gloria's private parts, so she announces that \"the party's over,\" and they all scatter.\n",
        "\n",
        "Marty is next to arrive, only he does so in style, riding onto the beach on the backs of some dolphins. Alex is surprised to see Marty, but in short order he realizes that all this grief he and the others are experiencing is because of Marty, so he starts chasing Marty around the beach, intending to beat him up. Melman and Gloria intervene and the four of them begin wondering just where they are. Melman looks around and offers his opinion that they are somewhere near San Diego, given the terrain and vegetation. Alex decides to chase Marty some more, because he doesn't want to be in San Diego where he likely won't be the star of the zoo anymore. Gloria stops Alex.\n",
        "\n",
        "Alex hears some sounds coming from deep in the jungle. It sounds like humans, so they all charge off towards the noise. Alex has trouble when he keeps running into things, including a large spider web, that causes him to fall behind. Meanwhile, Gloria, Melman and Marty come across a clearing that is filled with about a hundred lemurs of various sizes and ages. They don't recognize what sort of animals the lemurs are, just that they definitely aren't human. They watch the lemurs as they dance, sing and generally carry on. Melman tells the others that he's counted 27 health code violations taking place.\n",
        "\n",
        "A lemur named Maurice (Cedric the Entertainer) calls for quiet and introduces the lemur King Julien XIII (Sacha Baron Cohen). King Julien then launches into a rap song, \"I Like to Move It.\" The lemurs are all enjoying themselves when suddenly an alarm is sounded and a lemur shouts that the fossa are coming. There are four fossa, which are animals that look like a cross between a cat and a dog, and prey on the lemurs. The lemurs all run, but the fossa catch a baby lemur named Mort (Andy Richter), and start making a salad, with Mort as the main ingredient.\n",
        "\n",
        "Alex finally catches up with his friends and as he gazes down on the scene before him, Gloria sees a large spider on his back. She picks up a stick, preparing to swat the spider. Before she can do that, the spider speaks, saying hello. That causes Alex to look and when he sees the spider, he freaks out, letting out a huge roar, which frightens the fossa and they run away, allowing the Mort lemur to escape. Meanwhile, Gloria starts hammering away at the spider on Alex's back, nearly beating him unconscious in the process.\n",
        "\n",
        "King Julien and his fellow lemurs don't know what to make of the strange animals who have suddenly appeared and scared the fossa away. He decides they must be aliens. To confirm whether the aliens are friendly or not, the king grabs Mort and tosses him towards them. Alex approaches Mort and attempts to make friends, but he's frightened of him and cries. Gloria picks up baby Mort and calms him down. The king now decides that these aliens are a bunch of pansies.\n",
        "\n",
        "King Julien, aka, \"The Lord of the Lemurs,\" steps forward and says, \"welcome giant pansies!\" Alex decides that all the lemurs must be some sort of squirrels, because they act so weird. The king asks \"where are you giants from?\" and when Alex says, \"New York,\" the king says, \"All hail the New York Giants!\"\n",
        "\n",
        "Alex asks the king about any people on the island, knowing that they need people to come find and rescue them. The king says there are people on the island, but they aren't very active. He then points up at a tree, where there's a skeleton hanging from a parachute. Not far away are the remnants of an airplane, also perched high in a tree. The king says there are no live people on the island.\n",
        "\n",
        "Alex loses it and takes off for the beach, intending to jump in the ocean and swim back to New York. Gloria again has to stop him and calm him down. She assures him that the people must be missing them and should be coming at any time to rescue them.\n",
        "\n",
        "Melman decides their situation is hopeless and he digs himself a grave there on the beach. Extending out from the grave is a long will and testament that he'd written in the sand. He starts to read it to the others, informing them of what he will leave to each of them, when a wave comes rolling in and erases the bottom third of the will. Melman says, \"sorry Alex,\" as that portion of the will pertained to Alex's inheritance.\n",
        "\n",
        "Marty decides that he doesn't care if the humans come to find and rescue them, because he loves his newfound freedom there in the wild. That upsets Alex and he takes that large coconut log and draws a line in the sand, telling Marty that he must stay on the other side of that line, while Alex and the others, who want to be rescued, will stay on the opposite side. That doesn't bother Marty, as he immediately sets about building himself a little beachside patio, complete with a large umbrella, fire pit, bar and lounge seats.\n",
        "\n",
        "Alex starts building something too. It ends up being a large figure, similar to the Statue of Liberty, and he calls it the \"Beacon of Liberty.\" His intent is to set it ablaze once they see a ship out on the ocean, so the people will see it and come to their rescue. While Marty has already started a little fire in his fire pit, Alex has ordered Melman to work on starting a fire for them. Melman is getting very tired rubbing two sticks together, trying to create a spark. However, he eventually does get some sparks, then flames, but the flames catch the sticks on fire and Melman becomes frightened and starts running around. He accidentally sets the Beacon of Liberty on fire and it quickly burns down. Alex can't believe it.\n",
        "\n",
        "Back at the lemurs place, the king is telling his fellow lemurs that he wants to make the New York Giants their friends, especially Alex, who would then keep the fossa away from them. Maurice then poses a question to them all, asking them to consider why it is the fossa are afraid of Alex, and perhaps the lemurs should be afraid of him too. No one seems particularly concerned.\n",
        "\n",
        "Back on the beach, Alex has built another structure, made of coconut logs. This one spells out the word, \"HELP.\"\n",
        "\n",
        "Melman and Gloria decide to join Marty on the \"fun side\" of the island, and are over sitting under the umbrella, near the fire pit. Marty invites Alex to join them, telling him that it's not really the fun side without him there. Marty continues to refuse. As he sits there pouting, the \"P\" in his help sign collapses.\n",
        "\n",
        "After some time, Alex decides to go over to the fun side, apologizing to Marty and asking to join the others. Marty welcomes him to \"Casa del Wild,\" and prepares Alex a drink, which he serves to him in a coconut shell. Alex takes a big gulp and immediately spits it out. Marty has to explain that the drinks are just sea water, but that's only until the plumbing is fixed. Gloria, Melman, Marty and Alex then continue sipping their drinks and spitting it out.\n",
        "\n",
        "Marty then prepares something he calls \"seaweed on a stick,\" and offers some up to his friends. Gloria and Melman think it's very tasty, but Alex starts choking when he eats it. He is very hungry for meat, and misses the steaks he used to eat at the zoo in New York. After he falls asleep, he dreams about steak, and he starts to lick one. He is jolted awake by the others and finds himself licking Marty's backside. Marty wants to know what Alex thinks he's doing. Alex pretends to have been counting Marty's stripes.\n",
        "\n",
        "Meanwhile, 2,500 miles to the south, the penguins have arrived at their dream destination, Antarctica, and are standing quietly on the ice, near the bow of the ship. The wind is blowing and there's nothing around them except ice and snow as far as the eye can see. Finally, Private turns to the others and says, \"well, this sucks.\"\n",
        "\n",
        "King Julien and all the lemurs make noise and wake up Alex and the others, surprising them by leading them to an overlook where they gaze out upon a huge expanse of open area, with beautiful green grass, trees and waterfalls visible way in the distance. The king says, \"Welcome to Madagascar.\"\n",
        "\n",
        "It all looks just like the poster that Marty had on the wall back in New York. It was just like the land in his dreams. He and Alex rush forward and romp through the grass, wrestling and teasing each other as they went. Alex tires temporarily, as he hadn't eaten for a long time, but rediscovers that he has more energy than ever and continues his romp.\n",
        "\n",
        "Marty suggests to Alex that he perform his zoo routine for the lemurs. When Julien hears Alex refer to himself as a \"king,\" he becomes concerned, thinking that there can't be two kings on Madagascar.\n",
        "\n",
        "The fossa arrive and observe Alex's performance from behind some rocks. As Alex continues his performance, he looks out over the crowd, comprised of the lemurs and his friends, and they all start to look like steaks to him. When Alex lets out a huge roar, the fossa run off and everyone else decides to run as well, because Alex is attacking. When Alex snaps out of his hunger induced temporary insanity, he has his jaws attached to Marty's butt, but he hadn't bitten down yet.\n",
        "\n",
        "Maurice takes advantage of the moment to educate everyone about the fact that Alex is an apex predator, and if he's hungry, no one is safe. About that time, Alex sees steaks again and starts another attack. He always seems to hone in on Marty as his target of choice. Alex is about to pounce on Marty when Maurice fires a coconut from atop a tree and beans Alex with it, saving Marty.\n",
        "\n",
        "Alex recovers and is very upset at himself for attacking Marty. He's worried what might happen, so he runs off. He falls into a river and ends up floating to another part of the island, the place where the fossa live. There, he sharpens a bunch of sticks and inserts them into the ground, sharp sides up. He's created a sort of jail for himself, with the sticks at the base of some rocks and himself sitting on the rocks.\n",
        "\n",
        "Marty, Gloria and Melman also find themselves roaming around in the land of the predators, where the fossa live. As they walk, they observe several small animals get eaten up by much larger predators, and they are not at all comfortable.\n",
        "\n",
        "A boat horn sounds out on the ocean, so Marty, Gloria and Melman hurry to the beach. They see the same ship that they'd been on earlier. Melman struggles to hoist Gloria up high on his head, his neck bending severely as he does it, but her waving seems to work, as the ship begins to turn around and head for the island. Marty decides to hurry and go find Alex, but Gloria stops him, knowing he wouldn't last long in the land of the predators.\n",
        "\n",
        "The ship arrives at the island and the bow comes right up onto the beach, flush against Melman's face, as he's standing there in shocked amazement. The anchor drops onto the beach and the four penguins appear. Gloria asks them where the people from the ship are and Skipper tells her that they \"are on a slow lifeboat to China.\"\n",
        "\n",
        "During the distraction of the ship coming aground on the island, Marty has run off to go find Alex. He finds him sitting in his little jail enclosure. Alex talks to Marty for a little bit, then lunges and takes a swipe at him. He couldn't help it, he was so hungry. He regains his senses and goes deeper into the rocks to hide. Marty follows him partway and tells Alex he isn't going anywhere without him. He starts to sing, \"New York, New York.\" Alex doesn't respond.\n",
        "\n",
        "The fossa have arrived and are gathering around Marty in force. Soon, they attack and Marty has to run, calling for help. Just as it appears Marty is doomed, Melman and Gloria arrive. Melman sweeps down from on high and scoops up Marty, carrying him to safety. The penguins are there too, and Skipper steps forward, producing a flare gun which he fires into the air, distracting the fossa. He and the other penguins then quickly rig up a device using the steering wheel from the ship, and spin it around rapidly, striking the attacking fossa and knocking them unconscious.\n",
        "\n",
        "Alex shows up, roaring and showing his teeth. He makes as though he's attacking Marty, claiming Marty as his territory, as his meal. Marty is about to faint, thinking Alex is really going to eat him, until Alex whispers to him that it's all for show.\n",
        "\n",
        "Alex then grabs Marty, Gloria and Melman and hoists them all above his head, proclaiming to the fossa that they all belong to him. Then he sets the three back on the ground and starts taking it directly to the fossa, knocking them this way and that, until they all decide to run off. He shouts at them to never come back.\n",
        "\n",
        "The king observes all this and becomes very happy that his plan had worked out after all.\n",
        "\n",
        "Back on the beach, Rico is working feverishly as a chef, preparing some sushi, which is then fed to Alex. Alex is tentative, but he tries it and decides he likes it. He orders 300 pieces to go.\n",
        "\n",
        "Everyone enjoys themselves at the \"Thank You Freaks,\" banquet put on by the lemurs. There's a massive toast, with everyone taking a drink from their coconut cups and then simultaneously spitting it right back out.\n",
        "\n",
        "Marty tells his friends that he's ok with just staying on Madagascar, or going back to New York, just as long as he can be with his friends. They decide to go back to New York.\n",
        "\n",
        "The king gives Alex his crown, then produces a new one for himself. His new crown is larger and has a live gecko on it.\n",
        "\n",
        "Marty, Alex, Gloria, and Melman board the ship. The chimps are still there too. The ship is loaded with lots of fresh fruit and sushi, ready to sail. Alex envisions them making some side trips, since it will be winter at the New York zoo. The penguins, however, are all sitting in beach chairs, sunning themselves. One of them wonders if they should tell those on the ship that it's out of gas.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HF7hS4-FC_Qe"
      },
      "outputs": [],
      "source": [
        "predict_sentiment(model, madagascar_synopsis)"
      ]
    }
  ]
}